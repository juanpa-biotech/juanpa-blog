---
title: "Basic Linear Algebra with R"
categories: ["R", "tutorial", "linear algebra"]
date: "2023-12-20"
description: "A brief tutorial with R code to perform some linear algebra operations"
toc: true
toc-location: left
---

![](/posts/linear-algebra/images/linear_algebra.jpg){fig-align="center"}

# What is linear algebra?

<a href="https://www.math.ucdavis.edu/~linear/linear-guest.pdf" target="_blank">*Linear algebra*</a> *is the study of vectors and linear functions*. In this post, I'm going to show you how to perform some basic linear algebra operations with R code.

# Vector operations

## Vectors

A vector is a mathematical object that possesses both magnitude and direction. In other words, a vector is a mathematical entity representing a physical quantity with both a size (or magnitude) and a specific orientation in space. Vectors are used to describe displacements, velocities, forces, and other concepts in mathematics, physics, and other disciplines.

Formally, a vector in three-dimensional space $\mathbb{R}^3$ can be represented as an ordered triplet of real numbers $(a, b, c)$, where $a$, $b$, and $c$ are the components of the vector along the x, y, and z axes, respectively. In general, in an $n$-dimensional space $\mathbb{R}^n$, a vector is represented as an $n$-tuple $(a_1, a_2, \ldots, a_n)$.

Vectors can also be geometrically represented as arrows in a coordinate system, where the length of the arrow represents the magnitude of the vector, and the direction of the arrow represents the orientation of the vector in space.

To define vectors, in this case in twodimensions, we can use the function `c()`:

```{r define vectors in two dimensions}
# Define three vectors in two dimensions
a <- c(-3, 1)
b <- c(1, -3)
c <- c(-2, 2)

# Display the vectors
a
b
c
```

The above vectors can be represented visually as follows (code not shown):

```{r 2D vector, echo=FALSE, message=FALSE, warning=FALSE}
# Install and load the ggplot2 package if you haven't done so before
# install.packages("ggplot2")
library(ggplot2)

# Create a dataframe with vector data
data <- data.frame(
  x = c(0, a[1], 0, b[1], 0, c[1]),
  y = c(0, a[2], 0, b[2], 0, c[2]),
  vector = rep(c("a", "b", "c"), each = 2)
)

# Create the scatter plot with ggplot2
ggplot(data, aes(x, y)) +
  geom_segment(aes(x = 0, y = 0, xend = x, yend = y, color = vector), arrow = arrow()) +
  geom_vline(xintercept = 0) +
  geom_hline(yintercept = 0) +
  xlim(c(min(data$x) - 1, max(data$x) + 1)) +
  ylim(c(min(data$y) - 1, max(data$y) + 1)) +
  labs(x = "X", y = "Y") +
  theme_classic()
```

To create vectors in four dimensions, we use the same procedure, but add more elements to each vector:

```{r define vectors in fourth dimensions}
# Define two vectors in fourth dimensions
x <- c(30, 20, 40, 10)
y <- c(20, 15, 18, 40)
```

The above procedure can be generalized to any vector in $\mathbb{R}^n$.

## Sum

The sum of two vectors is a mathematical operation that involves adding the corresponding components of the two vectors. Let $\mathbf{A} = (a_1, a_2, \ldots, a_n)$ and $\mathbf{B} = (b_1, b_2, \ldots, b_n)$ be two vectors in $\mathbb{R}^n$. The sum of $\mathbf{A}$ and $\mathbf{B}$ is denoted as $\mathbf{A} + \mathbf{B}$ and is calculated as follows:

$\mathbf{A} + \mathbf{B} = (a_1 + b_1, a_2 + b_2, \ldots, a_n + b_n)$

In other words, the sum of two vectors produces a new vector whose components are the sum of the corresponding components of the original vectors.

Some important properties of vector addition include commutativity ($\mathbf{A} + \mathbf{B} = \mathbf{B} + \mathbf{A}$), associativity $(\mathbf{A} + \mathbf{B}) + \mathbf{C} = \mathbf{A} + (\mathbf{B} + \mathbf{C})$, and the existence of a zero vector ($\mathbf{0} + \mathbf{A} = \mathbf{A} + \mathbf{0} = \mathbf{A}$, where $\mathbf{0}$ is the zero vector).

Vector addition is a fundamental operation in linear algebra and is used in various contexts, such as representing displacements in physics, combining forces, and defining more advanced operations in vector spaces.

With R code you can add vectors with the same length:

```{r sum of vectors}
x + y
```

If the vectors don't have the same length, the elements of the smallest will be recycled:

```{r sum of vectos different lengths}
x + c(10, 10)
```

## Multiplying a vector by a scalar

The multiplication of a vector by a scalar is a mathematical operation that involves multiplying each component of the vector by the scalar. Let $\mathbf{V}$ be a vector and $k$ be a scalar; the multiplication of the vector by the scalar is denoted as $k \cdot \mathbf{V}$ and is calculated as follows:

If $\mathbf{V} = (v_1, v_2, \ldots, v_n)$, then

$k \cdot \mathbf{V} = (k \cdot v_1, k \cdot v_2, \ldots, k \cdot v_n)$

In other words, each component of the original vector is multiplied by the scalar, producing a new vector whose components are the product of each component of the original vector by the scalar.

This operation has various properties, such as associativity $(a \cdot b) \cdot \mathbf{V} = a \cdot (b \cdot \mathbf{V})$, distributivity with respect to the sum of vectors $k \cdot (\mathbf{A} + \mathbf{B}) = k \cdot \mathbf{A} + k \cdot \mathbf{B}$, and distributivity with respect to the sum of scalars $(a + b) \cdot \mathbf{V} = a \cdot \mathbf{V} + b \cdot \mathbf{V}$.

To multiply by a scalar, we use the operator `*` as follows:

```{r scalar product}
10 * x
```

## Dot or inner product

The dot product, also known as the scalar product or inner product, is a mathematical operation between two vectors that results in a scalar (a single number). The dot product of two vectors $\mathbf{A} = (a_1, a_2, \ldots, a_n)$ and $\mathbf{B} = (b_1, b_2, \ldots, b_n)$ is commonly denoted as $\mathbf{A} \cdot \mathbf{B}$ and is calculated as follows:

$\mathbf{A} \cdot \mathbf{B} = a_1 \cdot b_1 + a_2 \cdot b_2 + \ldots + a_n \cdot b_n$

In other words, you multiply the corresponding components of the two vectors and sum these products. The result is a scalar, not a vector.

Some important properties of the dot product include commutativity ($\mathbf{A} \cdot \mathbf{B} = \mathbf{B} \cdot \mathbf{A}$), distributivity with respect to the sum of vectors ($\mathbf{A} \cdot (\mathbf{B} + \mathbf{C}) = \mathbf{A} \cdot \mathbf{B} + \mathbf{A} \cdot \mathbf{C}$), and scalar multiplication ($k \cdot (\mathbf{A} \cdot \mathbf{B}) = (k \cdot \mathbf{A}) \cdot \mathbf{B} = \mathbf{A} \cdot (k \cdot \mathbf{B})$), where (*k*) is a scalar.

The dot product has various applications in geometry, physics, and other mathematical disciplines, and it is fundamental in defining concepts such as vector length, vector projection, and angles between vectors.

In R we use the operator `%*%` to perform the dot product:

```{r vectors dot product}
x %*% y
```

Note that this operator returns an object with the classes "matrix" and "array":

```{r class of dot product}
class(x %*% y)
```

If you need just the numeric value, use the `as.numeric()` function:

```{r vectors dot product just numeric value}
as.numeric(x %*% y)
```

## Norm

The norm of a vector, also known as magnitude or length, is a measure that indicates the absolute size of the vector in Euclidean space. The norm of a vector $\mathbf{v}$ in an $n$-dimensional space, commonly denoted as $\|\mathbf{v}\|$ or $\|\mathbf{v}\|_2$, is calculated using the following formula:

$\|\mathbf{v}\| = \sqrt{v_1^2 + v_2^2 + \ldots + v_n^2}$

In other words, the norm of a vector is the square root of the sum of the squares of its individual components. This is derived from the Euclidean distance in $n$-dimensional space.

Some important properties of the norm include:

1.  $\|\mathbf{v}\| \geq 0$: The norm of a vector is always non-negative.
2.  $\|\mathbf{v}\| = 0$ if and only if $\mathbf{v} = \mathbf{0}$, where $\mathbf{0}$ is the zero vector.
3.  $\|\alpha \cdot \mathbf{v}\| = |\alpha| \cdot \|\mathbf{v}\|$ for any scalar $\alpha$.
4.  Triangle Inequality: $\|\mathbf{u} + \mathbf{v}\| \leq \|\mathbf{u}\| + \|\mathbf{v}\|$.

With R code, the norm or magnitude of a vector can be obtained as follows:

```{r vector norm}
sqrt(x %*% x)
```

With a vector defined as a matrix object, you can also obtain the magnitude with the function `norm` :

```{r norm function}
norm(as.matrix(x), type = "F")
```

## Scalar Projections

The scalar projection of a vector $\mathbf{s}$ onto another vector $\mathbf{r}$ is the length of the projection of $\mathbf{s}$ onto the direction of $\mathbf{r}$. It is commonly denoted as $\text{proj}_{\mathbf{r}}(\mathbf{s})$ and is calculated using the following formula:

$\text{proj}_{\mathbf{r}}(\mathbf{s}) = \frac{\mathbf{s} \cdot \mathbf{r}}{\|\mathbf{r}\|}$

Where:

-   $\mathbf{s} \cdot \mathbf{r}$ denotes the dot product between the two vectors.

-   $\|\mathbf{r}\|$ is the norm (magnitude) of $\mathbf{r}$.

The scalar projection of $\mathbf{s}$ onto $\mathbf{r}$ represents the length of the line segment connecting the origin to the point where $\mathbf{s}$ projects onto the line following the direction of $\mathbf{r}$.

In R, the scalar projection of a vector `s` onto a vector `r` can be obtained with the next code:

```{r scalar projection}
r <- c(3, -4, 0)
s <- c(10, 5, -6)

r %*% s / sqrt(r %*% r)
```

In a similar way, you can obtain the vector projection of `s` onto `r` using the function `norm()`:

```{r vector projection}
r %*% s / norm(as.matrix(r), type = "F")
```

# Matrix operations

## Matrices

In linear algebra, a matrix is a two-dimensional array of numbers, symbols, or mathematical expressions arranged in rows and columns. A matrix is typically represented by uppercase letters, such as A, B, C, etc., and its elements are denoted by indices indicating the position of a number in the matrix.

For example, a matrix A of size *m Ã— n* is denoted as:

$A = \begin{bmatrix} a_{11} & a_{12} & \dots & a_{1n} \\ a_{21} & a_{22} & \dots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \dots & a_{mn} \end{bmatrix}$

Here, $a_{ij}$ represents the element in the *ith* row and *jth* column of the matrix. In this context, *m* is the number of rows, and *n* is the number of columns. When the number of rows is equal to the number of columns (*m* = *n*), it is called a square matrix.

Matrices are used in linear algebra to represent and manipulate systems of linear equations, linear transformations, and in various contexts in mathematics, physics, statistics, computer science, and other disciplines. Basic operations with matrices include addition, subtraction, scalar multiplication, and matrix multiplication.

In R, we use the function `matrix()` to define matrices:

```{r define a matrix}
# Define a matrix
m <- c(7, -6, 12, 8)
m <- matrix(m, nrow = 2, byrow = TRUE)
m
```

## Matrix multiplication by a vector

Matrix-vector multiplication is a fundamental operation in linear algebra. Given a matrix $A$ of dimensions $m \times n$ and a column vector $\mathbf{v}$ of dimension $n \times 1$, the product of the matrix by the vector, denoted as $A \mathbf{v}$, results in a new column vector of dimension $m \times 1$.

The operation is performed by multiplying each element in a matrix row by the corresponding element in the vector and summing the resulting products. The element in the $i$-th position of the resulting vector is the sum of the products of the elements in the $i$-th row of the matrix and the corresponding elements in the vector.

The mathematical expression for the multiplication of a matrix $A$ by a column vector $\mathbf{v}$ can be written as follows:

$A \mathbf{v} = \begin{bmatrix} a_{11} & a_{12} & \dots & a_{1n} \\ a_{21} & a_{22} & \dots & a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ a_{m1} & a_{m2} & \dots & a_{mn} \end{bmatrix} \begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix} = \begin{bmatrix} (a_{11}v_1 + a_{12}v_2 + \dots + a_{1n}v_n) \\ (a_{21}v_1 + a_{22}v_2 + \dots + a_{2n}v_n) \\ \vdots \\ (a_{m1}v_1 + a_{m2}v_2 + \dots + a_{mn}v_n) \end{bmatrix}$

It's important to note that matrix-vector multiplication is defined when the number of columns in the matrix is equal to the number of rows in the vector. In this case, the result is a new vector with the same number of rows as the matrix and the same number of columns as the original vector.

To perform this operation in R, we use the operator `%*%`:

```{r matrix by a vector}
# Matrix multiplication by a vector
v <- c(5, 6)
m %*% v
```

## Matrix multiplication

Matrix multiplication is an algebraic operation that combines two matrices to produce a third matrix. Given two matrices $A$ and $B$, where matrix $A$ has dimensions $m \times n$ and matrix $B$ has dimensions $n \times p$, the product of $A$ and $B$, denoted as $AB$, results in a new matrix $C$ with dimensions $m \times p$.

The operation is performed by multiplying each element in a row of the first matrix by the corresponding element in the column of the second matrix and summing the resulting products. The entry in the $i$-th row and $j$-th column of the resulting matrix $C$ is the result of this sum for the intersection of the $i$-th row of $A$ and the $j$-th column of $B$.

The mathematical expression for matrix multiplication can be represented as follows:

$C_{ij} = \sum_{k=1}^{n} A_{ik} \cdot B_{kj}$

where $C_{ij}$ is the element in the $i$-th row and $j$-th column of matrix $C$, $A_{ik}$ is the element in the $i$-th row and $k$-th column of matrix $A$, and $B_{kj}$ is the element in the $k$-th row and $j$-th column of matrix $B$.

It is important to note that for matrix multiplication to be defined, the number of columns in the first matrix must be equal to the number of rows in the second matrix. The resulting matrix has the number of rows of the first matrix and the number of columns of the second matrix.

To perform multiplications between matrices, we use the operator `%*%`. Always be sure that the number of columns in the first matrix is the same as the number of rows in the second:

```{r matrix multiplication}
# A matrix of 2 x 3
A <- matrix(c(1, 2, 3, 4, 0, 1), nrow = 2, byrow = TRUE)
A

# A matrix of 3 x 3
B <- matrix(c(1, 1, 0, 0, 1, 1, 1, 0, 1), nrow = 3, byrow = TRUE)
B

# Matrix multiplication
A %*% B
```

## Multiplication of a matrix by a scalar

Scalar multiplication by a matrix is an operation in which each element of the matrix is multiplied by the given scalar. Given a scalar $c$ and a matrix $A$ of dimensions $m \times n$, the product of $c$ by $A$, denoted as $cA$, is obtained by multiplying each element of $A$ by $c$, resulting in a new matrix of the same size $m \times n$.

The mathematical expression for scalar multiplication by a matrix can be written as:

$c \cdot A = \begin{bmatrix} c \cdot a_{11} & c \cdot a_{12} & \dots & c \cdot a_{1n} \\ c \cdot a_{21} & c \cdot a_{22} & \dots & c \cdot a_{2n} \\ \vdots & \vdots & \ddots & \vdots \\ c \cdot a_{m1} & c \cdot a_{m2} & \dots & c \cdot a_{mn} \end{bmatrix}$

Where $a_{ij}$ represents the elements of matrix $A$. Each element of the resulting matrix $cA$ is simply the product of the scalar $c$ with the corresponding element of matrix $A$.

As with vectors, we use the operator `*` to perfrom this operation:

```{r matrix by a scalar}
# Matrix multiplication by a scalar
100 * A
```

## Range of a matrix

The rank of a matrix is a measure indicating the maximum number of linearly independent rows or columns in the matrix. We can calculate the rank for both rectangular and square matrices. The specific definition may vary depending on whether you are dealing with the row rank or column rank, but both are related.

1.  *Row Rank Definition:* The row rank of a matrix is the maximum number of linearly independent rows in the matrix.

2.  *Column Rank Definition:* The column rank of a matrix is the maximum number of linearly independent columns in the matrix.

In more general terms, the rank of a matrix can be defined as the dimension of the space spanned by its row or column vectors. An important fact about the rank of a matrix is that the row rank is always equal to the column rank. This is known as the rank theorem and is a crucial property highlighting the relationship between the rows and columns of a matrix.

To calculate the rank of a matrix we use the function `qr()` as follows:

```{r matrix rank}
# A matrix of 3 x 3
C <- matrix(c(1, 0, 1, -2, -3, 1, 3, 3, 0), nrow = 3, byrow = TRUE)
C

# Rank of matrix C
qr(C)$rank

# A matrix of 2 x 4 
D <- matrix(c(1, 1, 0, 2, -1, -1, 0, -2), nrow = 2, byrow = TRUE)
D

# Rank of matrix D
qr(D)$rank
```

## Inverse of a matrix

The inverse of a matrix is a fundamental concept in linear algebra. Given a square matrix $A$, $A^{-1}$ is said to be the inverse of $A$ if the product of $A$ by $A^{-1}$ (or $A^{-1}$ by $A$) is equal to the identity matrix $I$, of the same size as $A$. Mathematically, this is expressed as:

$A \cdot A^{-1} = A^{-1} \cdot A = I$

Where:
* $A$ is the original matrix.
* $A^{-1}$ is the inverse matrix.
* $I$ is the identity matrix.

It's important to note that not all matrices have an inverse. For a matrix to have an inverse, it must be a square matrix, and it must be nonsingular, meaning its determinant should not be equal to zero.

If a matrix $A$ has an inverse, it is denoted as $A^{-1}$, and it satisfies:

$A \cdot A^{-1} = A^{-1} \cdot A = I$

The inverse matrix has important properties, such as the inverse of the inverse ($(A^{-1})^{-1} = A$) and the inverse of the product of matrices ($(AB)^{-1} = B^{-1}A^{-1}$), among others.

To calculate the inverse of a matrix we use the `solve()` function:

```{r inverse matrix with solve}
set.seed(9)
# A matrix of 4 x 4 
E <- matrix(
  round(runif(16, min = 1, max = 30), 0), 
  nrow = 4, byrow = TRUE
)
E

# Inverse of matrix E
E_i <- solve(E)
E_i
```

## Solving systems of linear equations

A system of linear equations is a set of two or more linear equations that share a common set of variables. Each linear equation within the system represents a linear relationship between the variables. In general, a system of linear equations can be expressed as follows:

$\begin{cases} a_{11}x_1 + a_{12}x_2 + \ldots + a_{1n}x_n = b_1 \\ a_{21}x_1 + a_{22}x_2 + \ldots + a_{2n}x_n = b_2 \\ \vdots \\ a_{m1}x_1 + a_{m2}x_2 + \ldots + a_{mn}x_n = b_m \end{cases}$

Where:

* $x_1, x_2, \ldots, x_n$ are the variables of the system.

* $a_{ij}$ are the coefficients accompanying the variables.

* $b_i$ are the constant terms in each equation.

* The system has $m$ linear equations.

The typical goal when solving a system of linear equations is to find the values of the variables $x_1, x_2, \ldots, x_n$ that satisfy all the equations simultaneously. These values represent the solution to the system. Depending on the nature of the system, it may have a unique solution, no solution, or infinitely many solutions. Common methods for solving systems of linear equations include substitution, elimination, and the use of matrices.

Now let us define the following system of linear equations. We want to obtain the values of $a$, $b$, and $c$ that satisfied the system:

-   $(1): a + b + c = 15$
-   $(2): 3a + 2b + c = 28$
-   $(3): 2a + b + 2c = 23$

First, define a matrix with the system coefficients and a vector with the results or constants for each equation:

```{r coefficient matrix}
# Coefficient matrix
C <- matrix(c(1, 1, 1, 3, 2, 1, 2, 1, 2), nrow = 3, byrow = TRUE)
C

# Constant vector 
r <- c(15, 28, 23)
r
```

Next, obtain the inverse of the coefficient matrix using the `solve()` function:

```{r coefficient inverse matrix}
# Coefficient matrix inverse
C_inv <- solve(C)
C_inv
```

To calculate the solution, we multiply the previous inverse matrix by the constant vector:

```{r solution}
# Solution
s <- C_inv %*% r
s
```

Thus the solution to our system of equations is as follows:

-   $a = 3$
-   $b = 7$
-   $c = 5$

## Determinant of a matrix

The determinant of a matrix is a scalar associated with that matrix and is commonly denoted as $\text{det}(A)$ or $|A|$. The process for calculating the determinant varies depending on the size of the matrix.

1.  *First-Order Matrix:*

For a $1 \times 1$ matrix, which is simply a number, the determinant is equal to that number.

  $\text{det}([a]) = a$

2.  *Second-Order Matrices:*

For a $2 \times 2$ matrix:

  $\text{det}\begin{bmatrix} a & b \\ c & d \end{bmatrix} = ad - bc$

3.  *Higher-Order Matrices:*

For an $n \times n$ square matrix $A$, the determinant can be calculated using cofactor expansion or more advanced methods such as diagonalization.

The general formula for the determinant of an $n \times n$ matrix $A$ can be expressed recursively in terms of minors and cofactors:

  $\text{det}(A) = \sum_{i=1}^{n} (-1)^{i+j} \cdot a_{ij} \cdot \text{det}(M_{ij})$

Where:

* $a_{ij}$ is the element in row $i$, column $j$ of matrix $A$.
* $\text{det}(M_{ij})$ is the determinant of the minor obtained by removing row $i$ and column $j$ from matrix $A$.

To carry out this operation, we use the function `det()`:

```{r matrix determinant}
C
det(C)
```

## Transpose of a Matrix

The transpose of a matrix is an operation that involves swapping its rows and columns. It is commonly denoted by $A^T$ or $A'$. If $A$ is an $m \times n$ matrix, then the transpose $A^T$ is an $n \times m$ matrix obtained by exchanging the rows and columns of the original matrix.

Mathematically, if $A$ has elements $a_{ij}$, then the transpose $A^T$ has elements $b_{ij}$ where $b_{ij} = a_{ji}$. In other words, the element in row $i$, column $j$ of $A^T$ is the same as the element in row $j$, column $i$ of $A$.

For example, if $A$ is:

$A = \begin{bmatrix} 1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}$

Then, the transpose $A^T$ would be:

$A^T = \begin{bmatrix} 1 & 4 \\ 2 & 5 \\ 3 & 6 \end{bmatrix}$

Some important properties of the transpose include:

1.  $(A^T)^T = A$
2.  $(cA)^T = cA^T$, where $c$ is a constant.
3.  $(A + B)^T = A^T + B^T$, where $A$ and $B$ are matrices compatible for addition.

We use the `t()` function to obtain the transpose of a matrix:

```{r matrix transpose}
C
t(C)
```

## Identity matrix

The identity matrix, commonly denoted by $I_n$ or simply $I$, is an $n \times n$ square matrix that has ones on its main diagonal (from the upper-left to the lower-right) and zeros in the rest of its elements. In other words, an identity matrix is a square matrix where all elements outside the main diagonal are zero, and all elements on the main diagonal are one.

The general form of an $n \times n$ identity matrix is:

$I_n = \begin{bmatrix} 1 & 0 & \ldots & 0 \\ 0 & 1 & \ldots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \ldots & 1 \end{bmatrix}$

Where $n$ represents the size of the identity matrix.

It is important to note that the distinctive property of the identity matrix is that when any square matrix $A$ is multiplied by the appropriate identity matrix, the result is the same matrix $A$. In mathematical terms, if $A$ is an $n \times m$ matrix, then $I_n \times A = A \times I_m = A$, provided that the dimensions are compatible for multiplication.

To define a identity matrix we use the `diag()` function:

```{r identity matrices}
# Identity matrix of 2 x 2
diag(2)

# Identity matrix of 4 x 4
diag(4)
```

## Eigenvalues and eingenvectors

The concepts of eigenvalues and eigenvectors are fundamental in linear algebra and apply to linear transformations represented by matrices.

1.  *Eigenvalues:*

Given a square matrix $A$, an eigenvalue $\lambda$ is a scalar such that when multiplied by a nonzero vector $v$, the result is simply the same vector scaled by $\lambda$:

  $A \mathbf{v} = \lambda \mathbf{v}$

Here, $\mathbf{v}$ is the corresponding eigenvector associated with the eigenvalue $\lambda$. It's important to note that for an eigenvalue and its corresponding eigenvector, the multiplication of matrix $A$ by vector $\mathbf{v}$ results in a mere scaling of the vector by the eigenvalue.

2.  *Eigenvectors:*

Given an eigenvalue $\lambda$, the corresponding eigenvector $\mathbf{v}$ is a nonzero vector that satisfies the equation:

  $A \mathbf{v} = \lambda \mathbf{v}$

In other words, when matrix $A$ is multiplied by the eigenvector $\mathbf{v}$, the result is simply the same vector scaled by the eigenvalue $\lambda$.

Eigenvectors provide special directions in which the linear transformation represented by matrix $A$ has only a scaling effect. The magnitude of this scaling is determined by the corresponding eigenvalue.

We use the function `eigen()` to obtain the eigenvectors and eigenvalues of a given matrix:

```{r function eigen}
# A matrix of 2 x 2
A <- matrix(c(3, 4, 0, 5), nrow = 2, byrow = TRUE)
A

# eigen function
A_ei <- eigen(A)

# eigenvalues
A_lmd <- A_ei$values
A_lmd

# eigenvectors
A_ev <- A_ei$vectors
A_ev
```

Interestingly, the original matrix can be decomposed as follows:

```{r decomposed matrix}
A_ev %*% diag(A_lmd) %*% solve(A_ev)
```

## Gram-Schmidt process

The Gram-Schmidt process is a method used in linear algebra to take a set of linearly independent vectors and construct from them an orthogonal or orthonormal set. This process is useful in various contexts, such as matrix diagonalization, solving systems of linear equations, and QR factorization.

Given a set of vectors $\{\mathbf{v}_1, \mathbf{v}_2, \ldots, \mathbf{v}_n\}$ in a Euclidean vector space, the Gram-Schmidt process generates an orthogonal set $\{\mathbf{u}_1, \mathbf{u}_2, \ldots, \mathbf{u}_n\}$ or an orthonormal set $\{\mathbf{q}_1, \mathbf{q}_2, \ldots, \mathbf{q}_n\}$ of vectors.

The Gram-Schmidt process is valuable in situations where we need to work with sets of vectors that are not orthogonal initially but desires to obtain orthogonal sets to simplify calculations and analyses.

To perform this process, you can use the `gramSchmidt()` function from `pracma` package:

```{r gram schmidt process, warning=FALSE}
#install.packages("pracma")
library(pracma)

# A matrix of 3 x 3
A <- matrix(c(2, -2, 18, 2, 1, 0, 1, 2, 0), nrow = 3, byrow = TRUE)
A

# gramSchmidt function
A_gs <- gramSchmidt(A)

# Orthogonalized matrix
A_gs$Q

# Upper triangular matrix 
A_gs$R
```

Note that the above operation allows us to decompose the original matrix such that:

```{r QR decomposition}
A_gs$Q %*% A_gs$R
```

# Calculation of mean and variance with linear algebra operations

Statistics like the mean and the variance can be easily done with matrix operations.

## Mean

Let's simulate some data and obtain the mean with the `mean()` function:

```{r data and data mean}
# Data
set.seed(5)
y <- runif(100, min = 10, max = 35)

# mean function
mean(y)
```

Using vector multiplication, the mean can be calculated as follows:

```{r average with matrix multiplication}
# Sample size
yl <- length(y) 

# Vector of ones of the same length as y
v1 <- rep(1, yl)

# Calculation of the mean
y_mean <- v1 %*% y / yl
y_mean
```

## Sample variance

Something similar can be done with the sample variance. First, let's calculate the sample variance with the `var()` function:

```{r data sample variance}
# var function
var(y)
```

Now let's calculate the sample variance with vector multiplication:

```{r sample variance with matrix operations}
# Differences between the data and the sample mean
r <- y - as.numeric(y_mean)

# Calculation of the variance
r %*% r / (yl - 1)
```

This post is licensed under the [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)

[![](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](http://creativecommons.org/licenses/by/4.0/)
