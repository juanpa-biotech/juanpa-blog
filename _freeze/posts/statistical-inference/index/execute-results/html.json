{
  "hash": "b9867c2864580f9fb44d405f33093df6",
  "result": {
    "markdown": "---\ntitle: \"Statistical Inference with R\"\ncategories: [\"R\", \"statistical inference\", \"statistics\", \"population\", \"sample\", \"parameters\", \"mean\", \"variance\", \"standard deviation\"]\ndate: \"2023-09-27\"\ndescription: 'Some statistical inference concepts and terms explained using R'\ntoc: true\ntoc-location: left\n---\n\n\n*Perfection is always impossible; always it's an approximation*\n\n# Introduction\n\nFormally, <a href=\"https://www.sciencedirect.com/topics/neuroscience/statistical-inference\" target=\"_blank\">statistical inference</a> can be defined as the process by which conclusions are drawn about a population based on values calculated from a sample drawn from that population.\n\nIf your work is related to science in any of its branches I am sure you know that statistical inference plays an important role in the exploration, analysis and validation of our experimental results. A statement is often used as evidence that you have succeeded in testing some hypothesis derived from your research: *these results show significant differences with a p-value < 0.05*.\n\nIn this post I will explain some concepts related to statistical inference and how it can be illustrated using R code.\n\n# Populations, Samples, Parameters and Statistics\n\nAccording to <a href=\"https://dictionary.cambridge.org/es/diccionario/ingles/inference\" target=\"_blank\">Cambridge Dictionary</a> , an inference is an assumption that is made or an opinion that is formed from available information. In statistics, the purpose of an inference is to draw conclusions about the population from a sample.\n\nThe *population* is the complete set of objects of interest to us. For example, we may be interested in all the trees of a species located in a forest, all the people in a city, or even the elements of a continuous process such as the cookies produced in a factory. It is important to keep in mind that most of the time we do not have access to the entire population because it is too large and it would be impractical to analyze each element or because the population can be considered a continuum as in the cookie factory.\n\nStatistical inference can help us when we do not have all the information of a population. The first step is to take some elements of this population, which we call a *sample*. For this we must ensure that the sampling process guarantees that each element of the population has an equal chance of being selected.\n\nOnce we obtain the sample we may be interested in some property of the population, so we proceed to measure this characteristic in each element of the sample. This property can be the height, the expression of some gene, the amount of sugar, the hardness, etc. Then, a number is calculated that summarizes the characteristic measured across all elements of the sample. These numbers are usually the mean, variance and standard deviation, which together are called *statistics*.\n\nFinally, with some assumptions, the population *parameters* can be inferred from the statistics. The parameters are the mean, standard deviation and variance of the entire population.\n\nSee the following figure where I illustrate the whole process with cookies:\n\n![](/posts/statistical-inference/statistical_inference.jpg){fig-align=\"center\" width=\"800\"}\n\nDepending on the measured characteristic or property different inference methods may be required. In this explanation I will only work with numbers on a continuous scale.\n\n# Calculation of Statistics\n\nIt is time to do some calculations with R code. First I will define a simulated population of 1500 values. Suppose we have measured all the heights of a given tree species:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(34)\npopulation_trees <- round(rnorm(1500, mean = 15, sd = 4), 2)\n```\n:::\n\n\nTo simulate the values I used the `rnorm()` function, which returns random values from a normal distribution with a given mean and standard deviation. The `round()` function rounds the values to the given number of significant figures, and finally the `set.seed()` function ensures that we always get the same results from this and other code chunks, so if you decide to reproduce them on your own computer the results should also be the same.\n\nNext, let's take a sample of 30 elements from our simulated population:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(12)\nsample_trees <- sample(population_trees, size = 30)\nsample_trees\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 11.89  9.95 12.65 19.30 12.51 21.27 15.75 12.46 18.07 14.08 15.30 20.11\n[13] 10.78 17.12 14.09 14.57 16.02 14.89 13.87 18.05 14.37 17.60 14.55 23.41\n[25] 11.80 14.35 15.52 19.99 14.00 18.24\n```\n:::\n:::\n\n\nNote that the `sample()` function takes the elements of the given data set randomly, so I also used the `set.seed()` function to ensure the reproducibility of this example.\n\n## Sample Mean or Sample Average\n\nWe can calculate the sample mean with the following equation:\n\n![](/posts/statistical-inference/mean_s.png){fig-align=\"center\" width=\"150\"}\n\nThe above is the sum (*summation*) of all the measurements in the sample divided by the sample size (*n*). *i* is the index of each item or measurement.\n\nWith R code in our previous sample:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nms_trees <- sum(sample_trees) / length(sample_trees)\nms_trees\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15.552\n```\n:::\n:::\n\n\nWe can directly use the `mean()` function with our data to calculate the mean:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(sample_trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15.552\n```\n:::\n:::\n\n\n## Sample Variance and Dtandard Deviation\n\nFor the sample variance you use the next equation:\n\n![](/posts/statistical-inference/variance_s.png){fig-align=\"center\" width=\"265\"}\n\nThis is described as the sum of the differences between each sample measurement and the sample mean. Each difference is squared, so negative and positive differences do not cancel each other out. The sum is then divided by the sample size minus one.\n\nWith R code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvs_trees <- sum((sample_trees - ms_trees)^2) / (length(sample_trees) - 1)\nvs_trees\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10.32456\n```\n:::\n:::\n\n\nIf you are wondering why we have divided by n - 1, I can briefly tell you that this way offers the best estimation of the population variance. You can also check out the next page: <a href=\"https://www.willemsleegers.com/post/2020/08/05/why-divide-by-n-1/\" target=\"_blank\">Why divide by N - 1 when calculating the variance of a sample?</a>.\n\nTo obtain the standard deviation, it is sufficient to calculate the square root of the variance, thus providing a measure of the dispersion of the sample in the original units of measurement:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nss_trees <- sqrt(vs_trees)\nss_trees\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.213186\n```\n:::\n:::\n\n\nAs in the case of the mean, with R code we can obtain the variance and standard deviation directly with the `var()` and `sd()` functions, respectively:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#Sample varianza\nvar(sample_trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 10.32456\n```\n:::\n\n```{.r .cell-code}\n# Standard deviation \nsd(sample_trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.213186\n```\n:::\n:::\n\n\n# Calculation of Parameters\n\nIf you have all the data for a population, we can also directly calculate the mean, variance and standard deviation.\n\n## Mean or average of the population\n\nTo calculate population mean we use the next equation:\n\n![](mean_p.png){fig-align=\"center\" width=\"141\"}\n\nYou can see that this equation is quite similar to the sample mean, but here *N* refers to all elements of the population. Let's calculate the mean in our simulated data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmp_trees <- sum(population_trees) / length(population_trees)\nmp_trees\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14.89803\n```\n:::\n:::\n\n\nInstead of the previous operation you can use the `mean()` function with the same purpose:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(population_trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14.89803\n```\n:::\n:::\n\n\n## Variance and Standard Deviation of the Population\n\nThe equation for population variance:\n\n![](variance_p.png){fig-align=\"center\" width=\"282\"}\n\nNote that here we divide by the total population size *N*, not by *n-1* as in the sampling variance. Let us calculate this parameter in our previous data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum_square <- sum((population_trees - mean(population_trees))^2)\npv_trees <- sum_square / length(population_trees)\npv_trees\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15.39988\n```\n:::\n:::\n\n\nThe base functions of R do not have one that calculates the population variance, but this can be solved by multiplying the value obtained with var()by (n-1)/n:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvarp_trees <- var(population_trees)\nvarp_trees <- varp_trees * (length(population_trees) - 1) / length(population_trees)\nvarp_trees\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15.39988\n```\n:::\n:::\n\n\nTo calculate the standard deviation it would be sufficient to obtain the square root of the previous value:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(varp_trees)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 3.924268\n```\n:::\n:::\n\n\nNote that the population mean and standard deviation are close to those specified in the code we used to simulate the data `rnorm(1500, mean = 15, sd = 4)`.\n\n# Distributions\n\nA probability distribution is a function (a table, an equation or a graph) that relates all possible values in a population to their frequencies, counts or probabilities of occurrence.\n\nTo illustrate the above let's simulate a population of 1000 cookies, where we are interested in the diameter of each cookie. The frequency distribution of the diameter would look like this:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Packages\nlibrary(dplyr)\nlibrary(ggplot2)\n\nset.seed(5) # For reproducibility\n\n# Diameter data from a normal distribution\ndiameter <- rnorm(1000, mean = 5, sd = 0.4)  # mean = 5 cm\ndiameter <- tibble(D = diameter)\n\n# Histogram \nfreq_hist <- diameter %>% \n  ggplot(aes(x = D)) +\n  geom_histogram(color = \"black\", fill = \"white\", binwidth = 0.2) +\n  xlab(\"Diameter (cm)\") +\n  ylab(\"Count\") +\n  theme_classic() \nfreq_hist\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/cookies distribution-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nAlthough in many cases we will not have all the data from a population, the concept of probability distribution is still important in statistical inference. This is because we can assume that our data come from a theoretical distribution and then test whether the sample statistics belong to that distribution.\n\n## Sampling Mean Distribution\n\nThe concept of distribution can also be applied to statistics such as the mean. To illustrate the above for our population of cookies let's repeatedly take samples of size 5 and make a histogram with the averages of these samples:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(151) # For reproducibility\n\n# 1000 samples\nn_samples <- 1000\n\n# Empty vector where all means will be stored \nmean_cookies <- vector(\"numeric\", length = n_samples)\n\n# Take a random sample of size 5, calculate the mean and save it in previous vector\n# Repeat the process 1000 times\nfor (i in 1:n_samples) {\n  mean_cookies[i] <- mean(sample(diameter$D, size = 5)) \n}\n\n# Make a data frame with all means from samples of size 5\nmean_cookies <- data.frame(mean_n5 = mean_cookies)\n\n# Histogram\nfreq_hist_mean <- mean_cookies %>% \n  ggplot(aes(x = mean_n5)) +\n  geom_histogram(color = \"black\", fill = \"white\", binwidth = 0.1) +\n  xlab(\"Diameter (cm)\") +\n  ylab(\"Count\") +\n  theme_classic() \nfreq_hist_mean\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/mean cookies distribution-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe previous code can be repeated with other sample sizes and this will be important in a later section (*Central Limit Theorem*).\n\n## The normal distribution\n\nSince it is rare to know the actual distribution of data, we have to resort to theoretical distributions. There are a lot of such distributions and undoubtedly the *normal distribution* is the most important in statistical inference. This distribution applies to continuous data (length, weight, temperature, etc.) and is defined by the following equation:\n\n![](normal_eq.png){fig-align=\"center\" width=\"285\"}\n\nA graph of the above equation looks like this (with mean equal to zero and standard deviation equal to one):\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/normal plot-1.png){fig-align='center' width=672}\n:::\n:::\n\n\n## Student's t distribution\n\nWhen sample sizes are small and the true standard deviation of the population is not known, *Student's t-distribution* can be used to make a good guess of the distribution of the data. The shape of this distribution depends on the sample size and for large sample sizes the t-distribution tends to be normal:\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](index_files/figure-html/t distributions-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThe black line corresponds to a sample size of 2, the red line to a sample size of 4, the blue one to a size of 10, and the green one to a size of 50.\n\n# Central Limit Theorem\n\nThe central limit theorem states that even if the original data do not follow a normal distribution, the sampling distribution of the mean of the data will tend to follow a normal distribution when the sample size is large enough.\n\nTo illustrate this theorem let's simulate data from a non-normal distribution:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(51)\n\n# Simulate data from a uniform distribution\nnon_normal_data <- data.frame(x = runif(10000, max = 100)) \n\n# Histogram\nnon_normal_hist <- ggplot(non_normal_data, aes(x)) +\n  geom_histogram(color = \"black\", fill = \"white\") +\n  ylab(\"Count\") +\n  theme_classic()\nnon_normal_hist\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/data non-normal-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nNow let's take 1000 samples of size 2, calculate their means, and made a histogram with all these means:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(15)\n\nmean_n2 <- vector(\"numeric\", length = 1000)\nfor (i in 1:1000) {\n  mean_n2[i] <- mean(sample(non_normal_data$x, size = 2))\n}\n\nhist_n2 <- ggplot(data = data.frame(x = mean_n2), aes(x)) +\n  geom_histogram(color = \"black\", fill = \"white\") +\n  ylab(\"Count\") +\n  xlab(\"Mean value (n = 2)\") +\n  theme_classic()\n\nhist_n2\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/n5 means-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWe can repeat the same process with samples of size 10:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(15)\n\nmean_n10 <- vector(\"numeric\", length = 1000)\nfor (i in 1:1000) {\n  mean_n10[i] <- mean(sample(non_normal_data$x, size = 10))\n}\n\nhist_n10 <- ggplot(data = data.frame(x = mean_n10), aes(x)) +\n  geom_histogram(color = \"black\", fill = \"white\") +\n  ylab(\"Count\") +\n  xlab(\"Mean value (n = 10)\") +\n  theme_classic()\n\nhist_n10\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/n10 means-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nThen with samples of size 30:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(13)\n\nmean_n30 <- vector(\"numeric\", length = 1000)\nfor (i in 1:1000) {\n  mean_n30[i] <- mean(sample(non_normal_data$x, size = 30))\n}\n\nhist_n30 <- ggplot(data = data.frame(x = mean_n30), aes(x)) +\n  geom_histogram(color = \"black\", fill = \"white\") +\n  ylab(\"Count\") +\n  xlab(\"Mean value (n = 30)\") +\n  theme_classic()\n\nhist_n30\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/n30 means-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nIt can be observed that as the sample size increases the distribution of the mean tends to adopt a bell shape. That is, the distribution of the mean tends to normality as we increase the sample size. Note also how all distributions of the mean are centered around the mean of the original data:\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nlibrary(ggpubr) # Arrange multiple ggplots on the same figure\n\nhist_means <- ggarrange(non_normal_hist, hist_n2, hist_n10, hist_n30)\nhist_means\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/all means dist-1.png){fig-align='center' width=672}\n:::\n:::\n\n\nWhat happens to the variance and standard deviation of the sampling distributions of the mean? These parameters decrease in each distribution as we increase the sample size. Let's calculate the standard deviations:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndist_sds <- data.frame(\n  Distribution = c(\"Non normal\", \"n = 2\", \"n = 10\", \"n = 30\"),\n  SD = c(sd(non_normal_data$x), sd(mean_n2), sd(mean_n10), sd(mean_n30))\n  )\ndist_sds\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  Distribution        SD\n1   Non normal 28.954840\n2        n = 2 20.406067\n3       n = 10  9.225895\n4       n = 30  5.056629\n```\n:::\n:::\n\n\nThe reduction of standard deviation values tends to be equal to dividing the standard deviation of the original data by the square root of the sample size (this value is also known as the *standard error*):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Standard deviation for samples of size 2\nsd(non_normal_data$x) / sqrt(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 20.47416\n```\n:::\n\n```{.r .cell-code}\n# Standard deviation for samples of size 10\nsd(non_normal_data$x) / sqrt(10)  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 9.156324\n```\n:::\n\n```{.r .cell-code}\n# Standard deviation for samples of size 30\nsd(non_normal_data$x) / sqrt(30)  \n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5.286406\n```\n:::\n:::\n\n\nThus, the central limit theorem allows us to use statistical methods based on a normal distribution with data from non-normal distributions.\n\n# Statistical Inference\n\nIt is time to put into practice the concepts we have covered so far. We are going to work with a sample of size 10 drawn from the non-normal distribution we simulated in the previous section. The objective is to infer whether the population mean is equal to 30 based on the sample mean we obtain.\n\nFirst, we take the sample and calculate its mean:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(92)\n\nsample_n10 <- sample(non_normal_data$x, size = 10)\nmean_n10   <- mean(sample_n10)\nmean_n10\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 53.70605\n```\n:::\n:::\n\n\nAs a next step we standardize this value in order to compare it with the values of a theoretical distribution. This calculation is performed by subtracting the hypothetical population mean (30) from the sample mean we obtained. The result of the above subtraction is then divided by the standard error:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nst_error  <- sd(sample_n10) / sqrt(length(sample_n10))\nstand_n10 <- (mean_n10 - 30) / st_error\nstand_n10\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.458524\n```\n:::\n:::\n\n\nSuppose we do not have all the values of the simulated population, but we do have the values of a Student's t-distribution with mean 30 and an error or standard deviation equal to that of our sample:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx_vals <- seq(0, 1, by = 0.001)\ndata_t <- data.frame(t_vals = qt(x_vals, df = 9))\nggplot(data_t, aes(t_vals)) +\n  geom_histogram(fill = \"steelblue1\", color = \"black\") +\n  xlab(\"t values\") +\n  ylab(\"Count\") +\n  theme_classic()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nWarning: Removed 2 rows containing non-finite values (`stat_bin()`).\n```\n:::\n\n::: {.cell-output-display}\n![](index_files/figure-html/t distribution-1.png){width=672}\n:::\n:::\n\n\nIn this example the `qt()` function generates standardized values by default, so no additional operations have to be performed.\n\nNow let us ask the question: What proportion of values in the t distribution are greater than the standardized value of our sample? We can answer with the following code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsum(abs(data_t$t_vals) >= stand_n10) / length(data_t$t_vals)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.03796204\n```\n:::\n:::\n\n\nOnly a small proportion of all values in the theoretical distribution are larger, in absolute terms, than the mean we obtained, so we could infer that the population mean we are interested in is other than 30.\n\nAll of the above can be done with the `t.test()` function and our sample data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(sample_n10, alternative = \"two.sided\", mu = 30)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n\n\tOne Sample t-test\n\ndata:  sample_n10\nt = 2.4585, df = 9, p-value = 0.03625\nalternative hypothesis: true mean is not equal to 30\n95 percent confidence interval:\n 31.89345 75.51865\nsample estimates:\nmean of x \n 53.70605 \n```\n:::\n:::\n\n\nThe ratio we calculated previously is better known as *p-value*. Note how the value obtained by the `t.test()` function is quite similar.\n\nIt is also important to note that the argument `alternative = \"two.sided\"` in the `t.test()` function refers to a *two-tailed hypothesis test*. That is, sufficiently large or sufficiently small sample means will lead us to conclude that the population mean is different from 30. For this reason I used the `abs()` function in the proportion calculation to include the negative and positive values of the t distribution whose absolute values were greater than the standardized sample value.\n\nHow small must the proportion of values in the theoretical distribution be to infer that our statistic is not part of that distribution? A general and widely accepted rule of thumb is usually set at a proportion of 0.05. In other words, if our *p-value* is less than 0.05 we can conclude that the population mean is significantly different from the hypothesized value.\n\n# Key points\n\n-   Statistical inference allows us to make inferences about the parameters of a population based on statistics calculated from a sample.\n-   For data on a continuous measurement scale, statistics such as mean, variance and standard deviation are usually calculated. With R code it is possible to calculate these statistics with `mean()`, `var()` and `sd()` functions.\n-   A probability distribution relates the possible values of a population to their frequencies, counts or probabilities of occurrence.\n-   The central limit theorem states that the sampling distribution of the mean tends to be normal when the sample size is large enough, even if the original data do not follow a normal distribution.\n-   The `t.test()` function can be used to make inferences about the population mean based on sample data.\n-   As a rule of thumb, if the obtained *p-value* is less than 0.05 we can state that the population mean is different from some hypothetical value.\n\nThe code on this post is licensed under the [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)\n\n[![](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](http://creativecommons.org/licenses/by/4.0/)\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}