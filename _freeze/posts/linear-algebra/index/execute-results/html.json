{
  "hash": "c4e920880d9e2b4244cac7f7bb031961",
  "result": {
    "markdown": "---\ntitle: \"Basic Linear Algebra with R\"\ncategories: [\"R\", \"tutorial\", \"linear algebra\"]\ndate: \"2023-12-20\"\ndescription: \"A brief tutorial with R code to perform some linear algebra operations\"\ntoc: true\ntoc-location: left\n---\n\n\n![](/posts/linear-algebra/images/325px-Linear_subspaces_with_shading.svg.png){fig-align=\"center\"}\n\n# What is linear algebra?\n\n<a href=\"https://www.math.ucdavis.edu/~linear/linear-guest.pdf\" target=\"_blank\">*Linear algebra*</a> *is the study of vectors and linear functions*. In this post, I'm going to show you how to perform some basic linear algebra operations with R code.\n\n# Vector operations\n\n## Vectors\n\nA vector is a mathematical object that possesses both magnitude and direction. In other words, a vector is a mathematical entity representing a physical quantity with both a size (or magnitude) and a specific orientation in space. Vectors are used to describe displacements, velocities, forces, and other concepts in mathematics, physics, and other disciplines.\n\nFormally, a vector in three-dimensional space $\\mathbb{R}^3$ can be represented as an ordered triplet of real numbers $(a, b, c)$, where $a$, $b$, and $c$ are the components of the vector along the x, y, and z axes, respectively. In general, in an $n$-dimensional space $\\mathbb{R}^n$, a vector is represented as an $n$-tuple $(a_1, a_2, \\ldots, a_n)$.\n\nVectors can also be geometrically represented as arrows in a coordinate system, where the length of the arrow represents the magnitude of the vector, and the direction of the arrow represents the orientation of the vector in space.\n\nTo define vectors, in this case in twodimensions, we can use the function `c()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define three vectors in two dimensions\na <- c(-3, 1)\nb <- c(1, -3)\nc <- c(-2, 2)\n\n# Display the vectors\na\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -3  1\n```\n:::\n\n```{.r .cell-code}\nb\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1]  1 -3\n```\n:::\n\n```{.r .cell-code}\nc\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -2  2\n```\n:::\n:::\n\n\nThe above vectors can be represented visually as follows (code not shown):\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](index_files/figure-html/2D vector-1.png){width=672}\n:::\n:::\n\n\nTo create vectors in four dimensions, we use the same procedure, but add more elements to each vector:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define two vectors in fourth dimensions\nx <- c(30, 20, 40, 10)\ny <- c(20, 15, 18, 40)\n```\n:::\n\n\nThe above procedure can be generalized to any vector in $\\mathbb{R}^n$.\n\n## Sum\n\nThe sum of two vectors is a mathematical operation that involves adding the corresponding components of the two vectors. Let $\\mathbf{A} = (a_1, a_2, \\ldots, a_n)$ and $\\mathbf{B} = (b_1, b_2, \\ldots, b_n)$ be two vectors in $\\mathbb{R}^n$. The sum of $\\mathbf{A}$ and $\\mathbf{B}$ is denoted as $\\mathbf{A} + \\mathbf{B}$ and is calculated as follows:\n\n$\\mathbf{A} + \\mathbf{B} = (a_1 + b_1, a_2 + b_2, \\ldots, a_n + b_n)$\n\nIn other words, the sum of two vectors produces a new vector whose components are the sum of the corresponding components of the original vectors.\n\nSome important properties of vector addition include commutativity ($\\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}$), associativity $(\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})$, and the existence of a zero vector ($\\mathbf{0} + \\mathbf{A} = \\mathbf{A} + \\mathbf{0} = \\mathbf{A}$, where $\\mathbf{0}$ is the zero vector).\n\nVector addition is a fundamental operation in linear algebra and is used in various contexts, such as representing displacements in physics, combining forces, and defining more advanced operations in vector spaces.\n\nWith R code you can add vectors with the same length:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx + y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 50 35 58 50\n```\n:::\n:::\n\n\nIf the vectors don't have the same length, the elements of the smallest will be recycled:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx + c(10, 10)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 40 30 50 20\n```\n:::\n:::\n\n\n## Multiplying a vector by a scalar\n\nThe multiplication of a vector by a scalar is a mathematical operation that involves multiplying each component of the vector by the scalar. Let $\\mathbf{V}$ be a vector and $k$ be a scalar; the multiplication of the vector by the scalar is denoted as $k \\cdot \\mathbf{V}$ and is calculated as follows:\n\nIf $\\mathbf{V} = (v_1, v_2, \\ldots, v_n)$, then\n\n$k \\cdot \\mathbf{V} = (k \\cdot v_1, k \\cdot v_2, \\ldots, k \\cdot v_n)$\n\nIn other words, each component of the original vector is multiplied by the scalar, producing a new vector whose components are the product of each component of the original vector by the scalar.\n\nThis operation has various properties, such as associativity $(a \\cdot b) \\cdot \\mathbf{V} = a \\cdot (b \\cdot \\mathbf{V})$, distributivity with respect to the sum of vectors $k \\cdot (\\mathbf{A} + \\mathbf{B}) = k \\cdot \\mathbf{A} + k \\cdot \\mathbf{B}$, and distributivity with respect to the sum of scalars $(a + b) \\cdot \\mathbf{V} = a \\cdot \\mathbf{V} + b \\cdot \\mathbf{V}$.\n\nTo multiply by a scalar, we use the operator `*` as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n10 * x\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 300 200 400 100\n```\n:::\n:::\n\n\n## Dot or inner product\n\nThe dot product, also known as the scalar product or inner product, is a mathematical operation between two vectors that results in a scalar (a single number). The dot product of two vectors $\\mathbf{A} = (a_1, a_2, \\ldots, a_n)$ and $\\mathbf{B} = (b_1, b_2, \\ldots, b_n)$ is commonly denoted as $\\mathbf{A} \\cdot \\mathbf{B}$ and is calculated as follows:\n\n$\\mathbf{A} \\cdot \\mathbf{B} = a_1 \\cdot b_1 + a_2 \\cdot b_2 + \\ldots + a_n \\cdot b_n$\n\nIn other words, you multiply the corresponding components of the two vectors and sum these products. The result is a scalar, not a vector.\n\nSome important properties of the dot product include commutativity ($\\mathbf{A} \\cdot \\mathbf{B} = \\mathbf{B} \\cdot \\mathbf{A}$), distributivity with respect to the sum of vectors ($\\mathbf{A} \\cdot (\\mathbf{B} + \\mathbf{C}) = \\mathbf{A} \\cdot \\mathbf{B} + \\mathbf{A} \\cdot \\mathbf{C}$), and scalar multiplication ($k \\cdot (\\mathbf{A} \\cdot \\mathbf{B}) = (k \\cdot \\mathbf{A}) \\cdot \\mathbf{B} = \\mathbf{A} \\cdot (k \\cdot \\mathbf{B})$), where (*k*) is a scalar.\n\nThe dot product has various applications in geometry, physics, and other mathematical disciplines, and it is fundamental in defining concepts such as vector length, vector projection, and angles between vectors.\n\nIn R we use the operator `%*%` to perform the dot product:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx %*% y\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,] 2020\n```\n:::\n:::\n\n\nNote that this operator returns an object with the classes \"matrix\" and \"array\":\n\n\n::: {.cell}\n\n```{.r .cell-code}\nclass(x %*% y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] \"matrix\" \"array\" \n```\n:::\n:::\n\n\nIf you need just the numeric value, use the `as.numeric()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nas.numeric(x %*% y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2020\n```\n:::\n:::\n\n\n## Norm\n\nThe norm of a vector, also known as magnitude or length, is a measure that indicates the absolute size of the vector in Euclidean space. The norm of a vector $\\mathbf{v}$ in an $n$-dimensional space, commonly denoted as $\\|\\mathbf{v}\\|$ or $\\|\\mathbf{v}\\|_2$, is calculated using the following formula:\n\n$\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2}$\n\nIn other words, the norm of a vector is the square root of the sum of the squares of its individual components. This is derived from the Euclidean distance in $n$-dimensional space.\n\nSome important properties of the norm include:\n\n1.  $\\|\\mathbf{v}\\| \\geq 0$: The norm of a vector is always non-negative.\n2.  $\\|\\mathbf{v}\\| = 0$ if and only if $\\mathbf{v} = \\mathbf{0}$, where $\\mathbf{0}$ is the zero vector.\n3.  $\\|\\alpha \\cdot \\mathbf{v}\\| = |\\alpha| \\cdot \\|\\mathbf{v}\\|$ for any scalar $\\alpha$.\n4.  Triangle Inequality: $\\|\\mathbf{u} + \\mathbf{v}\\| \\leq \\|\\mathbf{u}\\| + \\|\\mathbf{v}\\|$.\n\nWith R code, the norm or magnitude of a vector can be obtained as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsqrt(x %*% x)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         [,1]\n[1,] 54.77226\n```\n:::\n:::\n\n\nWith a vector defined as a matrix object, you can also obtain the magnitude with the function `norm` :\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnorm(as.matrix(x), type = \"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 54.77226\n```\n:::\n:::\n\n\n## Scalar Projections\n\nThe scalar projection of a vector $\\mathbf{s}$ onto another vector $\\mathbf{r}$ is the length of the projection of $\\mathbf{s}$ onto the direction of $\\mathbf{r}$. It is commonly denoted as $\\text{proj}_{\\mathbf{r}}(\\mathbf{s})$ and is calculated using the following formula:\n\n$\\text{proj}_{\\mathbf{r}}(\\mathbf{s}) = \\frac{\\mathbf{s} \\cdot \\mathbf{r}}{\\|\\mathbf{r}\\|}$\n\nWhere:\n\n-   $\\mathbf{s} \\cdot \\mathbf{r}$ denotes the dot product between the two vectors.\n\n-   $\\|\\mathbf{r}\\|$ is the norm (magnitude) of $\\mathbf{r}$.\n\nThe scalar projection of $\\mathbf{s}$ onto $\\mathbf{r}$ represents the length of the line segment connecting the origin to the point where $\\mathbf{s}$ projects onto the line following the direction of $\\mathbf{r}$.\n\nIn R, the scalar projection of a vector `s` onto a vector `r` can be obtained with the next code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr <- c(3, -4, 0)\ns <- c(10, 5, -6)\n\nr %*% s / sqrt(r %*% r)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]    2\n```\n:::\n:::\n\n\nIn a similar way, you can obtain the vector projection of `s` onto `r` using the function `norm()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nr %*% s / norm(as.matrix(r), type = \"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]    2\n```\n:::\n:::\n\n\n# Matrix operations\n\n## Matrices\n\nIn linear algebra, a matrix is a two-dimensional array of numbers, symbols, or mathematical expressions arranged in rows and columns. A matrix is typically represented by uppercase letters, such as A, B, C, etc., and its elements are denoted by indices indicating the position of a number in the matrix.\n\nFor example, a matrix A of size *m × n* is denoted as:\n\n$A = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{bmatrix}$\n\nHere, $a_{ij}$ represents the element in the *ith* row and *jth* column of the matrix. In this context, *m* is the number of rows, and *n* is the number of columns. When the number of rows is equal to the number of columns (*m* = *n*), it is called a square matrix.\n\nMatrices are used in linear algebra to represent and manipulate systems of linear equations, linear transformations, and in various contexts in mathematics, physics, statistics, computer science, and other disciplines. Basic operations with matrices include addition, subtraction, scalar multiplication, and matrix multiplication.\n\nIn R, we use the function `matrix()` to define matrices:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Define a matrix\nm <- c(7, -6, 12, 8)\nm <- matrix(m, nrow = 2, byrow = TRUE)\nm\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    7   -6\n[2,]   12    8\n```\n:::\n:::\n\n\n## Matrix multiplication by a vector\n\nMatrix-vector multiplication is a fundamental operation in linear algebra. Given a matrix $A$ of dimensions $m \\times n$ and a column vector $\\mathbf{v}$ of dimension $n \\times 1$, the product of the matrix by the vector, denoted as $A \\mathbf{v}$, results in a new column vector of dimension $m \\times 1$.\n\nThe operation is performed by multiplying each element in a matrix row by the corresponding element in the vector and summing the resulting products. The element in the $i$-th position of the resulting vector is the sum of the products of the elements in the $i$-th row of the matrix and the corresponding elements in the vector.\n\nThe mathematical expression for the multiplication of a matrix $A$ by a column vector $\\mathbf{v}$ can be written as follows:\n\n$A \\mathbf{v} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} (a_{11}v_1 + a_{12}v_2 + \\dots + a_{1n}v_n) \\\\ (a_{21}v_1 + a_{22}v_2 + \\dots + a_{2n}v_n) \\\\ \\vdots \\\\ (a_{m1}v_1 + a_{m2}v_2 + \\dots + a_{mn}v_n) \\end{bmatrix}$\n\nIt's important to note that matrix-vector multiplication is defined when the number of columns in the matrix is equal to the number of rows in the vector. In this case, the result is a new vector with the same number of rows as the matrix and the same number of columns as the original vector.\n\nTo perform this operation in R, we use the operator `%*%`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Matrix multiplication by a vector\nv <- c(5, 6)\nm %*% v\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]   -1\n[2,]  108\n```\n:::\n:::\n\n\n## Matrix multiplication\n\nMatrix multiplication is an algebraic operation that combines two matrices to produce a third matrix. Given two matrices $A$ and $B$, where matrix $A$ has dimensions $m \\times n$ and matrix $B$ has dimensions $n \\times p$, the product of $A$ and $B$, denoted as $AB$, results in a new matrix $C$ with dimensions $m \\times p$.\n\nThe operation is performed by multiplying each element in a row of the first matrix by the corresponding element in the column of the second matrix and summing the resulting products. The entry in the $i$-th row and $j$-th column of the resulting matrix $C$ is the result of this sum for the intersection of the $i$-th row of $A$ and the $j$-th column of $B$.\n\nThe mathematical expression for matrix multiplication can be represented as follows:\n\n$C_{ij} = \\sum_{k=1}^{n} A_{ik} \\cdot B_{kj}$\n\nwhere $C_{ij}$ is the element in the $i$-th row and $j$-th column of matrix $C$, $A_{ik}$ is the element in the $i$-th row and $k$-th column of matrix $A$, and $B_{kj}$ is the element in the $k$-th row and $j$-th column of matrix $B$.\n\nIt is important to note that for matrix multiplication to be defined, the number of columns in the first matrix must be equal to the number of rows in the second matrix. The resulting matrix has the number of rows of the first matrix and the number of columns of the second matrix.\n\nTo perform multiplications between matrices, we use the operator `%*%`. Always be sure that the number of columns in the first matrix is the same as the number of rows in the second:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A matrix of 2 x 3\nA <- matrix(c(1, 2, 3, 4, 0, 1), nrow = 2, byrow = TRUE)\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    0    1\n```\n:::\n\n```{.r .cell-code}\n# A matrix of 3 x 3\nB <- matrix(c(1, 1, 0, 0, 1, 1, 1, 0, 1), nrow = 3, byrow = TRUE)\nB\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    1    0\n[2,]    0    1    1\n[3,]    1    0    1\n```\n:::\n\n```{.r .cell-code}\n# Matrix multiplication\nA %*% B\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    4    3    5\n[2,]    5    4    1\n```\n:::\n:::\n\n\n## Multiplication of a matrix by a scalar\n\nScalar multiplication by a matrix is an operation in which each element of the matrix is multiplied by the given scalar. Given a scalar $c$ and a matrix $A$ of dimensions $m \\times n$, the product of $c$ by $A$, denoted as $cA$, is obtained by multiplying each element of $A$ by $c$, resulting in a new matrix of the same size $m \\times n$.\n\nThe mathematical expression for scalar multiplication by a matrix can be written as:\n\n$c \\cdot A = \\begin{bmatrix} c \\cdot a_{11} & c \\cdot a_{12} & \\dots & c \\cdot a_{1n} \\\\ c \\cdot a_{21} & c \\cdot a_{22} & \\dots & c \\cdot a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ c \\cdot a_{m1} & c \\cdot a_{m2} & \\dots & c \\cdot a_{mn} \\end{bmatrix}$\n\nWhere $a_{ij}$ represents the elements of matrix $A$. Each element of the resulting matrix $cA$ is simply the product of the scalar $c$ with the corresponding element of matrix $A$.\n\nAs with vectors, we use the operator `*` to perfrom this operation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Matrix multiplication by a scalar\n100 * A\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]  100  200  300\n[2,]  400    0  100\n```\n:::\n:::\n\n\n## Range of a matrix\n\nThe rank of a matrix is a measure indicating the maximum number of linearly independent rows or columns in the matrix. We can calculate the rank for both rectangular and square matrices. The specific definition may vary depending on whether you are dealing with the row rank or column rank, but both are related.\n\n1.  *Row Rank Definition:* The row rank of a matrix is the maximum number of linearly independent rows in the matrix.\n\n2.  *Column Rank Definition:* The column rank of a matrix is the maximum number of linearly independent columns in the matrix.\n\nIn more general terms, the rank of a matrix can be defined as the dimension of the space spanned by its row or column vectors. An important fact about the rank of a matrix is that the row rank is always equal to the column rank. This is known as the rank theorem and is a crucial property highlighting the relationship between the rows and columns of a matrix.\n\nTo calculate the rank of a matrix we use the function `qr()` as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A matrix of 3 x 3\nC <- matrix(c(1, 0, 1, -2, -3, 1, 3, 3, 0), nrow = 3, byrow = TRUE)\nC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    0    1\n[2,]   -2   -3    1\n[3,]    3    3    0\n```\n:::\n\n```{.r .cell-code}\n# Rank of matrix C\nqr(C)$rank\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2\n```\n:::\n\n```{.r .cell-code}\n# A matrix of 2 x 4 \nD <- matrix(c(1, 1, 0, 2, -1, -1, 0, -2), nrow = 2, byrow = TRUE)\nD\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    0    2\n[2,]   -1   -1    0   -2\n```\n:::\n\n```{.r .cell-code}\n# Rank of matrix D\nqr(D)$rank\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\n## Inverse of a matrix\n\nThe inverse of a matrix is a fundamental concept in linear algebra. Given a square matrix $A$, $A^{-1}$ is said to be the inverse of $A$ if the product of $A$ by $A^{-1}$ (or $A^{-1}$ by $A$) is equal to the identity matrix $I$, of the same size as $A$. Mathematically, this is expressed as:\n\n$A \\cdot A^{-1} = A^{-1} \\cdot A = I$\n\nWhere:\n* $A$ is the original matrix.\n* $A^{-1}$ is the inverse matrix.\n* $I$ is the identity matrix.\n\nIt's important to note that not all matrices have an inverse. For a matrix to have an inverse, it must be a square matrix, and it must be nonsingular, meaning its determinant should not be equal to zero.\n\nIf a matrix $A$ has an inverse, it is denoted as $A^{-1}$, and it satisfies:\n\n$A \\cdot A^{-1} = A^{-1} \\cdot A = I$\n\nThe inverse matrix has important properties, such as the inverse of the inverse ($(A^{-1})^{-1} = A$) and the inverse of the product of matrices ($(AB)^{-1} = B^{-1}A^{-1}$), among others.\n\nTo calculate the inverse of a matrix we use the `solve()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(9)\n# A matrix of 4 x 4 \nE <- matrix(\n  round(runif(16, min = 1, max = 30), 0), \n  nrow = 4, byrow = TRUE\n)\nE\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]    7    2    7    7\n[2,]   14    5   12   12\n[3,]   20   30    4    1\n[4,]   27   10   15   16\n```\n:::\n\n```{.r .cell-code}\n# Inverse of matrix E\nE_i <- solve(E)\nE_i\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n           [,1]       [,2]       [,3]       [,4]\n[1,]  -6.597222   4.472222 -0.1527778 -0.4583333\n[2,]   7.305556  -5.055556  0.1944444  0.5833333\n[3,] -30.625000  21.750000 -0.6250000 -2.8750000\n[4,]  35.277778 -24.777778  0.7222222  3.1666667\n```\n:::\n:::\n\n\n## Solving systems of linear equations\n\nA system of linear equations is a set of two or more linear equations that share a common set of variables. Each linear equation within the system represents a linear relationship between the variables. In general, a system of linear equations can be expressed as follows:\n\n$\\begin{cases} a_{11}x_1 + a_{12}x_2 + \\ldots + a_{1n}x_n = b_1 \\\\ a_{21}x_1 + a_{22}x_2 + \\ldots + a_{2n}x_n = b_2 \\\\ \\vdots \\\\ a_{m1}x_1 + a_{m2}x_2 + \\ldots + a_{mn}x_n = b_m \\end{cases}$\n\nWhere:\n\n* $x_1, x_2, \\ldots, x_n$ are the variables of the system.\n\n* $a_{ij}$ are the coefficients accompanying the variables.\n\n* $b_i$ are the constant terms in each equation.\n\n* The system has $m$ linear equations.\n\nThe typical goal when solving a system of linear equations is to find the values of the variables $x_1, x_2, \\ldots, x_n$ that satisfy all the equations simultaneously. These values represent the solution to the system. Depending on the nature of the system, it may have a unique solution, no solution, or infinitely many solutions. Common methods for solving systems of linear equations include substitution, elimination, and the use of matrices.\n\nNow let us define the following system of linear equations. We want to obtain the values of $a$, $b$, and $c$ that satisfied the system:\n\n-   $(1): a + b + c = 15$\n-   $(2): 3a + 2b + c = 28$\n-   $(3): 2a + b + 2c = 23$\n\nFirst, define a matrix with the system coefficients and a vector with the results or constants for each equation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Coefficient matrix\nC <- matrix(c(1, 1, 1, 3, 2, 1, 2, 1, 2), nrow = 3, byrow = TRUE)\nC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    3    2    1\n[3,]    2    1    2\n```\n:::\n\n```{.r .cell-code}\n# Constant vector \nr <- c(15, 28, 23)\nr\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 15 28 23\n```\n:::\n:::\n\n\nNext, obtain the inverse of the coefficient matrix using the `solve()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Coefficient matrix inverse\nC_inv <- solve(C)\nC_inv\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,] -1.5  0.5  0.5\n[2,]  2.0  0.0 -1.0\n[3,]  0.5 -0.5  0.5\n```\n:::\n:::\n\n\nTo calculate the solution, we multiply the previous inverse matrix by the constant vector:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Solution\ns <- C_inv %*% r\ns\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]    3\n[2,]    7\n[3,]    5\n```\n:::\n:::\n\n\nThus the solution to our system of equations is as follows:\n\n-   $a = 3$\n-   $b = 7$\n-   $c = 5$\n\n## Determinant of a matrix\n\nThe determinant of a matrix is a scalar associated with that matrix and is commonly denoted as $\\text{det}(A)$ or $|A|$. The process for calculating the determinant varies depending on the size of the matrix.\n\n1.  *First-Order Matrix:*\n\nFor a $1 \\times 1$ matrix, which is simply a number, the determinant is equal to that number.\n\n    $\\text{det}([a]) = a$\n\n2.  *Second-Order Matrices:*\n\nFor a $2 \\times 2$ matrix:\n\n    $\\text{det}\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = ad - bc$\n\n3.  *Higher-Order Matrices:*\n\nFor an $n \\times n$ square matrix $A$, the determinant can be calculated using cofactor expansion or more advanced methods such as diagonalization.\n\nThe general formula for the determinant of an $n \\times n$ matrix $A$ can be expressed recursively in terms of minors and cofactors:\n\n    $\\text{det}(A) = \\sum_{i=1}^{n} (-1)^{i+j} \\cdot a_{ij} \\cdot \\text{det}(M_{ij})$\n\nWhere:\n\n* $a_{ij}$ is the element in row $i$, column $j$ of matrix $A$.\n* $\\text{det}(M_{ij})$ is the determinant of the minor obtained by removing row $i$ and column $j$ from matrix $A$.\n\nTo carry out this operation, we use the function `det()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    3    2    1\n[3,]    2    1    2\n```\n:::\n\n```{.r .cell-code}\ndet(C)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] -2\n```\n:::\n:::\n\n\n## Transpose of a Matrix\n\nThe transpose of a matrix is an operation that involves swapping its rows and columns. It is commonly denoted by $A^T$ or $A'$. If $A$ is an $m \\times n$ matrix, then the transpose $A^T$ is an $n \\times m$ matrix obtained by exchanging the rows and columns of the original matrix.\n\nMathematically, if $A$ has elements $a_{ij}$, then the transpose $A^T$ has elements $b_{ij}$ where $b_{ij} = a_{ji}$. In other words, the element in row $i$, column $j$ of $A^T$ is the same as the element in row $j$, column $i$ of $A$.\n\nFor example, if $A$ is:\n\n$A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}$\n\nThen, the transpose $A^T$ would be:\n\n$A^T = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}$\n\nSome important properties of the transpose include:\n\n1.  $(A^T)^T = A$\n2.  $(cA)^T = cA^T$, where $c$ is a constant.\n3.  $(A + B)^T = A^T + B^T$, where $A$ and $B$ are matrices compatible for addition.\n\nWe use the `t()` function to obtain the transpose of a matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nC\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    3    2    1\n[3,]    2    1    2\n```\n:::\n\n```{.r .cell-code}\nt(C)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    1    3    2\n[2,]    1    2    1\n[3,]    1    1    2\n```\n:::\n:::\n\n\n## Identity matrix\n\nThe identity matrix, commonly denoted by $I_n$ or simply $I$, is an $n \\times n$ square matrix that has ones on its main diagonal (from the upper-left to the lower-right) and zeros in the rest of its elements. In other words, an identity matrix is a square matrix where all elements outside the main diagonal are zero, and all elements on the main diagonal are one.\n\nThe general form of an $n \\times n$ identity matrix is:\n\n$I_n = \\begin{bmatrix} 1 & 0 & \\ldots & 0 \\\\ 0 & 1 & \\ldots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\ldots & 1 \\end{bmatrix}$\n\nWhere $n$ represents the size of the identity matrix.\n\nIt is important to note that the distinctive property of the identity matrix is that when any square matrix $A$ is multiplied by the appropriate identity matrix, the result is the same matrix $A$. In mathematical terms, if $A$ is an $n \\times m$ matrix, then $I_n \\times A = A \\times I_m = A$, provided that the dimensions are compatible for multiplication.\n\nTo define a identity matrix we use the `diag()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Identity matrix of 2 x 2\ndiag(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n```\n:::\n\n```{.r .cell-code}\n# Identity matrix of 4 x 4\ndiag(4)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1\n```\n:::\n:::\n\n\n## Eigenvalues and eingenvectors\n\nThe concepts of eigenvalues and eigenvectors are fundamental in linear algebra and apply to linear transformations represented by matrices.\n\n1.  *Eigenvalues:*\n\nGiven a square matrix $A$, an eigenvalue $\\lambda$ is a scalar such that when multiplied by a nonzero vector $v$, the result is simply the same vector scaled by $\\lambda$:\n\n    $A \\mathbf{v} = \\lambda \\mathbf{v}$\n\nHere, $\\mathbf{v}$ is the corresponding eigenvector associated with the eigenvalue $\\lambda$. It's important to note that for an eigenvalue and its corresponding eigenvector, the multiplication of matrix $A$ by vector $\\mathbf{v}$ results in a mere scaling of the vector by the eigenvalue.\n\n2.  *Eigenvectors:*\n\nGiven an eigenvalue $\\lambda$, the corresponding eigenvector $\\mathbf{v}$ is a nonzero vector that satisfies the equation:\n\n    $A \\mathbf{v} = \\lambda \\mathbf{v}$\n\nIn other words, when matrix $A$ is multiplied by the eigenvector $\\mathbf{v}$, the result is simply the same vector scaled by the eigenvalue $\\lambda$.\n\nEigenvectors provide special directions in which the linear transformation represented by matrix $A$ has only a scaling effect. The magnitude of this scaling is determined by the corresponding eigenvalue.\n\nWe use the function `eigen()` to obtain the eigenvectors and eigenvalues of a given matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A matrix of 2 x 2\nA <- matrix(c(3, 4, 0, 5), nrow = 2, byrow = TRUE)\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    3    4\n[2,]    0    5\n```\n:::\n\n```{.r .cell-code}\n# eigen function\nA_ei <- eigen(A)\n\n# eigenvalues\nA_lmd <- A_ei$values\nA_lmd\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 5 3\n```\n:::\n\n```{.r .cell-code}\n# eigenvectors\nA_ev <- A_ei$vectors\nA_ev\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1] [,2]\n[1,] 0.8944272    1\n[2,] 0.4472136    0\n```\n:::\n:::\n\n\nInterestingly, the original matrix can be decomposed as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA_ev %*% diag(A_lmd) %*% solve(A_ev)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2]\n[1,]    3    4\n[2,]    0    5\n```\n:::\n:::\n\n\n## Gram-Schmidt process\n\nThe Gram-Schmidt process is a method used in linear algebra to take a set of linearly independent vectors and construct from them an orthogonal or orthonormal set. This process is useful in various contexts, such as matrix diagonalization, solving systems of linear equations, and QR factorization.\n\nGiven a set of vectors $\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\}$ in a Euclidean vector space, the Gram-Schmidt process generates an orthogonal set $\\{\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_n\\}$ or an orthonormal set $\\{\\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_n\\}$ of vectors.\n\nThe Gram-Schmidt process is valuable in situations where we need to work with sets of vectors that are not orthogonal initially but desires to obtain orthogonal sets to simplify calculations and analyses.\n\nTo perform this process, you can use the `gramSchmidt()` function from `pracma` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#install.packages(\"pracma\")\nlibrary(pracma)\n\n# A matrix of 3 x 3\nA <- matrix(c(2, -2, 18, 2, 1, 0, 1, 2, 0), nrow = 3, byrow = TRUE)\nA\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    2   -2   18\n[2,]    2    1    0\n[3,]    1    2    0\n```\n:::\n\n```{.r .cell-code}\n# gramSchmidt function\nA_gs <- gramSchmidt(A)\n\n# Orthogonalized matrix\nA_gs$Q\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n          [,1]       [,2]       [,3]\n[1,] 0.6666667 -0.6666667  0.3333333\n[2,] 0.6666667  0.3333333 -0.6666667\n[3,] 0.3333333  0.6666667  0.6666667\n```\n:::\n\n```{.r .cell-code}\n# Upper triangular matrix \nA_gs$R\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    3    0   12\n[2,]    0    3  -12\n[3,]    0    0    6\n```\n:::\n:::\n\n\nNote that the above operation allows us to decompose the original matrix such that:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nA_gs$Q %*% A_gs$R\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1] [,2] [,3]\n[1,]    2   -2   18\n[2,]    2    1    0\n[3,]    1    2    0\n```\n:::\n:::\n\n\n# Calculation of mean and variance with linear algebra operations\n\nStatistics like the mean and the variance can be easily done with matrix operations.\n\n## Mean\n\nLet's simulate some data and obtain the mean with the `mean()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data\nset.seed(5)\ny <- runif(100, min = 10, max = 35)\n\n# mean function\nmean(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 22.96092\n```\n:::\n:::\n\n\nUsing vector multiplication, the mean can be calculated as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Sample size\nyl <- length(y) \n\n# Vector of ones of the same length as y\nv1 <- rep(1, yl)\n\n# Calculation of the mean\ny_mean <- v1 %*% y / yl\ny_mean\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         [,1]\n[1,] 22.96092\n```\n:::\n:::\n\n\n## Sample variance\n\nSomething similar can be done with the sample variance. First, let's calculate the sample variance with the `var()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# var function\nvar(y)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 57.33112\n```\n:::\n:::\n\n\nNow let's calculate the sample variance with vector multiplication:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Differences between the data and the sample mean\nr <- y - as.numeric(y_mean)\n\n# Calculation of the variance\nr %*% r / (yl - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         [,1]\n[1,] 57.33112\n```\n:::\n:::\n\n\nThis post is licensed under the [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)\n\n[![](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](http://creativecommons.org/licenses/by/4.0/)\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}