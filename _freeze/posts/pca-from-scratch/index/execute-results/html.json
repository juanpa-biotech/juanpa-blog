{
  "hash": "ea7076017f95153d937ecb7aa0424873",
  "result": {
    "markdown": "---\ntitle: \"R tutorial: Principal Component Analysis from scratch\"\ncategories: [\"R\", \"tutorial\", \"pca\", \"principal component analysis\", \"linear algebra\"]\ndate: \"2023-10-30\"\ndescription: 'How to perform PCA step by step using just basic linear algebra functions and operations'\ntoc: true\ntoc-location: left\n---\n\n\n![](img/cover.jpeg){fig-align=\"center\" width=\"450\"}\n\nAll the data and code of this post can be download on the next repository : [pca-from-scratch](https://github.com/juanpa-biotech/pca-from-scratch)\n\n# What is principal component analysis?\n\n<a href=\"https://en.wikipedia.org/wiki/Principal_component_analysis\" target=\"_blank\">Principal component analysis (PCA)</a> is an exploratory data analysis based on the reduction of dimensions of a given data set. The general idea is to reduce the data set so that it has fewer dimensions while retaining as much information as possible.\n\nPCA allows us to make visual representations in two dimensions and to check for groupings or differences in the data related to different states, treatments, etc. In addition, we can get some clue as to which variables in the data are responsible for the visual differences.\n\nIt is important to emphasize that the PCA is not used exclusively for the above, and given that it is an exploratory analysis, the similarities or differences observed in the data should be considered with caution and in the context from which they originate.\n\n# A simple case with two-dimensional data\n\nLet's start with a simple example with two-dimensional data, which will allow us to visualize some of the basic concepts behind PCA and then generalize what we have learned to data with more than two dimensions.\n\n## Packages\n\nFor this post we will use some functions included in `ggplot2`, `ggpubr`, `readr`, `purrr` and `dplyr`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Run the next line if you have not installed the packages:\n# install.packages(c(\"ggplot2\", \"ggpubr\", \"readr\", \"purrr\", \"dplyr\"))\n\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(dplyr)\n```\n:::\n\n\n## Data simulation\n\nFirst let's simulate data with two dimensions. To do this let's make the second variable directly dependent on the first one and store everything in a data frame:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1) # For data reproducibility\n\n# Variable 1\nvar_1 <- rnorm(50, 50, sd = 3)\n\n# Variable 2\nvar_2 <- .5*var_1 + rnorm(50, sd = sqrt(3))\n\n# Both variables in a data frame\ndata_set_1 <- tibble(var_1, var_2)\n\nhead(data_set_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 2\n  var_1 var_2\n  <dbl> <dbl>\n1  48.1  24.7\n2  50.6  24.2\n3  47.5  24.3\n4  54.8  25.4\n5  51.0  28.0\n6  47.5  27.2\n```\n:::\n:::\n\n\nIf we make a scatter plot we can observe the clear dependence between both variables:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A scatter plot with the two simulated variables\nggplot(data_set_1, aes(x = var_1, y = var_2)) +\n  geom_point(color = \"blue\", size = 2) +\n  xlab(\"Variable 1\") +\n  ylab(\"Variable 2\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/scatter plot for 2d data-1.png){width=672}\n:::\n:::\n\n\n## First step: Center each variable\n\nThe first step in the PCA is to center each variable with respect to its average value:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndata_set_1 <- data_set_1 %>% \n  mutate(varc_1 = var_1 - mean(var_1), varc_2 = var_2 - mean(var_2))\n\nhead(data_set_1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 6 × 4\n  var_1 var_2 varc_1  varc_2\n  <dbl> <dbl>  <dbl>   <dbl>\n1  48.1  24.7 -2.18  -0.604 \n2  50.6  24.2  0.250 -1.14  \n3  47.5  24.3 -2.81  -1.02  \n4  54.8  25.4  4.48   0.0829\n5  51.0  28.0  0.687  2.62  \n6  47.5  27.2 -2.76   1.85  \n```\n:::\n:::\n\n\nThis did not change the relative position between each point, so the data look the same:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data_set_1, aes(x = varc_1, y = varc_2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/scatter plot for centered data-1.png){width=672}\n:::\n:::\n\n\n## Second step: Obtaining the covariance matrix\n\nThe next step is to obtain the covariance matrix for the previous data set. For this we perform a matrix multiplication with the data as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Select only the centered variables\ndata_set_2 <- data_set_1 %>% \n  select(varc_1, varc_2) %>% \n  as.matrix()\n\n# Calculate the covariance matrix\ncov_m <- (t(data_set_2) %*% data_set_2) / (nrow(data_set_2) - 1) \n\ncov_m\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         varc_1   varc_2\nvarc_1 6.220943 2.946877\nvarc_2 2.946877 4.207523\n```\n:::\n:::\n\n\nIn the resulting matrix the diagonal contains the variance of each variable while the values outside the diagonal are the covariances between the variables (see figure below):\n\n![](img/image_1.jpg){fig-align=\"center\" width=\"450\"}\n\nWe can obtain the covariance matrix directly with the `cov()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncov(data_set_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         varc_1   varc_2\nvarc_1 6.220943 2.946877\nvarc_2 2.946877 4.207523\n```\n:::\n:::\n\n\nOr with the crossprod() function as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncrossprod(data_set_2) / (nrow(data_set_2) - 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         varc_1   varc_2\nvarc_1 6.220943 2.946877\nvarc_2 2.946877 4.207523\n```\n:::\n:::\n\n\n## Third step: Obtain the eigenvalues and eigenvectors of the covariance matrix\n\nPrincipal components represent the directions in the data that explain the maximum amount of variance. They are \"lines\" that collect most of the information in the data. These directions can be obtained by calculating the eigenvalues and eigenvectors of the covariance matrix:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Use eigen() to obtain eigenvectors and eigenvalues\ncov_e <- eigen(cov_m)\n\n# Eigenvectors\ne_vec <- cov_e$vectors\n\n# Eigenvalues\ne_val <- cov_e$values\n```\n:::\n\n\nThe span of each eigenvector can be considered the \"line\" that captures most of the variation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# First eigenvector \nev_1 <- e_vec[,1]\n\n# Slope of the first eigenvector\nev1_m <- ev_1[2] / ev_1[1]\n\n# Second eigenvector \nev_2 <- e_vec[,2]\n\n# Slope of the second eigenvector\nev2_m <- ev_2[2] / ev_2[1]\n\n# Scatter plot showing the span of both eigenvectors \nggplot(data.frame(data_set_2), aes(x = varc_1, y = varc_2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  geom_abline(slope = ev1_m, color = \"blue\", linewidth = 0.7) +\n  geom_abline(slope = ev2_m, color = \"red\", linewidth = 0.7) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/scatter plot with each principal component for 2d data-1.png){width=672}\n:::\n:::\n\n\nAs can be seen, there is one eigenvector for each variable in the data set, in this case two. Also note that the eigenvectors are perpendicular:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Multiply both eigenvectors \nev_1 %*% ev_2\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n     [,1]\n[1,]    0\n```\n:::\n:::\n\n\nAs for the eigenvalues, their numerical values are equal to the sum of squares of the distances of each projected data point in the corresponding principal component. This sum of squares is maximized in the first principal component.\n\n## Fourth step: Make a Scree Plot\n\nDividing each eigenvalue by *n - 1* (*n* is the number of rows in the original data) will give an estimate of the variance represented by each principal component. The sum of all variances (the total variance) can be used to calculate the percentage of variation and visualized with a Scree Plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the estimated variance for each eigenvalue\ne_var <- e_val / (nrow(data_set_2) - 1)\n\n# Data frame with variance percentages\nvar_per <- tibble(\n  PC  = c(\"PC1\", \"PC2\"),\n  PER = c(e_var) * 100 / sum(e_var) # Calculate the percentage\n    )\n\n# Scree plot \nggplot(var_per, aes(x = PC, y = PER)) +\n  geom_col(width = 0.5, color = \"black\") +\n  xlab(\"Principal component\") +\n  ylab(\"Percentage of variation (%)\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/scree plot 2d data-1.png){width=672}\n:::\n:::\n\n\n## Fifth step: Obtain the loadings of each variable.\n\nThe eigenvectors obtained using the `eigen()` function are normalized. This means that their length is equal to 1:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Norm of the first eigenvector\nnorm(as.matrix(ev_1), \"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n\n```{.r .cell-code}\n# Norm of the second eigenvector\nnorm(as.matrix(ev_2), \"F\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 1\n```\n:::\n:::\n\n\nThe elements of each eigenvector are also called loadings and can be interpreted as the contribution of each variable in the data set to the corresponding principal component or, more strictly, as the coefficients of the linear combination of the original variables from which the principal components are constructed.\n\nWe can make a table with these values and see the contributions of each variable to each principal component:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data frame with both eigenvectors\nloads <- tibble(\n  VAR   = c(\"var_1\", \"var_2\"),\n  PC1 = ev_1, # First eigenvecor\n  PC2 = ev_2  # Second eigenvectors\n)\n\nloads\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n# A tibble: 2 × 3\n  VAR      PC1    PC2\n  <chr>  <dbl>  <dbl>\n1 var_1 -0.813  0.582\n2 var_2 -0.582 -0.813\n```\n:::\n:::\n\n\nThe above can be useful in data with many dimensions to get an idea of which variables cause the groupings or differences in the PCA plot.\n\n## Sixth step: Representing data in fewer dimensions\n\nIf we change the base of the original data to that indicated by the eigenvectors, we are basically rotating the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Change the basis of the original data \ndata_set_3 <- data_set_2 %*% solve(e_vec) # Inverse of eigenvectors matrix\n\n# Scatter showing the rotation \nggplot(data.frame(data_set_3), aes(X1, X2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  xlab(\"PC1 (78.8%)\") +\n  ylab(\"PC2 (21.2%)\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/change of basis-1.png){width=672}\n:::\n:::\n\n\nComparing the two graphs gives us an idea of how the data have been rotated once we change the base:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Scatter plot with the centered data \nplot_data <- ggplot(data.frame(data_set_2), aes(x = varc_1, y = varc_2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  ylim(c(-8, 8.5)) +\n  ggtitle(\"Original Data\") +\n  theme_classic()\n\n# Scatter plot with the rotated data\nplot_rotation <- ggplot(data.frame(data_set_3), aes(X1, X2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  xlab(\"PC1 (78.8%)\") +\n  ylab(\"PC2 (21.2%)\") +\n  ylim(c(-8, 8.5)) +\n  ggtitle(\"Change of Basis to Eigenvectors\") +\n  theme_classic()\n\n# Both graphs side by side\nggarrange(plot_data, plot_rotation)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/compare rotation and original 2d data-1.png){width=672}\n:::\n:::\n\n\nSince principal component 1 (PC1) explains most of the variance in the data, we can omit principal component 2 (PC2) and represent each point in a single dimension, here as red dots:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data points just from PC 1\ndata_pc1 <- data.frame(v1 = data_set_3[,1], v2 = rep(0, nrow(data_set_3)))\n\n# Scatter plot showing the projected points from PC1 (red points)\nggplot(data.frame(data_set_3), aes(X1, X2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_point(data = data_pc1, aes(v1, v2), color = \"red\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  xlab(\"PC1 (78.8%)\") +\n  ylab(\"PC2 (21.2%)\") +\n  ylim(c(-8, 8.5)) +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/projections on PC1-1.png){width=672}\n:::\n:::\n\n\nThe above ideas can be used in data with many variables to reduce the dimensions and make two-dimensional representations of the data.\n\n# Generalization to data with more than two dimensions\n\nLet us now apply the same procedure to a data set with more variables, namely <a href=\"https://ucphchemometrics.com/2023/06/01/wine-samples-analyzed-by-gc-ms-and-ft-ir-instruments/\" target=\"_blank\">wine samples analyzed by GC-MS and FT-IR instruments</a>, where the following responses were measured:\n\n![](img/image_2.jpg){fig-align=\"center\" width=\"350\"}\n\nThe wine samples come from Argentina, Chile, Australia and South Africa:\n\n![](img/image_3.jpg){fig-align=\"center\" width=\"230\"}\n\nThe first six rows of the data set look as follows:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Variable names\nvar_names <- read_csv(\"https://raw.githubusercontent.com/juanpa-biotech/pca-from-scratch/master/data/Label_Pred_values_IR.csv\")\nvar_names <- names(var_names)\n\n# Wine labels\nwine_label <- read_csv(\"https://raw.githubusercontent.com/juanpa-biotech/pca-from-scratch/master/data/Label_Wine_samples.csv\")\nwine_label <- unname(unlist(wine_label))\n\n# Wine data set\ndata_set_wine <- read_csv(\n  \"https://raw.githubusercontent.com/juanpa-biotech/pca-from-scratch/master/data/Pred_values.csv\", \n  col_names = var_names\n)\nrow.names(data_set_wine) <- wine_label\n\nhead(as.matrix(data_set_wine))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Ethanol TotalAcid VolatileA MalicAcid   pH LacticAc ReSugar CitricAcid\nARG-BNS1   13.62      3.54      0.29      0.89 3.71     0.78    1.46       0.31\nARG-DDA1   14.06      3.74      0.59      0.24 3.73     1.25    2.42       0.18\nARG-FFL1   13.74      3.27      0.47     -0.07 3.87     1.13    1.52       0.39\nARG-FLM1   13.95      3.66      0.47      0.09 3.79     1.00    4.17       0.41\nARG-ICR1   14.47      3.66      0.38      0.61 3.70     0.81    1.25       0.14\nARG-SAL1   14.61      3.45      0.52      0.16 3.92     1.76    1.40       0.10\n            CO2 Density FolinC Glycerol Methanol TartaricA\nARG-BNS1  85.61    0.99  60.92     9.72     0.16      1.74\nARG-DDA1 175.20    1.00  70.64    10.05     0.20      1.58\nARG-FFL1 513.74    0.99  63.59    10.92     0.18      1.24\nARG-FLM1 379.40    1.00  73.30     9.69     0.23      2.26\nARG-ICR1 154.88    0.99  71.69    10.81     0.20      1.22\nARG-SAL1 156.30    0.99  71.79    10.19     0.19      0.90\n```\n:::\n:::\n\n\nNote that the wine samples are marked as the names of the rows in the data frame and in turn wine samples have associated values for each response (in the columns). Pay attention to this when calculating covariance matrices or using functions to perform PCA, do you want to reduce the dimensions with respect to rows or with respect to columns? Here we are interested in reducing the number of dimensions with respect to the responses (columns) and try to detect if there are any similarities or differences between the wine samples.\n\n## First step: Center the data\n\nSubtract the average of each variable (columns) and divide by its standard deviation:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Means for each variable\nvar_means <- unlist(map(data_set_wine, mean))\n\n# Standard deviation for each variable\nvar_sd <- unlist(map(data_set_wine, sd))\n\n# Center each variable\ndata_set_wine_2 <- map2(\n  data_set_wine, var_means, .f = function(x, mean) x - mean\n  )\n\n# Devide by the standard deviation of each variable\ndata_set_wine_2 <- map2(\n  data_set_wine_2, var_sd, .f = function(x, sd) x / sd\n)\n\n# Make a matrix from the previous list\ndata_set_wine_2 <- as.matrix(data.frame(data_set_wine_2))\n```\n:::\n\n\nEach row of the transformed data corresponds to the same wine sample as the original data.\n\nThe first six rows of the transformed data look like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhead(data_set_wine_2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n         Ethanol   TotalAcid  VolatileA  MalicAcid         pH    LacticAc\n[1,] -0.63678489 -0.40870999 -0.7811641  2.2627949 0.18823560 -0.88226788\n[2,]  0.29224039  0.01096889  1.3231961 -0.5694123 0.40060357  0.38388752\n[3,] -0.38341492 -0.97527630  0.4814521 -1.9201573 1.88717938  0.06061381\n[4,]  0.05998306 -0.15690246  0.4814521 -1.2229986 1.03770749 -0.28959935\n[5,]  1.15792167 -0.15690246 -0.1498560  1.0427673 0.08205162 -0.80144937\n[6,]  1.45351898 -0.59756526  0.8321787 -0.9179917 2.41810183  1.75780072\n        ReSugar CitricAcid        CO2    Density     FolinC     Glycerol\n[1,] -0.5311188  1.1594919 -1.6049333 -0.7844233 -0.1371321 -0.525885112\n[2,]  0.1996454  0.1084153 -0.9482843  1.2458487  1.2376582 -0.160707985\n[3,] -0.4854461  1.8063082  1.5330409 -0.7844233  0.2405111  0.802031815\n[4,]  1.5317676  1.9680124  0.5483973  1.2458487  1.6138874 -0.559083800\n[5,] -0.6909735 -0.2149929 -1.0972195 -0.7844233  1.3861700  0.680306454\n[6,] -0.5767916 -0.5384012 -1.0868116 -0.7844233  1.4003137 -0.005784995\n       Methanol   TartaricA\n[1,] -1.1081037  0.43877622\n[2,] -0.0870238 -0.03865555\n[3,] -0.5975636 -1.05319836\n[4,]  0.6787860  1.99042973\n[5,] -0.0870238 -1.11287729\n[6,] -0.3422939 -2.06774119\n```\n:::\n:::\n\n\nDividing by standard deviations is a way of giving each variable equal importance despite its range, magnitude and/or scale of measurement. In addition to dividing by the standard deviation, it is possible to apply other transformations depending on the data. See the resources at the end of this post if you are interested.\n\n## Second step: Calculating the covariance matrix\n\nWe multiply the data (in matrix form) by its transpose:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate the covariance matrix\ncov_wine <- (t(data_set_wine_2) %*% data_set_wine_2) / \n  (nrow(data_set_wine_2) - 1)\n\ncov_wine[1:5, 1:5]\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n             Ethanol  TotalAcid  VolatileA   MalicAcid         pH\nEthanol   1.00000000  0.3262321  0.2028382  0.04166778  0.1697347\nTotalAcid 0.32623209  1.0000000  0.4660575 -0.26067574 -0.3518303\nVolatileA 0.20283816  0.4660575  1.0000000 -0.74628880  0.3011611\nMalicAcid 0.04166778 -0.2606757 -0.7462888  1.00000000 -0.2929542\npH        0.16973470 -0.3518303  0.3011611 -0.29295421  1.0000000\n```\n:::\n:::\n\n\nHere we only show the first five rows and columns of the covariance matrix. If you want to display the whole matrix a good way is to use the `View()` function.\n\nEach value of the cov_wine matrix has the same interpretation as in the example with two variables, the values on the diagonal are the variances of each variable, and the values outside the diagonal are the covariances between variables. As can be seen, all variances are equal to 1. This is precisely the effect of centering and dividing by the standard deviation.\n\n## Third step: Obtain the eigenvalues and eigenvectors of the covariance matrix.\n\nLet's use the `eigen()` function to obtain the eigenvectors and their eigenvalues:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# eigen() to obtain eigenvalues and eigenvectors\neg_wine <- eigen(cov_wine)\n\n# Eigenvalues\neg_vals <- eg_wine$values\n\n# Eigenvectors\neg_vecs <- eg_wine$vectors\n```\n:::\n\n\nThe number of vectors and eigenvalues is the same as the number of variables in the original data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Number of eigenvalues\nlength(eg_vals)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14\n```\n:::\n\n```{.r .cell-code}\n# Number of eigenvectors\nncol(eg_vecs)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 14\n```\n:::\n:::\n\n\n## Fourth step: Make a Scree Plot\n\nWe calculated the percentage of variation of each component and made a Scree plot:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Calculate variances from each eigenvalue\neg_vars <- eg_vals / (nrow(data_set_wine_2) - 1)\n\n# Data frame with variance percenatges\nvars_perc <- tibble(\n  PC  = unlist(map(1:14, function(x) paste0(\"PC\", x))),\n  PER = round((eg_vars * 100) / sum(eg_vars), 4)\n    )\n\n# Scree plot\nggplot(\n  vars_perc, \n  aes(x = reorder(PC, order(PER, decreasing = TRUE)), y = PER)\n       ) +\n  geom_col(width = 0.5, color = \"black\") +\n  xlab(\"Principal component\") +\n  ylab(\"Percentage of variation (%)\") +\n  theme_classic()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/scree plor for wine data-1.png){width=672}\n:::\n:::\n\n\n## Fifth step: Obtain the loadings of each variable\n\nThe elements of each eigenvector represent the weight of each variable in the corresponding principal component:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Data frame with loading scores\nloads_wine <- data.frame(eg_vecs)\ncolnames(loads_wine) <- vars_perc$PC\nrownames(loads_wine) <- var_names\n\nhead(loads_wine)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n                 PC1        PC2         PC3         PC4          PC5\nEthanol   -0.2050720 -0.3452884  0.28833198 -0.33833697  0.001102612\nTotalAcid -0.1457210 -0.4545803 -0.06374908  0.42139270  0.041508881\nVolatileA  0.2959952 -0.4418538  0.03338975  0.08430388 -0.010891943\nMalicAcid -0.3401714  0.3173079  0.17907611 -0.04002812 -0.252870227\npH         0.2413635 -0.1030772 -0.04459655 -0.66472873 -0.170592677\nLacticAc   0.3462627 -0.3745378 -0.16738658  0.11850141 -0.108643313\n                  PC6          PC7         PC8        PC9        PC10\nEthanol   -0.02992588  0.106645514  0.40565762  0.2220668 -0.07364281\nTotalAcid -0.10721872  0.109080109 -0.15801515  0.1647662 -0.12640759\nVolatileA  0.12676353 -0.117134957 -0.01066998 -0.2323371  0.19698524\nMalicAcid  0.09495473  0.247290148 -0.35870967  0.1792700 -0.27203174\npH         0.01717167 -0.191605073 -0.30040129  0.3352703  0.28827462\nLacticAc   0.05496474  0.008009813 -0.10169448  0.1377493 -0.28605639\n                 PC11        PC12        PC13        PC14\nEthanol    0.52247393 -0.22486081  0.29025392 -0.04195694\nTotalAcid -0.09209975 -0.25488777 -0.01537897  0.65083548\nVolatileA  0.19114512 -0.33800898 -0.59864002 -0.27743084\nMalicAcid -0.07338526 -0.53559762 -0.18854505 -0.23066288\npH        -0.21200577 -0.09998927 -0.02838824  0.28223548\nLacticAc  -0.32247769 -0.11583440  0.49548494 -0.45696212\n```\n:::\n:::\n\n\nMaking a scatter plot with these values can help determine correlations between variables and/or explain why a particular clustering is observed in principal component scatter plots (next section):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Scatter plot with loadings of PC1 and PC2\nld_pc12 <- ggplot(loads_wine, aes(PC1, PC2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_text(aes(label = rownames(loads_wine)), hjust = -.2) +\n  ggtitle(\"Loadings for PC1 and PC2\") +\n  xlim(c(-.7, .7)) +\n  ylim(c(-.7, .7)) +\n  xlab(\"PC1 (24.4%)\") +\n  ylab(\"PC2 (21.3%)\") +\n  theme_classic()\n\n# Scatter plot with loadings of PC3 and PC4\nld_pc34 <- ggplot(loads_wine, aes(PC3, PC4)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_text(aes(label = rownames(loads_wine)), hjust = -.2) +\n  ggtitle(\"Loadings for PC3 and PC4\") +\n  xlim(c(-.7, .7)) +\n  ylim(c(-.7, .7)) +\n  xlab(\"PC3 (17.5%)\") +\n  ylab(\"PC4 (10.0%)\") +\n  theme_classic()\n\n# Both graphs side by side\nggarrange(ld_pc12, ld_pc34)  \n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plot loading scores-1.png){width=672}\n:::\n:::\n\n\n## Sixth step: Representing data in fewer dimensions\n\nIdeally, if PC1 and PC2 picked up most of the variation, say more than 90%, it would be possible to make a good representation of the data in a two-dimensional scatter plot. But since real data are almost never ideal, in this case PC1, PC2, PC3 and PC4 account for 73% of the variation. Let us try to observe clustering using two scatter plots, the first with PC1 and PC2, and the second with PC3 and PC4.\n\nFirst, we change the basis of the transformed data to that indicated by the eigenvectors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Change the basis of the data\ndata_set_wine_eb <- data_set_wine_2 %*% solve(eg_vecs)\n\n# Transfrom to a data frame\ndata_set_wine_eb <- data.frame(data_set_wine_eb)\ncolnames(data_set_wine_eb) <- vars_perc$PC\n\n# Add a column with the origin of each wine sample\ndata_set_wine_eb <- data_set_wine_eb %>% \n  mutate(\n    WineSample = unlist(map(wine_label, function(x) substr(x, 1, 3)))\n    ) %>% \n  relocate(WineSample)\n\nhead(data_set_wine_eb)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n  WineSample        PC1        PC2        PC3        PC4         PC5\n1        ARG -0.8708602  1.4271679  1.0042682 -0.4970542 -2.40194963\n2        ARG  0.9214933 -0.8023313  0.9170387 -0.3788901  0.04278493\n3        ARG  2.0993602 -1.1582579  0.1104490 -0.8588001  0.36878072\n4        ARG  2.6784566  0.6585042 -0.5914731 -1.1987690  0.55471303\n5        ARG -0.3093155 -0.7061611  0.9719731 -0.8791527 -1.48911945\n6        ARG  0.6656888 -1.9938918  2.0606329 -0.4319683 -0.58522246\n          PC6        PC7        PC8         PC9         PC10        PC11\n1 -0.50073335  0.7380182 -0.7113971  1.28454429  0.141197785  0.91258285\n2 -1.11669152 -0.6204253  0.1024002 -0.66345802  0.217715309 -0.44779807\n3 -0.01460188  0.6155704  1.2988198 -0.31647718 -1.945505546  1.56333545\n4 -1.77225038  1.1845744  0.7517347 -0.01741376 -0.458803511 -1.38374975\n5  0.58458758 -1.3525017  0.3597807  1.06873458  0.656603378 -0.04068736\n6  0.76213010 -0.9997045  2.4380628 -1.36498248 -0.004949351  1.27477965\n        PC12        PC13        PC14\n1 -0.0961029 -0.01151848  0.12807023\n2  0.6191680 -1.13863351 -0.22032005\n3 -0.2843139  1.44732864  0.08942808\n4  0.1136096 -1.16558478 -0.14095094\n5 -0.4097057  0.16732603 -0.70481769\n6 -0.5940964 -0.93709414 -0.52629641\n```\n:::\n:::\n\n\nNow, let's make both scatter plots taking only the values of PC1, PC2, PC3 and PC4:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Scatter plot for PC1 and PC2\npc12 <- ggplot(\n  data_set_wine_eb, \n  aes(PC1, PC2, color = WineSample, shape = WineSample)\n) +\n  geom_point(size = 3) +\n  ggtitle(\"PC1 and PC2\") +\n  xlab(\"PC1 (24.4%)\") +\n  ylab(\"PC2 (21.3%)\") +\n  theme_classic() +\n  theme(legend.position = \"none\") \n\n# Scatter plot for PC3 and PC4\npc34 <- ggplot(\n  data_set_wine_eb, \n  aes(PC3, PC4, color = WineSample, shape = WineSample)\n) +\n  geom_point(size = 3) +\n  scale_color_discrete(\n    name = \"Wine origin\", \n    labels = c(\"Argentina\", \"Australia\", \"Chile\", \"South Africa\")\n    ) +\n  scale_shape_discrete(\n    name = \"Wine origin\", \n    labels = c(\"Argentina\", \"Australia\", \"Chile\", \"South Africa\")\n  ) +\n  ggtitle(\"PC3 and PC4\") +\n  xlab(\"PC3 (17.5%)\") +\n  ylab(\"PC4 (10.0%)\") +\n  theme_classic()\n\n# Both graphs side by side\nggarrange(pc12, pc34)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/scatters plot for PC 1 2 3 4-1.png){width=672}\n:::\n:::\n\n\n# Key points\n\n-   Principal component analysis (PCA) is a methodology that allows us to reduce the dimensionality of data while preserving important information.\n\n-   PCA is used to visualize data with many dimensions in two-dimensional plots, identify groupings and differences, and determine which variables are most relevant. However, its interpretation should be done with caution due to its exploratory nature.\n\n-   PCA can be performed in several steps with basic functions and linear algebra operations using the following steps:\n\n    1.  Center each variable.\n    2.  Calculate the covariance matrix.\n    3.  Obtain eigenvalues and eigenvectors representing directions of maximum variation.\n    4.  Make a Scree Plot to visualize the variance explained by each component.\n    5.  Obtain the loadigns of the variables in the principal components.\n    6.  Represent the data in reduced dimensions, generally in two dimensions.\n\n# Additional Resources\n\nIt is important to know linear algebra to understand and visualize the core of PCA. The 3Blue1Brown video series is an excellent starting point for learning linear algebra:\n\n<a href=\"https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab\" target=\"_blank\">Essence of linear algebra</a>.\n\nIntuitive explanations of the PCA can be found in other channels:\n\n<a href=\"https://www.youtube.com/watch?v=g-Hb26agBFg\" target=\"_blank\">Principal Component Analysis (PCA)</a>.\n\n<a href=\"https://www.youtube.com/watch?v=FgakZw6K1QQ&t=914s\" target=\"_blank\">StatQuest: Principal Component Analysis (PCA), Step-by-Step</a>.\n\nIf you would like to see some examples of the application of this analysis to experimental data here are two articles oriented to chemometrics and metabolomics:\n\n<a href=\"https://pubs.rsc.org/en/content/articlelanding/2014/ay/c3ay41907j\" target=\"_blank\">Principal component analysis</a>.\n\n<a href=\"https://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-7-142\" target=\"_blank\">Centering, scaling, and transformations: improving the biological information content of metabolomics data</a>.\n\nThe code on this post is licensed under the [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)\n\n[![](https://img.shields.io/badge/License-CC%20BY%204.0-lightgrey.svg)](http://creativecommons.org/licenses/by/4.0/)\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}