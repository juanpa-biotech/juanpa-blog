[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data analysis, science and other stuff ü§ì",
    "section": "",
    "text": "Hello, welcome to my blog.\nMy name is Juan Pablo, I have a PhD in Biotechnology from the Universidad Aut√≥noma Metropolitana, Mexico City.\nIn this blog I share posts with topics of my interest, such as data analysis with R code, science communication and other things that will take shape little by little.\nSome time ago I maintained a blog called ‚ÄúR in the Lab‚Äù focused exclusively on the analysis with R of data obtained in the laboratory of chemical and biological sciences, but after some stumbles I decided to start again with a blog where I could write about any topic.\nPreviously I was using the ‚Äúblogdown‚Äù package, but I must admit that things became a bit tedious, especially the maintenance and the inclusion of new posts. In the search for alternatives I found the fabulous post by Rebecca Barter - Thanks, Quarto, for saving my blog!, which gave me the idea for the design of my new blog.\nIf you have any questions about any of the posts please email me at jpch_26@outlook.com or jpch_biotech@outlook.com. In the About the author section you will find my social networks, you can also contact me there!\nThank you very much for visiting my blog, I hope you find it useful, see you next time! ü§ì\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nBasic Linear Algebra with R\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\nlinear algebra\n\n\n\n\nA brief tutorial with R code to perform some linear algebra operations\n\n\n\n\n\n\nDec 20, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWhat is biotechnology?\n\n\n\n\n\n\n\nscience communication\n\n\nbiotechnology\n\n\n\n\nA brief explanation of what biotechnology is and some of its applications\n\n\n\n\n\n\nDec 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR tutorial: Principal Component Analysis with Metabolomics Data\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\npca\n\n\nprincipal component analysis\n\n\nmetabolomics\n\n\n\n\nAn R code tutorial on how to perform principal component analysis on plant metabolomics data.\n\n\n\n\n\n\nDec 1, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR tutorial: Principal Component Analysis from scratch\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\npca\n\n\nprincipal component analysis\n\n\nlinear algebra\n\n\n\n\nHow to perform PCA step by step using just basic linear algebra functions and operations\n\n\n\n\n\n\nOct 30, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR tutorial: Simultaneus Optimization of Several Response Variables\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\nsimultaneus optimization\n\n\nresponse surface design\n\n\ndesirability package\n\n\n\n\nA tutorial with R code to perform simultaneous optimization of several response variables\n\n\n\n\n\n\nOct 24, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPrickly pear, a sweet source of nutrients\n\n\n\n\n\n\n\nscience communication\n\n\nprickly pear\n\n\n\n\nSome facts about prickly pears\n\n\n\n\n\n\nOct 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR Tutorial: Analysis of results of a nested experimental design\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\nstatistical inference\n\n\nnested design\n\n\n\n\nA tutorial with R code for analyzing the results of nested designs\n\n\n\n\n\n\nOct 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWhat is science? - The way we understand and model reality\n\n\n\n\n\n\n\nscience communication\n\n\nscientific method\n\n\nscientific thinking\n\n\n\n\nSome ideas about science\n\n\n\n\n\n\nSep 28, 2023\n\n\n\n\n\n\n  \n\n\n\n\nStatistical Inference with R\n\n\n\n\n\n\n\nR\n\n\nstatistical inference\n\n\nstatistics\n\n\npopulation\n\n\nsample\n\n\nparameters\n\n\nmean\n\n\nvariance\n\n\nstandard deviation\n\n\n\n\nSome statistical inference concepts and terms explained using R\n\n\n\n\n\n\nSep 27, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the author",
    "section": "",
    "text": "Hello! Welcome to my personal blog.\nMy name is Juan Pablo. I have a PhD in Biotechnology from the Universidad Aut√≥noma Universidad Aut√≥noma Metropolitana.\nIn this blog I write about various topics of my interest such as data analysis and science in general. I hope you find it useful.\nYou can contact me through my email or my social networks:\njpch_26@outlook.com\njpch_biotech@outlook.com\nLinkedIn\nTwitter\nMy certifications:\nThe R Programming Environment\nIntroducci√≥n a Data Science: Programaci√≥n Estad√≠stica con R\nProgramming in R for Data Science\nExperimentation for Improvement\nProfessional Certificate: Data Analysis for Life Sciences\nI have also published some scientific articles, which can be consulted in my profile at Scholar Google.\nAll text, code and data in the tutorials are freely available under the Creative Commons Attribution 4.0 International License.\nThank you so much for visiting my blog! ü§ì"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/statistical-inference/index.html",
    "href": "posts/statistical-inference/index.html",
    "title": "Statistical Inference with R",
    "section": "",
    "text": "Perfection is always impossible; always it‚Äôs an approximation"
  },
  {
    "objectID": "posts/statistical-inference/index.html#introduction",
    "href": "posts/statistical-inference/index.html#introduction",
    "title": "Statistical Inference with R",
    "section": "Introduction",
    "text": "Introduction\nFormally, statistical inference can be defined as the process through which inferences about a population are made based on certain statistics calculated from a sample of data drawn from that population.\nIf your job is related to any kind of science I‚Äôm positive you know that statistical inference plays an important role in your results validations. A single statement is often used as irrefutable evidence that you have succeeded in your research: ‚Äúthis results showed significant differences (P &lt; 0.05)‚Äù. But does anything showing significant differences really mean anything?\nI‚Äôm going to try to answer these and more questions with some help, of course, of R code."
  },
  {
    "objectID": "posts/statistical-inference/index.html#populations-samples-parameters-and-statistics",
    "href": "posts/statistical-inference/index.html#populations-samples-parameters-and-statistics",
    "title": "Statistical Inference with R",
    "section": "Populations, Samples, Parameters and statistics",
    "text": "Populations, Samples, Parameters and statistics\nFrom Cambridge Dictionary an inference is a guess that you make or an opinion that you form based on the information that you have. Statistically, the objective of an inference is to draw conclusions about population from a sample.\nBy population I mean the complete set of objects of your interest and can be the trees from a particular species or even the elements from a continuous process like chip cookies in a cookies factory (yeah, I like chip cookies). Most of the time, you don‚Äôt have access to the entire population simply because is too big and it would be impractical analyze each element or, as I said, the population can be considered like a continuous.\nThis is the place where the statistical inference can help. All that is needed is a sample of the population, which is called a sample. For this you must be sure that you sampling process ensures that each element in the population has the same chance to be selected. Once you have your sample you may be interested in some population property, so you proceed to measure this feature on each sample element. This property can be the height, some gene expression, the amount of sugar, the hardness, etc.\nFinally, you calculate a number that summarize the measured feature of your sample. These numbers are usually the mean, the variance, and the standard deviation, which as a whole are referred as statistics.\nWith some assumptions, from your statistics you can infer the population parameters. In other words, from your statistics you can estimate the mean, standard deviation and variance of the whole population.\nSee the next figure where I illustrate all process with cookies:"
  },
  {
    "objectID": "posts/statistical-inference/index.html#sample-mean",
    "href": "posts/statistical-inference/index.html#sample-mean",
    "title": "Statistical Inference with R",
    "section": "Sample Mean",
    "text": "Sample Mean\nYou can calculate the sample mean with the next equation:\n\n\n\n\n\nThe previous is the sum (summation) of all sample measurements divided by the sample size (n). i is the index of each element in your sample.\nWith R code in our previous sample:\n\nms_trees &lt;- sum(sample_trees) / length(sample_trees)\nms_trees\n\n[1] 15.552"
  },
  {
    "objectID": "posts/statistical-inference/index.html#sample-variance",
    "href": "posts/statistical-inference/index.html#sample-variance",
    "title": "Statistical Inference with R",
    "section": "Sample Variance",
    "text": "Sample Variance\nFor the sample variance you use the next equation:\n\n\n\n\n\nThis is the summation of the differences between each measurement in the sample and the sample mean. Each difference is squared, so negative and positive differences will not cancel each other. Then you divide the sum by the sample size minus one.\nWith R code:\n\nvs_trees &lt;- sum((sample_trees - ms_trees)^2) / (length(sample_trees) - 1)\nvs_trees\n\n[1] 10.32456\n\n\nIf you are wondering why we have divided by n - 1, I can briefly tell you that this way offers the best estimation of the population variance. You can also check out the next nice page: Why divide by N - 1 when calculating the variance of a sample?. To obtain the standard deviation you simply calculate the squared root of the variance, this way you have a dispersion sample measurement in the original measurement units.\n\nss_trees &lt;- sqrt(vs_trees)\nss_trees\n\n[1] 3.213186"
  },
  {
    "objectID": "posts/statistical-inference/index.html#population-mean",
    "href": "posts/statistical-inference/index.html#population-mean",
    "title": "Statistical Inference with R",
    "section": "Population mean",
    "text": "Population mean\nTo calculate population mean we use the next equation:\n\n\n\n\n\nYou can see that this equation is pretty the same as sample mean, but here N refers to the entire population size. Let‚Äôs calculate the mean in our previous data.\n\nmp_trees &lt;- sum(population_trees) / length(population_trees)\nmp_trees\n\n[1] 14.89803\n\n\nInstead of the previous operation you can use the mean() function with the same purpose:\n\nmean(population_trees)\n\n[1] 14.89803"
  },
  {
    "objectID": "posts/statistical-inference/index.html#population-variance",
    "href": "posts/statistical-inference/index.html#population-variance",
    "title": "Statistical Inference with R",
    "section": "Population variance",
    "text": "Population variance\nThe equation for population variance:\n\n\n\n\n\nNote that here we divide by the entire population size N, not by n-1 as in sample variance. Let‚Äôs calculate this parameter in our previous data:\n\nsum_square &lt;- sum((population_trees - mean(population_trees))^2)\npv_trees &lt;- sum_square / length(population_trees)\npv_trees\n\n[1] 15.39988\n\n\nYou can also calculate this parameter directly with variance() function:\n\nvar(population_trees)\n\n[1] 15.41016\n\n\nThere‚Äôs a sightly difference, but nothing to be worried about.\nCalculating standard deviation is also easy with sd() function:\n\nsd(population_trees)\n\n[1] 3.925577\n\n\nNote that population mean and standard deviation are close to those specified in the data simulation code rnorm(1500, mean = 15, sd = 4).\nIt is also important to mention that the mean(), var() and sd() functions can also be used on any sample data. You can try this as an exercise."
  },
  {
    "objectID": "posts/statistical-inference/index.html#mean-distribution",
    "href": "posts/statistical-inference/index.html#mean-distribution",
    "title": "Statistical Inference with R",
    "section": "Mean distribution",
    "text": "Mean distribution\nThe concept of distribution can also be applied to statistics like mean. From my cookie population let‚Äôs take repeatedly samples of size 5 and then make an histogram:\n\nset.seed(151) # For reproducibility\n\n# 1000 samples\nn_samples &lt;- 1000\n\n# Empty vector where all means will be stored \nmean_cookies &lt;- vector(\"numeric\", length = n_samples)\n\n# Take a random sample of size 5, calculate the mean and save it in previous vector\n# Repeat the process 1000 times\nfor (i in 1:n_samples) {\n  mean_cookies[i] &lt;- mean(sample(diameter$D, size = 5)) \n}\n\n# Make a data frame with all means from samples of size 5\nmean_cookies &lt;- tibble(mean_n5 = mean_cookies)\n\n# Histogram\nfreq_hist_mean &lt;- mean_cookies %&gt;% \n  ggplot(aes(x = mean_n5)) +\n  geom_histogram(color = \"black\", fill = \"white\", binwidth = 0.1) +\n  xlab(\"Diameter (cm)\") +\n  ylab(\"Count\") +\n  theme_classic() \nfreq_hist_mean\n\n\n\n\n\n\n\n\nThe previous code can be repeated with other sample sizes. This will be important in a later section (Central Limit Theorem)."
  },
  {
    "objectID": "posts/statistical-inference/index.html#the-normal-distribution",
    "href": "posts/statistical-inference/index.html#the-normal-distribution",
    "title": "Statistical Inference with R",
    "section": "The normal distribution",
    "text": "The normal distribution\nSince it is rare to know the actual distribution of data, we have to resort to theoretical distributions. There are a lot of such distributions and undoubtedly the normal distribution is the most important in statistical inference. This distribution applies to continuous data (length, weight, temperature, etc.) and is defined by the following equation:\n\n\n\n\n\nA graph of the above equation looks like this (with mean equal to zero and standard deviation equal to one):"
  },
  {
    "objectID": "posts/statistical-inference/index.html#the-students-t-distribution",
    "href": "posts/statistical-inference/index.html#the-students-t-distribution",
    "title": "Statistical Inference with R",
    "section": "The Student‚Äôs t Distribution",
    "text": "The Student‚Äôs t Distribution\nWhen you have small sample sizes and you don‚Äôt know the real deviation standard of the population, you can use the t distribution to made a good supposition of the real distribution. The shape of this distribution depends on the sample size, and for big sample sizes the t distribution tends to be normal:\n\n\n\n\n\n\n\n\n\nThe black line corresponds to a sample size of 2, the red line to a sample size of 4, the blue one to a size of 10, and the green one to a size of 50."
  },
  {
    "objectID": "posts/statistical-inference/index.html#sample-mean-or-sample-average",
    "href": "posts/statistical-inference/index.html#sample-mean-or-sample-average",
    "title": "Statistical Inference with R",
    "section": "Sample Mean or Sample Average",
    "text": "Sample Mean or Sample Average\nWe can calculate the sample mean with the following equation:\n\n\n\n\n\nThe above is the sum (summation) of all the measurements in the sample divided by the sample size (n). i is the index of each item or measurement.\nWith R code in our previous sample:\n\nms_trees &lt;- sum(sample_trees) / length(sample_trees)\nms_trees\n\n[1] 15.552\n\n\nWe can directly use the mean() function with our data to calculate the mean:\n\nmean(sample_trees)\n\n[1] 15.552"
  },
  {
    "objectID": "posts/statistical-inference/index.html#sample-variance-and-dtandard-deviation",
    "href": "posts/statistical-inference/index.html#sample-variance-and-dtandard-deviation",
    "title": "Statistical Inference with R",
    "section": "Sample Variance and Dtandard Deviation",
    "text": "Sample Variance and Dtandard Deviation\nFor the sample variance you use the next equation:\n\n\n\n\n\nThis is described as the sum of the differences between each sample measurement and the sample mean. Each difference is squared, so negative and positive differences do not cancel each other out. The sum is then divided by the sample size minus one.\nWith R code:\n\nvs_trees &lt;- sum((sample_trees - ms_trees)^2) / (length(sample_trees) - 1)\nvs_trees\n\n[1] 10.32456\n\n\nIf you are wondering why we have divided by n - 1, I can briefly tell you that this way offers the best estimation of the population variance. You can also check out the next page: Why divide by N - 1 when calculating the variance of a sample?.\nTo obtain the standard deviation, it is sufficient to calculate the square root of the variance, thus providing a measure of the dispersion of the sample in the original units of measurement:\n\nss_trees &lt;- sqrt(vs_trees)\nss_trees\n\n[1] 3.213186\n\n\nAs in the case of the mean, with R code we can obtain the variance and standard deviation directly with the var() and sd() functions, respectively:\n\n#Sample varianza\nvar(sample_trees)\n\n[1] 10.32456\n\n# Standard deviation \nsd(sample_trees)\n\n[1] 3.213186"
  },
  {
    "objectID": "posts/statistical-inference/index.html#mean-or-average-of-the-population",
    "href": "posts/statistical-inference/index.html#mean-or-average-of-the-population",
    "title": "Statistical Inference with R",
    "section": "Mean or average of the population",
    "text": "Mean or average of the population\nTo calculate population mean we use the next equation:\n\n\n\n\n\nYou can see that this equation is quite similar to the sample mean, but here N refers to all elements of the population. Let‚Äôs calculate the mean in our simulated data:\n\nmp_trees &lt;- sum(population_trees) / length(population_trees)\nmp_trees\n\n[1] 14.89803\n\n\nInstead of the previous operation you can use the mean() function with the same purpose:\n\nmean(population_trees)\n\n[1] 14.89803"
  },
  {
    "objectID": "posts/statistical-inference/index.html#variance-and-standard-deviation-of-the-population",
    "href": "posts/statistical-inference/index.html#variance-and-standard-deviation-of-the-population",
    "title": "Statistical Inference with R",
    "section": "Variance and Standard Deviation of the Population",
    "text": "Variance and Standard Deviation of the Population\nThe equation for population variance:\n\n\n\n\n\nNote that here we divide by the total population size N, not by n-1 as in the sampling variance. Let us calculate this parameter in our previous data:\n\nsum_square &lt;- sum((population_trees - mean(population_trees))^2)\npv_trees &lt;- sum_square / length(population_trees)\npv_trees\n\n[1] 15.39988\n\n\nThe base functions of R do not have one that calculates the population variance, but this can be solved by multiplying the value obtained with var()by (n-1)/n:\n\nvarp_trees &lt;- var(population_trees)\nvarp_trees &lt;- varp_trees * (length(population_trees) - 1) / length(population_trees)\nvarp_trees\n\n[1] 15.39988\n\n\nTo calculate the standard deviation it would be sufficient to obtain the square root of the previous value:\n\nsqrt(varp_trees)\n\n[1] 3.924268\n\n\nNote that the population mean and standard deviation are close to those specified in the code we used to simulate the data rnorm(1500, mean = 15, sd = 4)."
  },
  {
    "objectID": "posts/statistical-inference/index.html#sampling-mean-distribution",
    "href": "posts/statistical-inference/index.html#sampling-mean-distribution",
    "title": "Statistical Inference with R",
    "section": "Sampling Mean Distribution",
    "text": "Sampling Mean Distribution\nThe concept of distribution can also be applied to statistics such as the mean. To illustrate the above for our population of cookies let‚Äôs repeatedly take samples of size 5 and make a histogram with the averages of these samples:\n\nset.seed(151) # For reproducibility\n\n# 1000 samples\nn_samples &lt;- 1000\n\n# Empty vector where all means will be stored \nmean_cookies &lt;- vector(\"numeric\", length = n_samples)\n\n# Take a random sample of size 5, calculate the mean and save it in previous vector\n# Repeat the process 1000 times\nfor (i in 1:n_samples) {\n  mean_cookies[i] &lt;- mean(sample(diameter$D, size = 5)) \n}\n\n# Make a data frame with all means from samples of size 5\nmean_cookies &lt;- data.frame(mean_n5 = mean_cookies)\n\n# Histogram\nfreq_hist_mean &lt;- mean_cookies %&gt;% \n  ggplot(aes(x = mean_n5)) +\n  geom_histogram(color = \"black\", fill = \"white\", binwidth = 0.1) +\n  xlab(\"Diameter (cm)\") +\n  ylab(\"Count\") +\n  theme_classic() \nfreq_hist_mean\n\n\n\n\n\n\n\n\nThe previous code can be repeated with other sample sizes and this will be important in a later section (Central Limit Theorem)."
  },
  {
    "objectID": "posts/statistical-inference/index.html#students-t-distribution",
    "href": "posts/statistical-inference/index.html#students-t-distribution",
    "title": "Statistical Inference with R",
    "section": "Student‚Äôs t distribution",
    "text": "Student‚Äôs t distribution\nWhen sample sizes are small and the true standard deviation of the population is not known, Student‚Äôs t-distribution can be used to make a good guess of the distribution of the data. The shape of this distribution depends on the sample size and for large sample sizes the t-distribution tends to be normal:\n\n\n\n\n\n\n\n\n\nThe black line corresponds to a sample size of 2, the red line to a sample size of 4, the blue one to a size of 10, and the green one to a size of 50."
  },
  {
    "objectID": "posts/what-is-science/index.html",
    "href": "posts/what-is-science/index.html",
    "title": "What is science? - The way we understand and model reality",
    "section": "",
    "text": "When you want to awaken people‚Äôs interest in science, it is important to explain in a general way what it consists of. I do not think I have enough experience to give a detailed explanation of what science is, so I will limit myself to describing what it represents for me and some related concepts.\n\nWhat is science?\nScience can be defined as a human activity in charge of gathering knowledge and describing the different phenomena that make up reality in all its extension and complexity.\nKnowing and describing different natural phenomena, including social phenomena, gives us the possibility to develop and improve solutions that satisfy diverse human needs. In this way we can use scientific knowledge to influence reality itself: we cure diseases, improve processes that did not work well before, send satellites to outer space and many other things. At the beginning, when scientific knowledge is produced, it may not have any use, but over time some use may be found, which may depend on advances in other areas of science. Also, of course, all of the above also depends on the branch of science to which we are referring.\nScience, above all, is a social activity and, like all human beings, people who do science can and are susceptible to making mistakes. It is worth noting that unlike other human activities, such as religion or politics, science has the necessary mechanisms to detect and correct errors, biases, omissions and other mistakes. Science is undoubtedly an activity that requires the people involved to be self-critical and always willing to improve based on their mistakes.\nAlthough science is not a panacea, a cure for all the ills and problems that afflict humanity, it offers us an objective way to bring prosperity to every human being.\n\n\nThe scientific method\nHow is science done? Each branch of science has its own methods for generating knowledge. To do science we can follow some general guidelines, but not recipes that must be followed point by point. Below I list several steps that we can follow to generate scientific knowledge, but, in line with what I have just mentioned, we must bear in mind that there may be intermediate steps or even some points can be eliminated or modified depending on the area of knowledge we want to address.\n\nRecognize a question. This may be related to some unexplained phenomenon or a problem to which we wish to provide a solution.\nMake an adequate conjecture, a hypothesis, of the possible answers to our question. This is usually done through literature research or with the help of other means.\nPredict the consequences of the hypothesis. Once we have researched the possible answers, we make a prediction of what they would imply if they were actually true.\nWe conduct several experiments to test our predictions. From these experiments we obtain data or records. This step is important because it helps us test the hypothersis against the reality.\nIf our data or records do not agree with our previous predictions, we go back on the fly and look for or generate another hypothesis that might provide the answer to our question.\n\n\n\n\nFlow chart of the scientific method\n\n\nAnd what conditions must be met for something to be considered science? There are two key aspects:\n\nThe observations and/or experiments produced when investigating a particular phenomenon must be reproducible. That is, each time we repeat the experiment and our measurements (or records) under the same conditions, the results should be very similar to each other, even if the research is attempted to be reproduced in different places or countries.\nEvery scientific proposition must be falsifiable, which means that we must be able to formulate experiments whose results would deny the explanations given by such propositions. If it is not possible to formulate, at least with imagination, experiments aimed at demonstrating that what is said by a science is false, then it is quite possible that we are talking not of a science, but of a pseudoscience.\n\n\n\nScientific measurements\nMeasurement plays a central role in science. A measurement, in very general terms, is the assignment of a number to a certain phenomenon in nature. Measurements must be objective and do not depend on the judgment of the person making them. For the mass of an object, for example, we can use a balance, which will show us a number with some associated unit (grams, kilograms, etc.). Undoubtedly, an important part of the development of science is to find good and rigorous ways of making measurements.\n\n\n\nSome basic measuring instruments\n\n\nIt is important to mention that, depending on the phenomenon, there may not be rigorous ways to measure it. How do we measure some of the phenomena described by the social sciences?\n\n\nHypothesis, law and theory\nAs we saw earlier, a hypothesis is an assumption that can be substantiated in research through various means. When a hypothesis is sufficiently tested and no contradiction is found, it is considered a law or principle. A set of concepts, abstractions and laws related to a natural phenomenon becomes a theory. Theories can be considered as the highest level of scientific knowledge, since from these theories research problems can be posed and thus generate more knowledge.\nIt should be noted that there is no hierarchical relationship between law and theory. A theory is not below a law, nor vice versa. In the case of laws, they usually establish the relationship between different concepts and are simpler than theories. Also, depending on the branch of science, laws may not be as rigorous; a law is not the same in biology as in physics, for example. As for theories, they explain a set of phenomena (they are broader than laws) and are susceptible to change and improve over time based on the evidence that is generated from them or with the help of other branches of science.\n\n\nMyths about scientific research\nThere are some beliefs that somehow contribute to people‚Äôs lack of interest in science, mainly two:\n\nScientific research is very complicated and difficult. False, any human being can do research if they follow the right process for the problem they want to address. If we think about it carefully we are always doing research in our daily life.\nResearch is not linked to the everyday world. Also false, many of the conveniences and services we enjoy today have been derived from scientific research: smartphones, internal combustion engines, medicines, nylon, among many other products.\n\nPersonally, I believe that these misconceptions are due to the inability of research centers, universities and governments to communicate science to as many people as possible. Added to this is the superfluous treatment that the sensationalist media give to science and its findings.\n\n\nDeduction and induction\nDeduction and induction play a fundamental role in scientific work.\n\nDeduction. Reaching a particular conclusion from general premises. This is what we do when we formulate hypotheses from existing knowledge or when we make predictions based on established theories and laws.\nInduction. Reaching a general conclusion from observation of particular cases. The conclusion is considered probable, but not necessarily certain since there is always the possibility that new observations or evidence will disprove the conclusion. This is what we do when we perform experimentation to test our hypotheses.\n\nFrom both definitions we can infer that deduction and induction are processes that feed back on each other: we derive hypotheses from existing knowledge (deduction), we test these hypotheses again and again, improving at each step the initial hypothesis (induction). When sufficiently tested, the hypothesis becomes part of the theory that will allow us to generate new hypotheses, thus repeating the process of deduction and induction.\n\n\nFinal reflection\nThe development of science depends and has depended on the contribution of many people throughout history. It is a constantly changing human activity where its different branches feed back to improve our understanding of the world and the universe of which we are a part.\nScientific thinking is within everyone‚Äôs reach, it is not exclusive to universities or special beings. To a greater or lesser extent we can all put into practice the scientific method to ask and solve questions about what surrounds us.\n\n\nTo learn more about this topic\nBelow is a list with a couple of references and web pages that may be of help if you wish to delve a little deeper into the subject of this publication.\nDahlstrom, M.F. (2014) ‚ÄòUsing narratives and storytelling to communicate science with nonexpert audiences‚Äô, Proceedings of the National Academy of Sciences, 111(supplement_4), pp.¬†13614‚Äì13620. Available at: https://doi.org/10.1073/pnas.1320645111.\nRose, S.P.R. (2003) ‚ÄòHow to (or not to) communicate science‚Äô, Biochemical Society Transactions, 31(2), pp.¬†307‚Äì312. Available at: https://doi.org/10.1042/bst0310307.\nscientific method (2023) Encyclop√¶dia Britannica. Available at: https://www.britannica.com/science/scientific-method (Accessed: 27 September 2023).\nWhat Is Science? (2021) NASA Science. Space Place. Explore Earth and Space! Available at: https://spaceplace.nasa.gov/science/en/ (Accessed: 27 September 2023).\nThe text in this publication is free to use under license Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nested-designs/index.html",
    "href": "posts/nested-designs/index.html",
    "title": "R Tutorial: Analysis of results of a nested experimental design",
    "section": "",
    "text": "A nested design is a type of experimental design in which the levels of one factor are hierarchically nested within the levels of another factor. For example, let‚Äôs imagine that we are interested in the effect of a type of drug on the expression of a specific gene. For this we design a nested experiment with mice as experimental units (in the figure two mice per treatment), where we include a control. From each mouse we took three cells and in each one we evaluated the expression of the gene twice (technical repetitions).\n\n\n\n\n\nAs shown in the figure, our design has a hierarchical appearance.\nIn this type of design we distinguish between two types of factors: fixed and random. A fixed factor is one that has discrete or finite values, while a random factor can take many values. In our example the drug factor would be considered a fixed factor and the mouse, cell and repeated measurements factors would be considered random factors. Note how the random factors are similar, such as mouse and cell, but not identical to each other and these are successively nested until the fixed factor drug.\nNow let‚Äôs see how we can analyze the results of this type of experimental design with the help of R code. It is important to mention that I took as a basis the publication in nature methods: Nested designs and replicated the example shown using, of course, R code. In addition, I added an extra drug and performed a multiple comparisons test to establish significant differences between the means of each treatment.\nIf you are interested about how I simulated the data, please take a look at the code in the data_simulation script found in the repository of this tutorial: (link)."
  },
  {
    "objectID": "posts/nested-designs/index.html#anova-table",
    "href": "posts/nested-designs/index.html#anova-table",
    "title": "R Tutorial: Analysis of results of a nested experimental design",
    "section": "ANOVA Table",
    "text": "ANOVA Table\nAs can be seen in our graph, there seems to be a difference between the effect of drug 2 with the other two levels of this factor (drug 1 and control). We can define if there are significant differences by means of an ANOVA table and subsequently a multiple comparisons test.\nLet‚Äôs obtain the ANOVA table with the GAD package, first we have to specify the fixed and random factors:\n\ndrug &lt;- as.fixed(mice_data$A)\nmice &lt;- as.random(mice_data$B)\ncell &lt;- as.random(mice_data$C)\n\nTo fit the linear model we must take into account the relationship between our response and the previously specified factors:\n\ndata_aov &lt;- aov(\n  expr ~ drug + mice:drug + cell:mice:drug,\n  data = mice_data\n  )\n\nThe term mice:drug denotes the variability of mice within each treatment, and the term cell:mice:drug denotes the variability of cells within each mouse and in turn within each treatment.\nTo display the ANOVA table we use the gad() function:\n\ngad(data_aov)\n\nAnalysis of Variance Table\n\nResponse: expr\n                Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndrug             2 743.44  371.72  12.796  0.001058 ** \ndrug:mice       12 348.59   29.05   5.163 7.413e-06 ***\ndrug:mice:cell  60 337.59    5.63  11.153 &lt; 2.2e-16 ***\nResidual       150  75.67    0.50                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe gad() function distinguishes between fixed and random effects, as well as the nested structure between these factors, so it makes corrections to calculate the F-ratios. The reason for this is that, depending on whether we are considering fixed or random effects, the expected values for the error mean squares (EM) change as follows:\n\n\n\n\n\nIn the above equations it is possible to observe the nested structure in the mean squares. Also note how for the case of treatments (\\(MS_A\\)) the expected values add up to the contribution to the variation of mice and cells. Therefore, it is necessary to divide \\(MS_A\\) by \\(MS_B\\) to obtain the F-ratio and infer differences between treatments.\nImportantly, in the case of technical replicates the variability of this random factor is properly estimated by the mean square of the Residual term in our ANOVA table."
  },
  {
    "objectID": "posts/nested-designs/index.html#estimation-of-the-variability-of-each-factor",
    "href": "posts/nested-designs/index.html#estimation-of-the-variability-of-each-factor",
    "title": "R Tutorial: Analysis of results of a nested experimental design",
    "section": "Estimation of the variability of each factor",
    "text": "Estimation of the variability of each factor\nWhen dealing with random factors, we are mainly interested in estimating their contribution to the variability of the response, as opposed to fixed factors where we are interested in estimating their effect on the population mean.\nWith the lme4 package we can estimate the contribution to variability of mice, cells and properly the error term (technical replicates). First we need to convert the data type in columns A, B and C into factors and then use the lmer() function as follows:\n\nmice_data3 &lt;- mice_data %&gt;% \n  mutate(A = as.factor(A), B = as.factor(B), C = as.factor(C))\n\ndata_lme &lt;- lmer(expr ~ 1 + A + (1|B:A) + (1|C:B:A), data = mice_data3)\n\nNote that fitting the linear model with the lmer() function requires a somewhat different syntax than that used with aov() and gad(). To show the contribution to variability of each factor we use the summary() function with the data_lme object as argument:\n\nsummary(data_lme)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: expr ~ 1 + A + (1 | B:A) + (1 | C:B:A)\n   Data: mice_data3\n\nREML criterion at convergence: 684.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.76545 -0.53144 -0.02996  0.59495  2.20017 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n C:B:A    (Intercept) 1.7073   1.3066  \n B:A      (Intercept) 1.5615   1.2496  \n Residual             0.5045   0.7103  \nNumber of obs: 225, groups:  C:B:A, 75; B:A, 15\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  10.0519     0.6224  16.151\nA2            1.4654     0.8801   1.665\nA3           -2.9085     0.8801  -3.305\n\nCorrelation of Fixed Effects:\n   (Intr) A2    \nA2 -0.707       \nA3 -0.707  0.500\n\n\nUnder Random effects the term Residual refers to technical replicates, the term B:A to mice and the term C:B:A to cells."
  },
  {
    "objectID": "posts/nested-designs/index.html#multiple-comparisons",
    "href": "posts/nested-designs/index.html#multiple-comparisons",
    "title": "R Tutorial: Analysis of results of a nested experimental design",
    "section": "Multiple comparisons",
    "text": "Multiple comparisons\nThe glth() function of the multcomp package can be used to perform the multiple comparison test:\n\nmult_drug &lt;- glht(data_lme, linfct = mcp(A = \"Tukey\"))\nsummary(mult_drug)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lmer(formula = expr ~ 1 + A + (1 | B:A) + (1 | C:B:A), data = mice_data3)\n\nLinear Hypotheses:\n           Estimate Std. Error z value Pr(&gt;|z|)    \n2 - 1 == 0   1.4654     0.8801   1.665   0.2188    \n3 - 1 == 0  -2.9085     0.8801  -3.305   0.0027 ** \n3 - 2 == 0  -4.3739     0.8801  -4.970   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nAlternatively or additionally it is also possible to use the HSD.test() function of the agricolae package. For this I specified the mean squared error for mice (29.05) as well as the degrees of freedom for this factor (12):\n\ntukey_hsd &lt;- with(mice_data, HSD.test(expr, A, DFerror = 12, MSerror = 29.05))\ntukey_hsd$groups\n\n       expr groups\n2 11.517297      a\n1 10.051919      a\n3  7.143408      b\n\n\nAccording to the results of the above analyses, we can conclude that drug 2 significantly reduced the gene expression levels compared to drug 1 and the control."
  },
  {
    "objectID": "posts/simultaneus_optimization/index.html",
    "href": "posts/simultaneus_optimization/index.html",
    "title": "R tutorial: Simultaneus Optimization of Several Response Variables",
    "section": "",
    "text": "The data and code for this publication can be found in the following repository: simultaneous-optimization."
  },
  {
    "objectID": "posts/simultaneus_optimization/index.html#definition-of-functions-for-simultaneous-optimization",
    "href": "posts/simultaneus_optimization/index.html#definition-of-functions-for-simultaneous-optimization",
    "title": "R tutorial: Simultaneus Optimization of Several Response Variables",
    "section": "Definition of functions for simultaneous optimization",
    "text": "Definition of functions for simultaneous optimization\nFor simultaneous optimization I defined a couple of functions. First a prediction function:\n\n# Prediction function\nrsm_opt &lt;- function(x, dObject, space = \"square\"){\n  \n  df &lt;- data.frame(x1 = x[1], x2 = x[2], x3 = x[3])\n  \n  y1 &lt;- predict.lm(y1_m, df)\n  y2 &lt;- predict.lm(y2_m, df)\n  y3 &lt;- predict.lm(y3_m, df)\n  y4 &lt;- predict.lm(y4_m, df)\n  \n  out &lt;- predict(dObject, data.frame(y1 = y1, y2 = y2, y3 = y3, y4 = y4))\n  \n  if(space == \"circular\" & sqrt(sum(x^2)) &gt; 1.63) out &lt;- 0\n  else if(space == \"square\" & any(abs(x) &gt; 1.63)) out &lt;- 0 \n  \n  out\n}\n\nThis function specifies two possible shapes for the experimental region: square and circular. This must be taken into account and may vary according to our experimental design.\nSubsequently, I defined a function that will be in charge of finding the best global desirability value:\n\n# Optimization function\nmaximize_overall &lt;- function(int_1 = c(-1.63, 1.63),\n                             int_2 = c(-1.63, 1.63),\n                             int_3 = c(-1.63, 1.63),\n                             dObject = NULL, \n                             space = \"square\"){\n  \n  searchGrid &lt;- expand.grid(\n    x1 = seq(int_1[1], int_1[2], length.out = 5),\n    x2 = seq(int_2[1], int_2[2], length.out = 5),\n    x3 = seq(int_3[1], int_3[2], length.out = 5)\n  )\n  \n  for(i in 1:dim(searchGrid)[1]){\n    \n    tmp &lt;- optim(as.vector(searchGrid[i,]),\n                 rsm_opt,\n                 dObject = dObject,\n                 space = space, \n                 control = list(fnscale = -1))\n    \n    if(i == 1) best &lt;- tmp\n    if(tmp$value &gt; best$value) best &lt;- tmp\n  }\n  \n  best\n}\n\nNote that in the previous functions I took into account the maximum and minimum coded values of each factor, in this case 1.63 and -1.63, respectively. If you wish to use this code with your own data do not forget to modify these values according to your experimental design."
  },
  {
    "objectID": "posts/simultaneus_optimization/index.html#definition-of-desirability-functions",
    "href": "posts/simultaneus_optimization/index.html#definition-of-desirability-functions",
    "title": "R tutorial: Simultaneus Optimization of Several Response Variables",
    "section": "Definition of desirability functions",
    "text": "Definition of desirability functions\nWith the functions included in the desirability package we define a desirability function for each response as follows:\n\nlibrary(desirability)\n\nD_y1 &lt;- dMax(120, 170)\nD_y2 &lt;- dMax(1000, 1300)\nD_y3 &lt;- dTarget(400, 500, 600)\nD_y4 &lt;- dTarget(60, 67.5, 75)\n\nEach target value was set for convenience and was sought to meet the above specifications as outlined in Derringer & Suich (1980).\nGlobal desirability is in turn defined by taking into account the above functions:\n\nD_overall &lt;- dOverall(D_y1, D_y2, D_y3, D_y4)"
  },
  {
    "objectID": "posts/simultaneus_optimization/index.html#carrying-out-simultaneous-optimization",
    "href": "posts/simultaneus_optimization/index.html#carrying-out-simultaneous-optimization",
    "title": "R tutorial: Simultaneus Optimization of Several Response Variables",
    "section": "Carrying out simultaneous optimization",
    "text": "Carrying out simultaneous optimization\nWith the previously defined functions we proceed to perform the simultaneous optimization:\n\noverall_opt &lt;- maximize_overall(dObject = D_overall)\n\nDepending on your data and the specifications of your computer, this process may be a bit slow.\nThe processing that allows you to obtain the maximum overall desirability can be deployed directly:\n\noverall_opt\n\n$par\n         x1          x2          x3 \n-0.05345891  0.14718872 -0.86635592 \n\n$value\n[1] 0.5833527\n\n$counts\nfunction gradient \n     502       NA \n\n$convergence\n[1] 1\n\n$message\nNULL\n\n\n$value refers to the overall desirability value obtained. This result is practically the same as that published in the Derringer & Suich paper."
  },
  {
    "objectID": "posts/prickly-pear-fruits/index.html",
    "href": "posts/prickly-pear-fruits/index.html",
    "title": "Prickly pear, a sweet source of nutrients",
    "section": "",
    "text": "The global prickly pear market has been growing gradually at present. The top exporter in 2022 was Canada, with an export value of 2.89 million USD , while the United States was the top importer in 2022, with an import value of 4.47 million USD , an increase of 6% over 2021. On the other hand, consumer demand for this fruit has also been growing slowly and continues to thrive thanks to its nutritional value and versatility in cooking and traditional medicine.\n\n\n\n\n\nMexico is considered the largest international producer of prickly pear, followed by South American countries such as Chile, Brazil and Argentina where large quantities are also produced. In Europe, Italy is the largest producer, while Morocco is the leading producer in Africa.\nThe prickly pear is a common fruit in Mexican markets, especially between July and September, which is the peak production period. In 2019 alone, up to 428,300 tons of this sweet delicacy were produced. And although its production is below other fruits such as oranges, bananas and apples, prickly pear fruits can be considered an important source of various nutrients.\nPrickly pear pulp is a good source of carbohydrates, fiber, vitamin C, vitamin A, calcium, potassium and not so well known components such as phenolic compounds, which are known to have several beneficial effects as part of a balanced diet. Its consumption can also be considered as a natural alternative to reduce cholesterol and triglyceride levels, relieve the discomfort caused by ulcers and help stabilize and regulate blood sugar levels.\nAlthough it is more common to find green prickly pears, there are also yellow, orange, red and purple ones. In these different colored varieties there is a group of compounds called betalains, pigments responsible for conferring the color. As an additional benefit, it is also known that the consumption of betalains can reduce cholesterol and triglyceride levels in the blood.\n\n\n\n\n\nPrickly pears can be used for the production of juice, jam, vinegar and liquors. From the varieties of different colors are obtained dyes that can be used as an alternative to synthetic dyes in food. The peel of this fruit is usually discarded, but it can also be considered an important source of pigments. Moreover, even edible oil with a high content of beneficial compounds, such as unsaturated fatty acids, vitamin E and vitamin K, can be obtained from the seeds.\n\n\n\nPrickly pear liqueur\n\n\nAs mentioned above, Mexico is the world‚Äôs leading producer of prickly pear. The main advantage of prickly pear cactus is its low water requirement, which in drought areas makes it a more viable crop than beans and corn. In areas where it is grown extensively, prickly pear cactus is the main source of income for many Mexican families. It is also important to mention that Mexico is the center of origin of the prickly pear, with at least 100 species from which edible fruits can be obtained.\n\n\n\n\n\nGood for you! Now that you know all this, aren‚Äôt you craving for some prickly pears? See you at the market!\nIf you want to know a little more about the prickly pear, do not hesitate to consult the pages and documents in the following links:\n\nFrutas, una delicia del campo mexicano.\nRazones para comer tunas m√°s seguido.\nInnovaci√≥n de productos de alto valor agregado a partir de la tuna mexicana.\nEcolog√≠a del cultivo, manejo y usos del nopal.\nM√©xico exporta tuna y xoconostle a EU, Canad√°, Jap√≥n, Emiratos √Årabes Unidos, Francia y Reino Unido.\n2023 Prickly Pear global market overview today - Tridge.\n Global Production of Cactus Pear - South Africa Online.\nSouth Africa: Prickly pear grower has big plans for the fruit - FreshPlaza.\nPrickly Pear Market Summary - Produce Blue Book.\nPears | Agricultural Marketing Resource Center.\n\nThe text in this publication is free to use under license Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/pca-from-scratch/index.html",
    "href": "posts/pca-from-scratch/index.html",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "",
    "text": "All the data and code of this post can be download on the next repository : pca-from-scratch"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#packages",
    "href": "posts/pca-from-scratch/index.html#packages",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Packages",
    "text": "Packages\nFor this post we will use some functions included in ggplot2, ggpubr, readr, purrr and dplyr:\n\n# Run the next line if you have not installed the packages:\n# install.packages(c(\"ggplot2\", \"ggpubr\", \"readr\", \"purrr\", \"dplyr\"))\n\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#data-simulation",
    "href": "posts/pca-from-scratch/index.html#data-simulation",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Data simulation",
    "text": "Data simulation\nFirst let‚Äôs simulate data with two dimensions. To do this let‚Äôs make the second variable directly dependent on the first one and store everything in a data frame:\n\nset.seed(1) # For data reproducibility\n\n# Variable 1\nvar_1 &lt;- rnorm(50, 50, sd = 3)\n\n# Variable 2\nvar_2 &lt;- .5*var_1 + rnorm(50, sd = sqrt(3))\n\n# Both variables in a data frame\ndata_set_1 &lt;- tibble(var_1, var_2)\n\nhead(data_set_1)\n\n# A tibble: 6 √ó 2\n  var_1 var_2\n  &lt;dbl&gt; &lt;dbl&gt;\n1  48.1  24.7\n2  50.6  24.2\n3  47.5  24.3\n4  54.8  25.4\n5  51.0  28.0\n6  47.5  27.2\n\n\nIf we make a scatter plot we can observe the clear dependence between both variables:\n\n# A scatter plot with the two simulated variables\nggplot(data_set_1, aes(x = var_1, y = var_2)) +\n  geom_point(color = \"blue\", size = 2) +\n  xlab(\"Variable 1\") +\n  ylab(\"Variable 2\") +\n  theme_classic()"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#first-step-center-each-variable",
    "href": "posts/pca-from-scratch/index.html#first-step-center-each-variable",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "First step: Center each variable",
    "text": "First step: Center each variable\nThe first step in the PCA is to center each variable with respect to its average value:\n\ndata_set_1 &lt;- data_set_1 %&gt;% \n  mutate(varc_1 = var_1 - mean(var_1), varc_2 = var_2 - mean(var_2))\n\nhead(data_set_1)\n\n# A tibble: 6 √ó 4\n  var_1 var_2 varc_1  varc_2\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1  48.1  24.7 -2.18  -0.604 \n2  50.6  24.2  0.250 -1.14  \n3  47.5  24.3 -2.81  -1.02  \n4  54.8  25.4  4.48   0.0829\n5  51.0  28.0  0.687  2.62  \n6  47.5  27.2 -2.76   1.85  \n\n\nThis did not change the relative position between each point, so the data look the same:\n\nggplot(data_set_1, aes(x = varc_1, y = varc_2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  theme_classic()"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#second-step-obtaining-the-covariance-matrix",
    "href": "posts/pca-from-scratch/index.html#second-step-obtaining-the-covariance-matrix",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Second step: Obtaining the covariance matrix",
    "text": "Second step: Obtaining the covariance matrix\nThe next step is to obtain the covariance matrix for the previous data set. For this we perform a matrix multiplication with the data as follows:\n\n# Select only the centered variables\ndata_set_2 &lt;- data_set_1 %&gt;% \n  select(varc_1, varc_2) %&gt;% \n  as.matrix()\n\n# Calculate the covariance matrix\ncov_m &lt;- (t(data_set_2) %*% data_set_2) / (nrow(data_set_2) - 1) \n\ncov_m\n\n         varc_1   varc_2\nvarc_1 6.220943 2.946877\nvarc_2 2.946877 4.207523\n\n\nIn the resulting matrix the diagonal contains the variance of each variable while the values outside the diagonal are the covariances between the variables (see figure below):\n\n\n\n\n\nWe can obtain the covariance matrix directly with the cov() function:\n\ncov(data_set_2)\n\n         varc_1   varc_2\nvarc_1 6.220943 2.946877\nvarc_2 2.946877 4.207523\n\n\nOr with the crossprod() function as follows:\n\ncrossprod(data_set_2) / (nrow(data_set_2) - 1)\n\n         varc_1   varc_2\nvarc_1 6.220943 2.946877\nvarc_2 2.946877 4.207523"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#third-step-obtain-the-eigenvalues-and-eigenvectors-of-the-covariance-matrix",
    "href": "posts/pca-from-scratch/index.html#third-step-obtain-the-eigenvalues-and-eigenvectors-of-the-covariance-matrix",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Third step: Obtain the eigenvalues and eigenvectors of the covariance matrix",
    "text": "Third step: Obtain the eigenvalues and eigenvectors of the covariance matrix\nPrincipal components represent the directions in the data that explain the maximum amount of variance. They are ‚Äúlines‚Äù that collect most of the information in the data. These directions can be obtained by calculating the eigenvalues and eigenvectors of the covariance matrix:\n\n# Use eigen() to obtain eigenvectors and eigenvalues\ncov_e &lt;- eigen(cov_m)\n\n# Eigenvectors\ne_vec &lt;- cov_e$vectors\n\n# Eigenvalues\ne_val &lt;- cov_e$values\n\nThe span of each eigenvector can be considered the ‚Äúline‚Äù that captures most of the variation:\n\n# First eigenvector \nev_1 &lt;- e_vec[,1]\n\n# Slope of the first eigenvector\nev1_m &lt;- ev_1[2] / ev_1[1]\n\n# Second eigenvector \nev_2 &lt;- e_vec[,2]\n\n# Slope of the second eigenvector\nev2_m &lt;- ev_2[2] / ev_2[1]\n\n# Scatter plot showing the span of both eigenvectors \nggplot(data.frame(data_set_2), aes(x = varc_1, y = varc_2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  geom_abline(slope = ev1_m, color = \"blue\", linewidth = 0.7) +\n  geom_abline(slope = ev2_m, color = \"red\", linewidth = 0.7) +\n  theme_classic()\n\n\n\n\nAs can be seen, there is one eigenvector for each variable in the data set, in this case two. Also note that the eigenvectors are perpendicular:\n\n# Multiply both eigenvectors \nev_1 %*% ev_2\n\n     [,1]\n[1,]    0\n\n\nAs for the eigenvalues, their numerical values are equal to the sum of squares of the distances of each projected data point in the corresponding principal component. This sum of squares is maximized in the first principal component."
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#fourth-step-make-a-scree-plot",
    "href": "posts/pca-from-scratch/index.html#fourth-step-make-a-scree-plot",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Fourth step: Make a Scree Plot",
    "text": "Fourth step: Make a Scree Plot\nDividing each eigenvalue by n - 1 (n is the number of rows in the original data) will give an estimate of the variance represented by each principal component. The sum of all variances (the total variance) can be used to calculate the percentage of variation and visualized with a Scree Plot:\n\n# Calculate the estimated variance for each eigenvalue\ne_var &lt;- e_val / (nrow(data_set_2) - 1)\n\n# Data frame with variance percentages\nvar_per &lt;- tibble(\n  PC  = c(\"PC1\", \"PC2\"),\n  PER = c(e_var) * 100 / sum(e_var) # Calculate the percentage\n    )\n\n# Scree plot \nggplot(var_per, aes(x = PC, y = PER)) +\n  geom_col(width = 0.5, color = \"black\") +\n  xlab(\"Principal component\") +\n  ylab(\"Percentage of variation (%)\") +\n  theme_classic()"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#fifth-step-obtain-the-loadings-of-each-variable.",
    "href": "posts/pca-from-scratch/index.html#fifth-step-obtain-the-loadings-of-each-variable.",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Fifth step: Obtain the loadings of each variable.",
    "text": "Fifth step: Obtain the loadings of each variable.\nThe eigenvectors obtained using the eigen() function are normalized. This means that their length is equal to 1:\n\n# Norm of the first eigenvector\nnorm(as.matrix(ev_1), \"F\")\n\n[1] 1\n\n# Norm of the second eigenvector\nnorm(as.matrix(ev_2), \"F\")\n\n[1] 1\n\n\nThe elements of each eigenvector are also called loadings and can be interpreted as the contribution of each variable in the data set to the corresponding principal component or, more strictly, as the coefficients of the linear combination of the original variables from which the principal components are constructed.\nWe can make a table with these values and see the contributions of each variable to each principal component:\n\n# Data frame with both eigenvectors\nloads &lt;- tibble(\n  VAR   = c(\"var_1\", \"var_2\"),\n  PC1 = ev_1, # First eigenvecor\n  PC2 = ev_2  # Second eigenvectors\n)\n\nloads\n\n# A tibble: 2 √ó 3\n  VAR      PC1    PC2\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 var_1 -0.813  0.582\n2 var_2 -0.582 -0.813\n\n\nThe above can be useful in data with many dimensions to get an idea of which variables cause the groupings or differences in the PCA plot."
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#sixth-step-representing-data-in-fewer-dimensions",
    "href": "posts/pca-from-scratch/index.html#sixth-step-representing-data-in-fewer-dimensions",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Sixth step: Representing data in fewer dimensions",
    "text": "Sixth step: Representing data in fewer dimensions\nIf we change the base of the original data to that indicated by the eigenvectors, we are basically rotating the data:\n\n# Change the basis of the original data \ndata_set_3 &lt;- data_set_2 %*% solve(e_vec) # Inverse of eigenvectors matrix\n\n# Scatter showing the rotation \nggplot(data.frame(data_set_3), aes(X1, X2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  xlab(\"PC1 (78.8%)\") +\n  ylab(\"PC2 (21.2%)\") +\n  theme_classic()\n\n\n\n\nComparing the two graphs gives us an idea of how the data have been rotated once we change the base:\n\n# Scatter plot with the centered data \nplot_data &lt;- ggplot(data.frame(data_set_2), aes(x = varc_1, y = varc_2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  ylim(c(-8, 8.5)) +\n  ggtitle(\"Original Data\") +\n  theme_classic()\n\n# Scatter plot with the rotated data\nplot_rotation &lt;- ggplot(data.frame(data_set_3), aes(X1, X2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  xlab(\"PC1 (78.8%)\") +\n  ylab(\"PC2 (21.2%)\") +\n  ylim(c(-8, 8.5)) +\n  ggtitle(\"Change of Basis to Eigenvectors\") +\n  theme_classic()\n\n# Both graphs side by side\nggarrange(plot_data, plot_rotation)\n\n\n\n\nSince principal component 1 (PC1) explains most of the variance in the data, we can omit principal component 2 (PC2) and represent each point in a single dimension, here as red dots:\n\n# Data points just from PC 1\ndata_pc1 &lt;- data.frame(v1 = data_set_3[,1], v2 = rep(0, nrow(data_set_3)))\n\n# Scatter plot showing the projected points from PC1 (red points)\nggplot(data.frame(data_set_3), aes(X1, X2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_point(data = data_pc1, aes(v1, v2), color = \"red\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  xlab(\"PC1 (78.8%)\") +\n  ylab(\"PC2 (21.2%)\") +\n  ylim(c(-8, 8.5)) +\n  theme_classic()\n\n\n\n\nThe above ideas can be used in data with many variables to reduce the dimensions and make two-dimensional representations of the data."
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#first-step-center-the-data",
    "href": "posts/pca-from-scratch/index.html#first-step-center-the-data",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "First step: Center the data",
    "text": "First step: Center the data\nSubtract the average of each variable (columns) and divide by its standard deviation:\n\n# Means for each variable\nvar_means &lt;- unlist(map(data_set_wine, mean))\n\n# Standard deviation for each variable\nvar_sd &lt;- unlist(map(data_set_wine, sd))\n\n# Center each variable\ndata_set_wine_2 &lt;- map2(\n  data_set_wine, var_means, .f = function(x, mean) x - mean\n  )\n\n# Devide by the standard deviation of each variable\ndata_set_wine_2 &lt;- map2(\n  data_set_wine_2, var_sd, .f = function(x, sd) x / sd\n)\n\n# Make a matrix from the previous list\ndata_set_wine_2 &lt;- as.matrix(data.frame(data_set_wine_2))\n\nEach row of the transformed data corresponds to the same wine sample as the original data.\nThe first six rows of the transformed data look like this:\n\nhead(data_set_wine_2)\n\n         Ethanol   TotalAcid  VolatileA  MalicAcid         pH    LacticAc\n[1,] -0.63678489 -0.40870999 -0.7811641  2.2627949 0.18823560 -0.88226788\n[2,]  0.29224039  0.01096889  1.3231961 -0.5694123 0.40060357  0.38388752\n[3,] -0.38341492 -0.97527630  0.4814521 -1.9201573 1.88717938  0.06061381\n[4,]  0.05998306 -0.15690246  0.4814521 -1.2229986 1.03770749 -0.28959935\n[5,]  1.15792167 -0.15690246 -0.1498560  1.0427673 0.08205162 -0.80144937\n[6,]  1.45351898 -0.59756526  0.8321787 -0.9179917 2.41810183  1.75780072\n        ReSugar CitricAcid        CO2    Density     FolinC     Glycerol\n[1,] -0.5311188  1.1594919 -1.6049333 -0.7844233 -0.1371321 -0.525885112\n[2,]  0.1996454  0.1084153 -0.9482843  1.2458487  1.2376582 -0.160707985\n[3,] -0.4854461  1.8063082  1.5330409 -0.7844233  0.2405111  0.802031815\n[4,]  1.5317676  1.9680124  0.5483973  1.2458487  1.6138874 -0.559083800\n[5,] -0.6909735 -0.2149929 -1.0972195 -0.7844233  1.3861700  0.680306454\n[6,] -0.5767916 -0.5384012 -1.0868116 -0.7844233  1.4003137 -0.005784995\n       Methanol   TartaricA\n[1,] -1.1081037  0.43877622\n[2,] -0.0870238 -0.03865555\n[3,] -0.5975636 -1.05319836\n[4,]  0.6787860  1.99042973\n[5,] -0.0870238 -1.11287729\n[6,] -0.3422939 -2.06774119\n\n\nDividing by standard deviations is a way of giving each variable equal importance despite its range, magnitude and/or scale of measurement. In addition to dividing by the standard deviation, it is possible to apply other transformations depending on the data. See the resources at the end of this post if you are interested."
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#second-step-calculating-the-covariance-matrix",
    "href": "posts/pca-from-scratch/index.html#second-step-calculating-the-covariance-matrix",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Second step: Calculating the covariance matrix",
    "text": "Second step: Calculating the covariance matrix\nWe multiply the data (in matrix form) by its transpose:\n\n# Calculate the covariance matrix\ncov_wine &lt;- (t(data_set_wine_2) %*% data_set_wine_2) / \n  (nrow(data_set_wine_2) - 1)\n\ncov_wine[1:5, 1:5]\n\n             Ethanol  TotalAcid  VolatileA   MalicAcid         pH\nEthanol   1.00000000  0.3262321  0.2028382  0.04166778  0.1697347\nTotalAcid 0.32623209  1.0000000  0.4660575 -0.26067574 -0.3518303\nVolatileA 0.20283816  0.4660575  1.0000000 -0.74628880  0.3011611\nMalicAcid 0.04166778 -0.2606757 -0.7462888  1.00000000 -0.2929542\npH        0.16973470 -0.3518303  0.3011611 -0.29295421  1.0000000\n\n\nHere we only show the first five rows and columns of the covariance matrix. If you want to display the whole matrix a good way is to use the View() function.\nEach value of the cov_wine matrix has the same interpretation as in the example with two variables, the values on the diagonal are the variances of each variable, and the values outside the diagonal are the covariances between variables. As can be seen, all variances are equal to 1. This is precisely the effect of centering and dividing by the standard deviation."
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#third-step-obtain-the-eigenvalues-and-eigenvectors-of-the-covariance-matrix.",
    "href": "posts/pca-from-scratch/index.html#third-step-obtain-the-eigenvalues-and-eigenvectors-of-the-covariance-matrix.",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Third step: Obtain the eigenvalues and eigenvectors of the covariance matrix.",
    "text": "Third step: Obtain the eigenvalues and eigenvectors of the covariance matrix.\nLet‚Äôs use the eigen() function to obtain the eigenvectors and their eigenvalues:\n\n# eigen() to obtain eigenvalues and eigenvectors\neg_wine &lt;- eigen(cov_wine)\n\n# Eigenvalues\neg_vals &lt;- eg_wine$values\n\n# Eigenvectors\neg_vecs &lt;- eg_wine$vectors\n\nThe number of vectors and eigenvalues is the same as the number of variables in the original data set:\n\n# Number of eigenvalues\nlength(eg_vals)\n\n[1] 14\n\n# Number of eigenvectors\nncol(eg_vecs)\n\n[1] 14"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#fourth-step-make-a-scree-plot-1",
    "href": "posts/pca-from-scratch/index.html#fourth-step-make-a-scree-plot-1",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Fourth step: Make a Scree Plot",
    "text": "Fourth step: Make a Scree Plot\nWe calculated the percentage of variation of each component and made a Scree plot:\n\n# Calculate variances from each eigenvalue\neg_vars &lt;- eg_vals / (nrow(data_set_wine_2) - 1)\n\n# Data frame with variance percenatges\nvars_perc &lt;- tibble(\n  PC  = unlist(map(1:14, function(x) paste0(\"PC\", x))),\n  PER = round((eg_vars * 100) / sum(eg_vars), 4)\n    )\n\n# Scree plot\nggplot(\n  vars_perc, \n  aes(x = reorder(PC, order(PER, decreasing = TRUE)), y = PER)\n       ) +\n  geom_col(width = 0.5, color = \"black\") +\n  xlab(\"Principal component\") +\n  ylab(\"Percentage of variation (%)\") +\n  theme_classic()"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#fifth-step-obtain-the-loadings-of-each-variable",
    "href": "posts/pca-from-scratch/index.html#fifth-step-obtain-the-loadings-of-each-variable",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Fifth step: Obtain the loadings of each variable",
    "text": "Fifth step: Obtain the loadings of each variable\nThe elements of each eigenvector represent the weight of each variable in the corresponding principal component:\n\n# Data frame with loading scores\nloads_wine &lt;- data.frame(eg_vecs)\ncolnames(loads_wine) &lt;- vars_perc$PC\nrownames(loads_wine) &lt;- var_names\n\nhead(loads_wine)\n\n                 PC1        PC2         PC3         PC4          PC5\nEthanol   -0.2050720 -0.3452884  0.28833198 -0.33833697  0.001102612\nTotalAcid -0.1457210 -0.4545803 -0.06374908  0.42139270  0.041508881\nVolatileA  0.2959952 -0.4418538  0.03338975  0.08430388 -0.010891943\nMalicAcid -0.3401714  0.3173079  0.17907611 -0.04002812 -0.252870227\npH         0.2413635 -0.1030772 -0.04459655 -0.66472873 -0.170592677\nLacticAc   0.3462627 -0.3745378 -0.16738658  0.11850141 -0.108643313\n                  PC6          PC7         PC8        PC9        PC10\nEthanol   -0.02992588  0.106645514  0.40565762  0.2220668 -0.07364281\nTotalAcid -0.10721872  0.109080109 -0.15801515  0.1647662 -0.12640759\nVolatileA  0.12676353 -0.117134957 -0.01066998 -0.2323371  0.19698524\nMalicAcid  0.09495473  0.247290148 -0.35870967  0.1792700 -0.27203174\npH         0.01717167 -0.191605073 -0.30040129  0.3352703  0.28827462\nLacticAc   0.05496474  0.008009813 -0.10169448  0.1377493 -0.28605639\n                 PC11        PC12        PC13        PC14\nEthanol    0.52247393 -0.22486081  0.29025392 -0.04195694\nTotalAcid -0.09209975 -0.25488777 -0.01537897  0.65083548\nVolatileA  0.19114512 -0.33800898 -0.59864002 -0.27743084\nMalicAcid -0.07338526 -0.53559762 -0.18854505 -0.23066288\npH        -0.21200577 -0.09998927 -0.02838824  0.28223548\nLacticAc  -0.32247769 -0.11583440  0.49548494 -0.45696212\n\n\nMaking a scatter plot with these values can help determine correlations between variables and/or explain why a particular clustering is observed in principal component scatter plots (next section):\n\n# Scatter plot with loadings of PC1 and PC2\nld_pc12 &lt;- ggplot(loads_wine, aes(PC1, PC2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_text(aes(label = rownames(loads_wine)), hjust = -.2) +\n  ggtitle(\"Loadings for PC1 and PC2\") +\n  xlim(c(-.7, .7)) +\n  ylim(c(-.7, .7)) +\n  xlab(\"PC1 (24.4%)\") +\n  ylab(\"PC2 (21.3%)\") +\n  theme_classic()\n\n# Scatter plot with loadings of PC3 and PC4\nld_pc34 &lt;- ggplot(loads_wine, aes(PC3, PC4)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_text(aes(label = rownames(loads_wine)), hjust = -.2) +\n  ggtitle(\"Loadings for PC3 and PC4\") +\n  xlim(c(-.7, .7)) +\n  ylim(c(-.7, .7)) +\n  xlab(\"PC3 (17.5%)\") +\n  ylab(\"PC4 (10.0%)\") +\n  theme_classic()\n\n# Both graphs side by side\nggarrange(ld_pc12, ld_pc34)"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#sixth-step-representing-data-in-fewer-dimensions-1",
    "href": "posts/pca-from-scratch/index.html#sixth-step-representing-data-in-fewer-dimensions-1",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Sixth step: Representing data in fewer dimensions",
    "text": "Sixth step: Representing data in fewer dimensions\nIdeally, if PC1 and PC2 picked up most of the variation, say more than 90%, it would be possible to make a good representation of the data in a two-dimensional scatter plot. But since real data are almost never ideal, in this case PC1, PC2, PC3 and PC4 account for 73% of the variation. Let us try to observe clustering using two scatter plots, the first with PC1 and PC2, and the second with PC3 and PC4.\nFirst, we change the basis of the transformed data to that indicated by the eigenvectors:\n\n# Change the basis of the data\ndata_set_wine_eb &lt;- data_set_wine_2 %*% solve(eg_vecs)\n\n# Transfrom to a data frame\ndata_set_wine_eb &lt;- data.frame(data_set_wine_eb)\ncolnames(data_set_wine_eb) &lt;- vars_perc$PC\n\n# Add a column with the origin of each wine sample\ndata_set_wine_eb &lt;- data_set_wine_eb %&gt;% \n  mutate(\n    WineSample = unlist(map(wine_label, function(x) substr(x, 1, 3)))\n    ) %&gt;% \n  relocate(WineSample)\n\nhead(data_set_wine_eb)\n\n  WineSample        PC1        PC2        PC3        PC4         PC5\n1        ARG -0.8708602  1.4271679  1.0042682 -0.4970542 -2.40194963\n2        ARG  0.9214933 -0.8023313  0.9170387 -0.3788901  0.04278493\n3        ARG  2.0993602 -1.1582579  0.1104490 -0.8588001  0.36878072\n4        ARG  2.6784566  0.6585042 -0.5914731 -1.1987690  0.55471303\n5        ARG -0.3093155 -0.7061611  0.9719731 -0.8791527 -1.48911945\n6        ARG  0.6656888 -1.9938918  2.0606329 -0.4319683 -0.58522246\n          PC6        PC7        PC8         PC9         PC10        PC11\n1 -0.50073335  0.7380182 -0.7113971  1.28454429  0.141197785  0.91258285\n2 -1.11669152 -0.6204253  0.1024002 -0.66345802  0.217715309 -0.44779807\n3 -0.01460188  0.6155704  1.2988198 -0.31647718 -1.945505546  1.56333545\n4 -1.77225038  1.1845744  0.7517347 -0.01741376 -0.458803511 -1.38374975\n5  0.58458758 -1.3525017  0.3597807  1.06873458  0.656603378 -0.04068736\n6  0.76213010 -0.9997045  2.4380628 -1.36498248 -0.004949351  1.27477965\n        PC12        PC13        PC14\n1 -0.0961029 -0.01151848  0.12807023\n2  0.6191680 -1.13863351 -0.22032005\n3 -0.2843139  1.44732864  0.08942808\n4  0.1136096 -1.16558478 -0.14095094\n5 -0.4097057  0.16732603 -0.70481769\n6 -0.5940964 -0.93709414 -0.52629641\n\n\nNow, let‚Äôs make both scatter plots taking only the values of PC1, PC2, PC3 and PC4:\n\n# Scatter plot for PC1 and PC2\npc12 &lt;- ggplot(\n  data_set_wine_eb, \n  aes(PC1, PC2, color = WineSample, shape = WineSample)\n) +\n  geom_point(size = 3) +\n  ggtitle(\"PC1 and PC2\") +\n  xlab(\"PC1 (24.4%)\") +\n  ylab(\"PC2 (21.3%)\") +\n  theme_classic() +\n  theme(legend.position = \"none\") \n\n# Scatter plot for PC3 and PC4\npc34 &lt;- ggplot(\n  data_set_wine_eb, \n  aes(PC3, PC4, color = WineSample, shape = WineSample)\n) +\n  geom_point(size = 3) +\n  scale_color_discrete(\n    name = \"Wine origin\", \n    labels = c(\"Argentina\", \"Australia\", \"Chile\", \"South Africa\")\n    ) +\n  scale_shape_discrete(\n    name = \"Wine origin\", \n    labels = c(\"Argentina\", \"Australia\", \"Chile\", \"South Africa\")\n  ) +\n  ggtitle(\"PC3 and PC4\") +\n  xlab(\"PC3 (17.5%)\") +\n  ylab(\"PC4 (10.0%)\") +\n  theme_classic()\n\n# Both graphs side by side\nggarrange(pc12, pc34)"
  },
  {
    "objectID": "posts/pca-matabolomics/index.html",
    "href": "posts/pca-matabolomics/index.html",
    "title": "R tutorial: Principal Component Analysis with Metabolomics Data",
    "section": "",
    "text": "In this tutorial we will cover the steps necessary to perform a principal component analysis (PCA) on metebolomic data from cell cultures of a specific plant. This will help us to visualize groupings and determine possible correlations."
  },
  {
    "objectID": "posts/pca-matabolomics/index.html#bar-chart-with-the-percentages-of-variation-of-each-main-component",
    "href": "posts/pca-matabolomics/index.html#bar-chart-with-the-percentages-of-variation-of-each-main-component",
    "title": "R tutorial: Principal Component Analysis with Metabolomics Data",
    "section": "Bar chart with the percentages of variation of each main component",
    "text": "Bar chart with the percentages of variation of each main component\nTo visualize the results of our analysis we need to extract some values from the qt_pca object. First let‚Äôs extract the data in qt_pca$sdev and do some operations to represent the percentages of variation of each component:\n\n# Calculate the variance of each principal component in PCA and store it in var_pca\nvar_pca &lt;- qt_pca$sdev^2\n\n# Calculate the percentage of total variance explained by each principal component\nper_var_pca &lt;- round((var_pca / sum(var_pca)) * 100, 2)\n\n# Create a tibble (data frame) to store the results, with columns PC and PER_VAR\nper_var_pca &lt;- tibble(\n  PC = 1:length(per_var_pca),\n  PER_VAR = per_var_pca\n)\n\nper_var_pca\n\n# A tibble: 24 √ó 2\n      PC PER_VAR\n   &lt;int&gt;   &lt;dbl&gt;\n 1     1   80.7 \n 2     2   11.5 \n 3     3    2.57\n 4     4    2.08\n 5     5    0.84\n 6     6    0.65\n 7     7    0.49\n 8     8    0.36\n 9     9    0.18\n10    10    0.16\n# ‚Ñπ 14 more rows\n\n\nLet‚Äôs represent the data in per_var_pca with a bar chart:\n\nbar_pca &lt;- per_var_pca[1:6,] %&gt;% \n  ggplot(aes(x = as.factor(PC), y = PER_VAR)) +\n  geom_col() + \n  xlab(\"PC\") +\n  ylab(\"% of total variance\")\n\nbar_pca\n\n\n\n\nAs can be seen, and as we also saw in the summary of the previous section, the first two components account for most of the variation observed in the data."
  },
  {
    "objectID": "posts/pca-matabolomics/index.html#scatterplot-with-the-first-two-principal-components",
    "href": "posts/pca-matabolomics/index.html#scatterplot-with-the-first-two-principal-components",
    "title": "R tutorial: Principal Component Analysis with Metabolomics Data",
    "section": "Scatterplot with the first two principal components",
    "text": "Scatterplot with the first two principal components\nSince the first two components account for most of the variation, it is possible to make a two-dimensional plot that adequately represents all the data. We first extract the qt_pca scores and give an appropriate shape to the data frame:\n\npca_data &lt;- as_tibble(qt_pca$x) %&gt;% \n  mutate(TIME = as_factor(substr(rownames(qt_pca$x), start = 3, stop =5))) %&gt;% \n  relocate(TIME)\n\nSubsequently, let‚Äôs make a scatter plot with the scores of the first two components:\n\npca_plot &lt;- pca_data %&gt;% \n  mutate(TIME = as_factor(TIME)) %&gt;% \n  ggplot(aes(x = PC1, y = PC2, color = TIME)) +\n  geom_point(size = 3) +\n  xlab(paste0(\"PC1 (\", per_var_pca$PER_VAR[1], \"%)\")) +\n  ylab(paste0(\"PC2 (\", per_var_pca$PER_VAR[2], \"%)\")) +\n  scale_color_brewer(palette = \"Dark2\", name = \"Time (h)\")\n\npca_plot\n\n\n\n\nIt is interesting to note that there are several well-defined groupings and differences. In particular, the samples at time 72 h are separated from the rest with respect to PC1, while the 24 h samples are separated with respect to PC2."
  },
  {
    "objectID": "posts/pca-matabolomics/index.html#line-graph-with-metabolite-loadings",
    "href": "posts/pca-matabolomics/index.html#line-graph-with-metabolite-loadings",
    "title": "R tutorial: Principal Component Analysis with Metabolomics Data",
    "section": "Line graph with metabolite loadings",
    "text": "Line graph with metabolite loadings\nWhich metabolites contribute to the differences observed in the scatter plot above? Let us answer this question with the help of the weights or loadings.\nLet us first extract the loads from qt_pca and also give a suitable shape to the data frame:\n\n# Compound names\ncompound_names &lt;- filter(main_data, SAMPLE == 1, TIME == 0.5) %&gt;% \n  select(MET) %&gt;% \n  unlist()          \n\nloadigns_data &lt;- as_tibble(qt_pca$rotation) %&gt;% \n  signif(3) %&gt;% \n  mutate(MET = compound_names, INDEX = 1:length(MET)) %&gt;% \n  relocate(MET, INDEX) \n\nLet us define a small function that replaces with a blank space the name of the metabolite if the absolute value of its charge is less than a selected threshold:\n\n# Function to assign compound name if absolute value of loading is bigger than set threshold\nload_tr &lt;- function(loadings, threshold, compound_name) {\n  compound_name[!abs(loadings) &gt; threshold] &lt;- \" \"\n  return(compound_name)\n}\n\nIn the following graph (PC1 loadings), using our function together with geom_text() shows the names of the metabolites whose load is greater than the selected threshold:\n\nload_line_pc1 &lt;- loadigns_data %&gt;% \n  select(MET, INDEX, PC1) %&gt;% \n  mutate(\n    MET = load_tr(loading = PC1, threshold = 0.5, compound_name = MET)\n  ) %&gt;% \n  ggplot(aes(x = INDEX, y = PC1, label = MET)) +\n  geom_line() +\n  geom_text(fontface = \"bold\", position=position_nudge(), size = 2.5) +\n  xlab(\"Compound Index\") +\n  ylab(\"Loadings\")\n\nload_line_pc1\n\n\n\n\nFor PC2 the loadings are displayed in the same way:\n\nload_line_pc2 &lt;- loadigns_data %&gt;% \n  select(MET, INDEX, PC2) %&gt;% \n  mutate(\n    MET = load_tr(loading = PC2, threshold = 0.19, compound_name = MET)\n  ) %&gt;% \n  ggplot(aes(x = INDEX, y = PC2, label = MET)) +\n  geom_line() +\n  geom_text(\n    fontface = \"bold\", position = position_nudge(), angle = -40, size = 2.5\n    ) +\n  xlab(\"Compound Index\") +\n  ylab(\"Loadings\")\n\nload_line_pc2\n\n\n\n\nFrom both graphs we can tentatively define the metabolites that contributed most to the similarities or differences observed in the scatter plot of PC1 and PC2: lactate, sucrose, tryptophan, phenylalanine, glycerol, inositol, lysine, uric acid, tyrosine and sucrose.\nIt is important to note that the thresholds selected are similar to those of the aforementioned article. However, I believe it is important to be careful with the criteria for determining whether the loads can be considered ‚Äúlarge‚Äù or ‚Äúsmall‚Äù. An alternative approach could be to simply take the tables with the loadings values and order them with respect to their absolute value, which would also allow us to determine the metabolites with the largest loadings."
  },
  {
    "objectID": "posts/pca-matabolomics/index.html#patterns-of-metabolite-variation",
    "href": "posts/pca-matabolomics/index.html#patterns-of-metabolite-variation",
    "title": "R tutorial: Principal Component Analysis with Metabolomics Data",
    "section": "Patterns of metabolite variation",
    "text": "Patterns of metabolite variation\nNow that we know which compounds could be considered the most important, we can focus our attention on these metabolites and make a graph to see their pattern of behavior over time of stress treatment.\nLet us first consider the most important metabolites with respect to PC1. From the original data, let us take only the lactate and sucrose data and obtain the average of the relative amounts at each treatment time. We can visualize the averages obtained with a line graph:\n\nmet_plot_pc1 &lt;- main_data %&gt;% \n  filter(MET == \"lactate\" | MET == \"sucrose\") %&gt;% \n  group_by(MET, TIME) %&gt;% \n  summarise(MEAN_QT = mean(QT)) %&gt;% \n  ggplot(aes(x = TIME, y = MEAN_QT, color = MET)) +\n  geom_line(size = 1) +\n  geom_point() +\n  xlab(\"Time (h)\") +\n  ylab(\"Relative quantity\")\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\nmet_plot_pc1\n\n\n\n\nIt is noticeable that at 72 h the levels of these compounds increased greatly. This is probably the reason for the differences observed in the scatter plot of PC1 and PC2.\nLet us repeat the same procedure for the most important metabolites with respect to PC2:\n\nmet_plot_pc2 &lt;- main_data %&gt;% \n  filter(MET == \"tryptophan\" | MET == \"phenylalanine\" | MET == \"glycerol\" |\n         MET == \"tyrosine\" | MET == \"inositol\" | MET == \"lysine\" |\n         MET == \"uric acid\") %&gt;% \n  group_by(MET, TIME) %&gt;% \n  summarise(MEAN_QT = mean(QT)) %&gt;% \n  ggplot(aes(x = TIME, y = MEAN_QT, color = MET)) +\n  geom_line(size = 1) +\n  geom_point() +\n  xlab(\"Time (h)\") +\n  ylab(\"Relative quantity\")\n\nmet_plot_pc2\n\n\n\n\nIn this visualization we omit sucrose because we have already seen its behavior pattern. Although this graph is a little less clear, we can see that at 24 hours, the levels of metabolites such as glycerol and inositol increased, while the levels of aromatic amino acids decreased."
  },
  {
    "objectID": "posts/biotechnology/index.html",
    "href": "posts/biotechnology/index.html",
    "title": "What is biotechnology?",
    "section": "",
    "text": "In general, biotechnology is defined as the use of living organisms for the production or improvement of goods and/or services. By living organisms, we mean complete organisms (plants, animals, microorganisms such as yeasts, etc.) or parts of them (cells, enzymes, DNA, RNA, etc.). On the other hand, to clarify what we mean by goods or services, let us list some examples. Biotechnology ranges from the production of bread, beer, cheese, and vinegar to the development of vaccines, soil bioremediation, and wastewater purification.\n\n\n\nBeer in Egypt\n\n\nAs shown in the image above, the Egyptians were already using biotechnology for beer production more than 4,000 years ago. For the production of this beverage, a grain or cereal is needed, commonly barley, which is germinated a little to induce the activity of enzymes and generate sugars in the seed. In later steps, these sugars will be the food for microscopic fungi, known as yeasts, which will produce alcohol as a waste product. Depending on the process and the addition of other ingredients, it is now possible to find a wide variety of beers. Cheers!\n\n\n\nA photograph of yeast obtained by electron microscopy\n\n\nAt present, we essentially use killed pathogens, weakened live pathogens, or parts of even smaller organisms (viruses) to vaccinate us. The application of a vaccine, whether injected or given orally, will stimulate our immune system to be prepared for real infection scenarios. For the production of vaccines against some viruses, genetically modified yeast or mammalian cells are used to produce some component of the virus, which will ultimately be sufficient for our immune system to act against future infections.\n\n\n\nHow the RNA vaccines work\n\n\nIn wastewater treatment, large tanks are used where bacteria, yeasts, cyanobacteria (blue algae), algae, protozoa, rotifers, mites, and nematodes (cylindrical worms) act together to degrade harmful substances in the water. It is also possible to use tank towers without air contact, where specialized bacteria (archaeobacteria) will degrade substances in the wastewater and produce methane gas, which can then be used as an energy source (biogas).\n\n\n\nWastewater treatment tanks\n\n\nIt goes without saying that biotechnology needs a lot of scientific disciplines: mathematics, chemistry, organic chemistry, biochemistry, molecular and cell biology, microbiology, and many others come together to support and shape it.\n\n\n\nGenetically modified organisms\n\n\nThe above are just a few examples of how biotechnology is applied in various fields. Keep in mind that it is currently a very active field of research where knowledge is constantly being generated. Biotechnology is everywhere and is important in our daily lives!\n\nTo learn more about biotechnology\nAbdelfattah, A., Ali, S. S., Ramadan, H., El-Aswar, E. I., Eltawab, R., Ho, S.-H., Elsamahy, T., Li, S., El-Sheekh, M. M., Schagerl, M., Kornaros, M., & Sun, J. (2023). Microalgae-based wastewater treatment: Mechanisms, challenges, recent advances, and future prospects. Environmental Science and Ecotechnology, 13, 100205. https://doi.org/10.1016/j.ese.2022.100205\nCapece, A., Romaniello, R., Siesto, G., & Romano, P. (2018). Conventional and non-conventional yeasts in beer production. Fermentation, 4(2), 38. https://doi.org/10.3390/fermentation4020038\nGupta, V., Sengupta, M., Prakash, J., & Tripathy, B. C. (2017). An introduction to biotechnology. En Basic and Applied Aspects of Biotechnology (pp.¬†1-21). Springer Singapore. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7119977/\nRenneberg, R., Berkling, V., & Loroch, V. (2017). Viruses, antibodies, and vaccines. En Biotechnology for Beginners (pp.¬†165-200). Elsevier. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7149558/\nThe Editors of Encyclop√¶dia Britannica. (2023). Biotechnology. Britannica. https://www.britannica.com/technology/biotechnology\nThe text in this publication is free to use under license Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/linear-algebra/index.html",
    "href": "posts/linear-algebra/index.html",
    "title": "Basic Linear Algebra with R",
    "section": "",
    "text": "Linear algebra is the study of vectors and linear functions. In this post, I‚Äôm going to show you how to perform some basic linear algebra operations with R code.\n}"
  },
  {
    "objectID": "posts/linear-algebra/index.html#define-vectors",
    "href": "posts/linear-algebra/index.html#define-vectors",
    "title": "Basic Linear Algebra with R",
    "section": "Define vectors",
    "text": "Define vectors\nYou can define vectors as follows:\n\n# Define two vectors\nx &lt;- c(30, 20, 40, 10)\ny &lt;- c(20, 15, 18, 40)"
  },
  {
    "objectID": "posts/linear-algebra/index.html#sum",
    "href": "posts/linear-algebra/index.html#sum",
    "title": "Basic Linear Algebra with R",
    "section": "Sum",
    "text": "Sum\nThe sum of two vectors is a mathematical operation that involves adding the corresponding components of the two vectors. Let \\(\\mathbf{A} = (a_1, a_2, \\ldots, a_n)\\) and \\(\\mathbf{B} = (b_1, b_2, \\ldots, b_n)\\) be two vectors in \\(\\mathbb{R}^n\\). The sum of \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\) is denoted as \\(\\mathbf{A} + \\mathbf{B}\\) and is calculated as follows:\n\\(\\mathbf{A} + \\mathbf{B} = (a_1 + b_1, a_2 + b_2, \\ldots, a_n + b_n)\\)\nIn other words, the sum of two vectors produces a new vector whose components are the sum of the corresponding components of the original vectors.\nSome important properties of vector addition include commutativity (\\(\\mathbf{A} + \\mathbf{B} = \\mathbf{B} + \\mathbf{A}\\)), associativity \\((\\mathbf{A} + \\mathbf{B}) + \\mathbf{C} = \\mathbf{A} + (\\mathbf{B} + \\mathbf{C})\\), and the existence of a zero vector (\\(\\mathbf{0} + \\mathbf{A} = \\mathbf{A} + \\mathbf{0} = \\mathbf{A}\\), where \\(\\mathbf{0}\\) is the zero vector).\nVector addition is a fundamental operation in linear algebra and is used in various contexts, such as representing displacements in physics, combining forces, and defining more advanced operations in vector spaces.\nWith R code you can add vectors with the same length:\n\nx + y\n\n[1] 50 35 58 50\n\n\nIf the vectors don‚Äôt have the same length, the elements of the smallest will be recycled:\n\nx + c(10, 10)\n\n[1] 40 30 50 20"
  },
  {
    "objectID": "posts/linear-algebra/index.html#multiplying-a-vector-by-a-scalar",
    "href": "posts/linear-algebra/index.html#multiplying-a-vector-by-a-scalar",
    "title": "Basic Linear Algebra with R",
    "section": "Multiplying a vector by a scalar",
    "text": "Multiplying a vector by a scalar\nThe multiplication of a vector by a scalar is a mathematical operation that involves multiplying each component of the vector by the scalar. Let \\(\\mathbf{V}\\) be a vector and \\(k\\) be a scalar; the multiplication of the vector by the scalar is denoted as \\(k \\cdot \\mathbf{V}\\) and is calculated as follows:\nIf \\(\\mathbf{V} = (v_1, v_2, \\ldots, v_n)\\), then\n\\(k \\cdot \\mathbf{V} = (k \\cdot v_1, k \\cdot v_2, \\ldots, k \\cdot v_n)\\)\nIn other words, each component of the original vector is multiplied by the scalar, producing a new vector whose components are the product of each component of the original vector by the scalar.\nThis operation has various properties, such as associativity \\((a \\cdot b) \\cdot \\mathbf{V} = a \\cdot (b \\cdot \\mathbf{V})\\), distributivity with respect to the sum of vectors \\(k \\cdot (\\mathbf{A} + \\mathbf{B}) = k \\cdot \\mathbf{A} + k \\cdot \\mathbf{B}\\), and distributivity with respect to the sum of scalars \\((a + b) \\cdot \\mathbf{V} = a \\cdot \\mathbf{V} + b \\cdot \\mathbf{V}\\).\nTo multiply by a scalar, we use the operator * as follows:\n\n10 * x\n\n[1] 300 200 400 100"
  },
  {
    "objectID": "posts/linear-algebra/index.html#dot-or-inner-product",
    "href": "posts/linear-algebra/index.html#dot-or-inner-product",
    "title": "Basic Linear Algebra with R",
    "section": "Dot or inner product",
    "text": "Dot or inner product\nThe dot product, also known as the scalar product or inner product, is a mathematical operation between two vectors that results in a scalar (a single number). The dot product of two vectors \\(\\mathbf{A} = (a_1, a_2, \\ldots, a_n)\\) and \\(\\mathbf{B} = (b_1, b_2, \\ldots, b_n)\\) is commonly denoted as \\(\\mathbf{A} \\cdot \\mathbf{B}\\) and is calculated as follows:\n\\(\\mathbf{A} \\cdot \\mathbf{B} = a_1 \\cdot b_1 + a_2 \\cdot b_2 + \\ldots + a_n \\cdot b_n\\)\nIn other words, you multiply the corresponding components of the two vectors and sum these products. The result is a scalar, not a vector.\nSome important properties of the dot product include commutativity (\\(\\mathbf{A} \\cdot \\mathbf{B} = \\mathbf{B} \\cdot \\mathbf{A}\\)), distributivity with respect to the sum of vectors (\\(\\mathbf{A} \\cdot (\\mathbf{B} + \\mathbf{C}) = \\mathbf{A} \\cdot \\mathbf{B} + \\mathbf{A} \\cdot \\mathbf{C}\\)), and scalar multiplication (\\(k \\cdot (\\mathbf{A} \\cdot \\mathbf{B}) = (k \\cdot \\mathbf{A}) \\cdot \\mathbf{B} = \\mathbf{A} \\cdot (k \\cdot \\mathbf{B})\\)), where (k) is a scalar.\nThe dot product has various applications in geometry, physics, and other mathematical disciplines, and it is fundamental in defining concepts such as vector length, vector projection, and angles between vectors.\nIn R we use the operator %*% to perform the dot product:\n\nx %*% y\n\n     [,1]\n[1,] 2020\n\n\nNote that this operator returns an object with the classes ‚Äúmatrix‚Äù and ‚Äúarray‚Äù:\n\nclass(x %*% y)\n\n[1] \"matrix\" \"array\" \n\n\nIf you need just the numeric value, use the as.numeric() function:\n\nas.numeric(x %*% y)\n\n[1] 2020"
  },
  {
    "objectID": "posts/linear-algebra/index.html#norm",
    "href": "posts/linear-algebra/index.html#norm",
    "title": "Basic Linear Algebra with R",
    "section": "Norm",
    "text": "Norm\nThe norm of a vector, also known as magnitude or length, is a measure that indicates the absolute size of the vector in Euclidean space. The norm of a vector \\(\\mathbf{v}\\) in an \\(n\\)-dimensional space, commonly denoted as \\(\\|\\mathbf{v}\\|\\) or \\(\\|\\mathbf{v}\\|_2\\), is calculated using the following formula:\n\\(\\|\\mathbf{v}\\| = \\sqrt{v_1^2 + v_2^2 + \\ldots + v_n^2}\\)\nIn other words, the norm of a vector is the square root of the sum of the squares of its individual components. This is derived from the Euclidean distance in \\(n\\)-dimensional space.\nSome important properties of the norm include:\n\n\\(\\|\\mathbf{v}\\| \\geq 0\\): The norm of a vector is always non-negative.\n\\(\\|\\mathbf{v}\\| = 0\\) if and only if \\(\\mathbf{v} = \\mathbf{0}\\), where \\(\\mathbf{0}\\) is the zero vector.\n\\(\\|\\alpha \\cdot \\mathbf{v}\\| = |\\alpha| \\cdot \\|\\mathbf{v}\\|\\) for any scalar \\(\\alpha\\).\nTriangle Inequality: \\(\\|\\mathbf{u} + \\mathbf{v}\\| \\leq \\|\\mathbf{u}\\| + \\|\\mathbf{v}\\|\\).\n\nWith R code, the norm or magnitude of a vector can be obtained as follows:\n\nsqrt(x %*% x)\n\n         [,1]\n[1,] 54.77226\n\n\nWith a vector defined as a matrix object, you can also obtain the magnitude with the function norm :\n\nnorm(as.matrix(x), type = \"F\")\n\n[1] 54.77226"
  },
  {
    "objectID": "posts/linear-algebra/index.html#vector-and-scalar-projections",
    "href": "posts/linear-algebra/index.html#vector-and-scalar-projections",
    "title": "Basic Linear Algebra with R",
    "section": "2.6 Vector and Scalar Projections",
    "text": "2.6 Vector and Scalar Projections\nThe scalar projection of a vector s onto a vector r can be obtained with the next code:\n\nr &lt;- c(3, -4, 0)\ns &lt;- c(10, 5, -6)\n\nr %*% s / sqrt(r %*% r)\n\n     [,1]\n[1,]    2\n\n\nIn a similar way, you can obtain the vector projection of s onto r:\n\n(r %*% s / r %*% r) %*% r\n\n     [,1] [,2] [,3]\n[1,]  1.2 -1.6    0\n\n\nThe code on this post is licensed under the Creative Commons Attribution 4.0 International License"
  },
  {
    "objectID": "posts/linear-algebra/index.html#vectors",
    "href": "posts/linear-algebra/index.html#vectors",
    "title": "Basic Linear Algebra with R",
    "section": "Vectors",
    "text": "Vectors\nA vector is a mathematical object that possesses both magnitude and direction. In other words, a vector is a mathematical entity representing a physical quantity with both a size (or magnitude) and a specific orientation in space. Vectors are used to describe displacements, velocities, forces, and other concepts in mathematics, physics, and other disciplines.\nFormally, a vector in three-dimensional space \\(\\mathbb{R}^3\\) can be represented as an ordered triplet of real numbers \\((a, b, c)\\), where \\(a\\), \\(b\\), and \\(c\\) are the components of the vector along the x, y, and z axes, respectively. In general, in an \\(n\\)-dimensional space \\(\\mathbb{R}^n\\), a vector is represented as an \\(n\\)-tuple \\((a_1, a_2, \\ldots, a_n)\\).\nVectors can also be geometrically represented as arrows in a coordinate system, where the length of the arrow represents the magnitude of the vector, and the direction of the arrow represents the orientation of the vector in space.\nTo define vectors, in this case in twodimensions, we can use the function c():\n\n# Define three vectors in two dimensions\na &lt;- c(-3, 1)\nb &lt;- c(1, -3)\nc &lt;- c(-2, 2)\n\n# Display the vectors\na\n\n[1] -3  1\n\nb\n\n[1]  1 -3\n\nc\n\n[1] -2  2\n\n\nThe above vectors can be represented visually as follows (code not shown):\n\n\n\n\n\nTo create vectors in four dimensions, we use the same procedure, but add more elements to each vector:\n\n# Define two vectors in fourth dimensions\nx &lt;- c(30, 20, 40, 10)\ny &lt;- c(20, 15, 18, 40)\n\nThe above procedure can be generalized to any vector in \\(\\mathbb{R}^n\\)."
  },
  {
    "objectID": "posts/linear-algebra/index.html#scalar-projections",
    "href": "posts/linear-algebra/index.html#scalar-projections",
    "title": "Basic Linear Algebra with R",
    "section": "Scalar Projections",
    "text": "Scalar Projections\nThe scalar projection of a vector \\(\\mathbf{s}\\) onto another vector \\(\\mathbf{r}\\) is the length of the projection of \\(\\mathbf{s}\\) onto the direction of \\(\\mathbf{r}\\). It is commonly denoted as \\(\\text{proj}_{\\mathbf{r}}(\\mathbf{s})\\) and is calculated using the following formula:\n\\(\\text{proj}_{\\mathbf{r}}(\\mathbf{s}) = \\frac{\\mathbf{s} \\cdot \\mathbf{r}}{\\|\\mathbf{r}\\|}\\)\nWhere:\n\n\\(\\mathbf{s} \\cdot \\mathbf{r}\\) denotes the dot product between the two vectors.\n\\(\\|\\mathbf{r}\\|\\) is the norm (magnitude) of \\(\\mathbf{r}\\).\n\nThe scalar projection of \\(\\mathbf{s}\\) onto \\(\\mathbf{r}\\) represents the length of the line segment connecting the origin to the point where \\(\\mathbf{s}\\) projects onto the line following the direction of \\(\\mathbf{r}\\).\nIn R, the scalar projection of a vector s onto a vector r can be obtained with the next code:\n\nr &lt;- c(3, -4, 0)\ns &lt;- c(10, 5, -6)\n\nr %*% s / sqrt(r %*% r)\n\n     [,1]\n[1,]    2\n\n\nIn a similar way, you can obtain the vector projection of s onto r using the function norm():\n\nr %*% s / norm(as.matrix(r), type = \"F\")\n\n     [,1]\n[1,]    2"
  },
  {
    "objectID": "posts/linear-algebra/index.html#matrices",
    "href": "posts/linear-algebra/index.html#matrices",
    "title": "Basic Linear Algebra with R",
    "section": "Matrices",
    "text": "Matrices\nIn linear algebra, a matrix is a two-dimensional array of numbers, symbols, or mathematical expressions arranged in rows and columns. A matrix is typically represented by uppercase letters, such as A, B, C, etc., and its elements are denoted by indices indicating the position of a number in the matrix.\nFor example, a matrix A of size m √ó n is denoted as:\n\\(A = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{bmatrix}\\)\nHere, \\(a_{ij}\\) represents the element in the ith row and jth column of the matrix. In this context, m is the number of rows, and n is the number of columns. When the number of rows is equal to the number of columns (m = n), it is called a square matrix.\nMatrices are used in linear algebra to represent and manipulate systems of linear equations, linear transformations, and in various contexts in mathematics, physics, statistics, computer science, and other disciplines. Basic operations with matrices include addition, subtraction, scalar multiplication, and matrix multiplication.\nIn R, we use the function matrix() to define matrices:\n\n# Define a matrix\nm &lt;- c(7, -6, 12, 8)\nm &lt;- matrix(m, nrow = 2, byrow = TRUE)\nm\n\n     [,1] [,2]\n[1,]    7   -6\n[2,]   12    8"
  },
  {
    "objectID": "posts/linear-algebra/index.html#matrix-multiplication-by-a-vector",
    "href": "posts/linear-algebra/index.html#matrix-multiplication-by-a-vector",
    "title": "Basic Linear Algebra with R",
    "section": "Matrix multiplication by a vector",
    "text": "Matrix multiplication by a vector\nMatrix-vector multiplication is a fundamental operation in linear algebra. Given a matrix \\(A\\) of dimensions \\(m \\times n\\) and a column vector \\(\\mathbf{v}\\) of dimension \\(n \\times 1\\), the product of the matrix by the vector, denoted as \\(A \\mathbf{v}\\), results in a new column vector of dimension \\(m \\times 1\\).\nThe operation is performed by multiplying each element in a matrix row by the corresponding element in the vector and summing the resulting products. The element in the \\(i\\)-th position of the resulting vector is the sum of the products of the elements in the \\(i\\)-th row of the matrix and the corresponding elements in the vector.\nThe mathematical expression for the multiplication of a matrix \\(A\\) by a column vector \\(\\mathbf{v}\\) can be written as follows:\n\\(A \\mathbf{v} = \\begin{bmatrix} a_{11} & a_{12} & \\dots & a_{1n} \\\\ a_{21} & a_{22} & \\dots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\dots & a_{mn} \\end{bmatrix} \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} = \\begin{bmatrix} (a_{11}v_1 + a_{12}v_2 + \\dots + a_{1n}v_n) \\\\ (a_{21}v_1 + a_{22}v_2 + \\dots + a_{2n}v_n) \\\\ \\vdots \\\\ (a_{m1}v_1 + a_{m2}v_2 + \\dots + a_{mn}v_n) \\end{bmatrix}\\)\nIt‚Äôs important to note that matrix-vector multiplication is defined when the number of columns in the matrix is equal to the number of rows in the vector. In this case, the result is a new vector with the same number of rows as the matrix and the same number of columns as the original vector.\nTo perform this operation in R, we use the operator %*%:\n\n# Matrix multiplication by a vector\nv &lt;- c(5, 6)\nm %*% v\n\n     [,1]\n[1,]   -1\n[2,]  108"
  },
  {
    "objectID": "posts/linear-algebra/index.html#matrix-multiplication",
    "href": "posts/linear-algebra/index.html#matrix-multiplication",
    "title": "Basic Linear Algebra with R",
    "section": "Matrix multiplication",
    "text": "Matrix multiplication\nMatrix multiplication is an algebraic operation that combines two matrices to produce a third matrix. Given two matrices \\(A\\) and \\(B\\), where matrix \\(A\\) has dimensions \\(m \\times n\\) and matrix \\(B\\) has dimensions \\(n \\times p\\), the product of \\(A\\) and \\(B\\), denoted as \\(AB\\), results in a new matrix \\(C\\) with dimensions \\(m \\times p\\).\nThe operation is performed by multiplying each element in a row of the first matrix by the corresponding element in the column of the second matrix and summing the resulting products. The entry in the \\(i\\)-th row and \\(j\\)-th column of the resulting matrix \\(C\\) is the result of this sum for the intersection of the \\(i\\)-th row of \\(A\\) and the \\(j\\)-th column of \\(B\\).\nThe mathematical expression for matrix multiplication can be represented as follows:\n\\(C_{ij} = \\sum_{k=1}^{n} A_{ik} \\cdot B_{kj}\\)\nwhere \\(C_{ij}\\) is the element in the \\(i\\)-th row and \\(j\\)-th column of matrix \\(C\\), \\(A_{ik}\\) is the element in the \\(i\\)-th row and \\(k\\)-th column of matrix \\(A\\), and \\(B_{kj}\\) is the element in the \\(k\\)-th row and \\(j\\)-th column of matrix \\(B\\).\nIt is important to note that for matrix multiplication to be defined, the number of columns in the first matrix must be equal to the number of rows in the second matrix. The resulting matrix has the number of rows of the first matrix and the number of columns of the second matrix.\nTo perform multiplications between matrices, we use the operator %*%. Always be sure that the number of columns in the first matrix is the same as the number of rows in the second:\n\n# A matrix of 2 x 3\nA &lt;- matrix(c(1, 2, 3, 4, 0, 1), nrow = 2, byrow = TRUE)\nA\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    0    1\n\n# A matrix of 3 x 3\nB &lt;- matrix(c(1, 1, 0, 0, 1, 1, 1, 0, 1), nrow = 3, byrow = TRUE)\nB\n\n     [,1] [,2] [,3]\n[1,]    1    1    0\n[2,]    0    1    1\n[3,]    1    0    1\n\n# Matrix multiplication\nA %*% B\n\n     [,1] [,2] [,3]\n[1,]    4    3    5\n[2,]    5    4    1"
  },
  {
    "objectID": "posts/linear-algebra/index.html#multiplication-of-a-matrix-by-a-scalar",
    "href": "posts/linear-algebra/index.html#multiplication-of-a-matrix-by-a-scalar",
    "title": "Basic Linear Algebra with R",
    "section": "Multiplication of a matrix by a scalar",
    "text": "Multiplication of a matrix by a scalar\nScalar multiplication by a matrix is an operation in which each element of the matrix is multiplied by the given scalar. Given a scalar \\(c\\) and a matrix \\(A\\) of dimensions \\(m \\times n\\), the product of \\(c\\) by \\(A\\), denoted as \\(cA\\), is obtained by multiplying each element of \\(A\\) by \\(c\\), resulting in a new matrix of the same size \\(m \\times n\\).\nThe mathematical expression for scalar multiplication by a matrix can be written as:\n\\(c \\cdot A = \\begin{bmatrix} c \\cdot a_{11} & c \\cdot a_{12} & \\dots & c \\cdot a_{1n} \\\\ c \\cdot a_{21} & c \\cdot a_{22} & \\dots & c \\cdot a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ c \\cdot a_{m1} & c \\cdot a_{m2} & \\dots & c \\cdot a_{mn} \\end{bmatrix}\\)\nWhere \\(a_{ij}\\) represents the elements of matrix \\(A\\). Each element of the resulting matrix \\(cA\\) is simply the product of the scalar \\(c\\) with the corresponding element of matrix \\(A\\).\nAs with vectors, we use the operator * to perfrom this operation:\n\n# Matrix multiplication by a scalar\n100 * A\n\n     [,1] [,2] [,3]\n[1,]  100  200  300\n[2,]  400    0  100"
  },
  {
    "objectID": "posts/linear-algebra/index.html#range-of-a-matrix",
    "href": "posts/linear-algebra/index.html#range-of-a-matrix",
    "title": "Basic Linear Algebra with R",
    "section": "Range of a matrix",
    "text": "Range of a matrix\nThe rank of a matrix is a measure indicating the maximum number of linearly independent rows or columns in the matrix. We can calculate the rank for both rectangular and square matrices. The specific definition may vary depending on whether you are dealing with the row rank or column rank, but both are related.\n\nRow Rank Definition: The row rank of a matrix is the maximum number of linearly independent rows in the matrix.\nColumn Rank Definition: The column rank of a matrix is the maximum number of linearly independent columns in the matrix.\n\nIn more general terms, the rank of a matrix can be defined as the dimension of the space spanned by its row or column vectors. An important fact about the rank of a matrix is that the row rank is always equal to the column rank. This is known as the rank theorem and is a crucial property highlighting the relationship between the rows and columns of a matrix.\nTo calculate the rank of a matrix we use the function qr() as follows:\n\n# A matrix of 3 x 3\nC &lt;- matrix(c(1, 0, 1, -2, -3, 1, 3, 3, 0), nrow = 3, byrow = TRUE)\nC\n\n     [,1] [,2] [,3]\n[1,]    1    0    1\n[2,]   -2   -3    1\n[3,]    3    3    0\n\n# Rank of matrix C\nqr(C)$rank\n\n[1] 2\n\n# A matrix of 2 x 4 \nD &lt;- matrix(c(1, 1, 0, 2, -1, -1, 0, -2), nrow = 2, byrow = TRUE)\nD\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    1    0    2\n[2,]   -1   -1    0   -2\n\n# Rank of matrix D\nqr(D)$rank\n\n[1] 1"
  },
  {
    "objectID": "posts/linear-algebra/index.html#solving-systems-of-linear-equations",
    "href": "posts/linear-algebra/index.html#solving-systems-of-linear-equations",
    "title": "Basic Linear Algebra with R",
    "section": "Solving systems of linear equations",
    "text": "Solving systems of linear equations\nA system of linear equations is a set of two or more linear equations that share a common set of variables. Each linear equation within the system represents a linear relationship between the variables. In general, a system of linear equations can be expressed as follows:\n\\(\\begin{cases} a_{11}x_1 + a_{12}x_2 + \\ldots + a_{1n}x_n = b_1 \\\\ a_{21}x_1 + a_{22}x_2 + \\ldots + a_{2n}x_n = b_2 \\\\ \\vdots \\\\ a_{m1}x_1 + a_{m2}x_2 + \\ldots + a_{mn}x_n = b_m \\end{cases}\\)\nWhere:\n\n\\(x_1, x_2, \\ldots, x_n\\) are the variables of the system.\n\\(a_{ij}\\) are the coefficients accompanying the variables.\n\\(b_i\\) are the constant terms in each equation.\nThe system has \\(m\\) linear equations.\n\nThe typical goal when solving a system of linear equations is to find the values of the variables \\(x_1, x_2, \\ldots, x_n\\) that satisfy all the equations simultaneously. These values represent the solution to the system. Depending on the nature of the system, it may have a unique solution, no solution, or infinitely many solutions. Common methods for solving systems of linear equations include substitution, elimination, and the use of matrices.\nNow let us define the following system of linear equations. We want to obtain the values of \\(a\\), \\(b\\), and \\(c\\) that satisfied the system:\n\n\\((1): a + b + c = 15\\)\n\\((2): 3a + 2b + c = 28\\)\n\\((3): 2a + b + 2c = 23\\)\n\nFirst, define a matrix with the system coefficients and a vector with the results or constants for each equation:\n\n# Coefficient matrix\nC &lt;- matrix(c(1, 1, 1, 3, 2, 1, 2, 1, 2), nrow = 3, byrow = TRUE)\nC\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    3    2    1\n[3,]    2    1    2\n\n# Constant vector \nr &lt;- c(15, 28, 23)\nr\n\n[1] 15 28 23\n\n\nNext, obtain the inverse of the coefficient matrix using the solve() function:\n\n# Coefficient matrix inverse\nC_inv &lt;- solve(C)\nC_inv\n\n     [,1] [,2] [,3]\n[1,] -1.5  0.5  0.5\n[2,]  2.0  0.0 -1.0\n[3,]  0.5 -0.5  0.5\n\n\nTo calculate the solution, we multiply the previous inverse matrix by the constant vector:\n\n# Solution\ns &lt;- C_inv %*% r\ns\n\n     [,1]\n[1,]    3\n[2,]    7\n[3,]    5\n\n\nThus the solution to our system of equations is as follows:\n\n\\(a = 3\\)\n\\(b = 7\\)\n\\(c = 5\\)"
  },
  {
    "objectID": "posts/linear-algebra/index.html#determinant-of-a-matrix",
    "href": "posts/linear-algebra/index.html#determinant-of-a-matrix",
    "title": "Basic Linear Algebra with R",
    "section": "Determinant of a matrix",
    "text": "Determinant of a matrix\nThe determinant of a matrix is a scalar associated with that matrix and is commonly denoted as \\(\\text{det}(A)\\) or \\(|A|\\). The process for calculating the determinant varies depending on the size of the matrix.\n\nFirst-Order Matrix:\n\nFor a \\(1 \\times 1\\) matrix, which is simply a number, the determinant is equal to that number.\n$\\text{det}([a]) = a$\n\nSecond-Order Matrices:\n\nFor a \\(2 \\times 2\\) matrix:\n$\\text{det}\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = ad - bc$\n\nHigher-Order Matrices:\n\nFor an \\(n \\times n\\) square matrix \\(A\\), the determinant can be calculated using cofactor expansion or more advanced methods such as diagonalization.\nThe general formula for the determinant of an \\(n \\times n\\) matrix \\(A\\) can be expressed recursively in terms of minors and cofactors:\n$\\text{det}(A) = \\sum_{i=1}^{n} (-1)^{i+j} \\cdot a_{ij} \\cdot \\text{det}(M_{ij})$\nWhere:\n\n\\(a_{ij}\\) is the element in row \\(i\\), column \\(j\\) of matrix \\(A\\).\n\\(\\text{det}(M_{ij})\\) is the determinant of the minor obtained by removing row \\(i\\) and column \\(j\\) from matrix \\(A\\).\n\nTo carry out this operation, we use the function det():\n\nC\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    3    2    1\n[3,]    2    1    2\n\ndet(C)\n\n[1] -2"
  },
  {
    "objectID": "posts/linear-algebra/index.html#transpose-of-a-matrix",
    "href": "posts/linear-algebra/index.html#transpose-of-a-matrix",
    "title": "Basic Linear Algebra with R",
    "section": "Transpose of a Matrix",
    "text": "Transpose of a Matrix\nThe transpose of a matrix is an operation that involves swapping its rows and columns. It is commonly denoted by \\(A^T\\) or \\(A'\\). If \\(A\\) is an \\(m \\times n\\) matrix, then the transpose \\(A^T\\) is an \\(n \\times m\\) matrix obtained by exchanging the rows and columns of the original matrix.\nMathematically, if \\(A\\) has elements \\(a_{ij}\\), then the transpose \\(A^T\\) has elements \\(b_{ij}\\) where \\(b_{ij} = a_{ji}\\). In other words, the element in row \\(i\\), column \\(j\\) of \\(A^T\\) is the same as the element in row \\(j\\), column \\(i\\) of \\(A\\).\nFor example, if \\(A\\) is:\n\\(A = \\begin{bmatrix} 1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\\)\nThen, the transpose \\(A^T\\) would be:\n\\(A^T = \\begin{bmatrix} 1 & 4 \\\\ 2 & 5 \\\\ 3 & 6 \\end{bmatrix}\\)\nSome important properties of the transpose include:\n\n\\((A^T)^T = A\\)\n\\((cA)^T = cA^T\\), where \\(c\\) is a constant.\n\\((A + B)^T = A^T + B^T\\), where \\(A\\) and \\(B\\) are matrices compatible for addition.\n\nWe use the t() function to obtain the transpose of a matrix:\n\nC\n\n     [,1] [,2] [,3]\n[1,]    1    1    1\n[2,]    3    2    1\n[3,]    2    1    2\n\nt(C)\n\n     [,1] [,2] [,3]\n[1,]    1    3    2\n[2,]    1    2    1\n[3,]    1    1    2"
  },
  {
    "objectID": "posts/linear-algebra/index.html#identity-matrix",
    "href": "posts/linear-algebra/index.html#identity-matrix",
    "title": "Basic Linear Algebra with R",
    "section": "Identity matrix",
    "text": "Identity matrix\nThe identity matrix, commonly denoted by \\(I_n\\) or simply \\(I\\), is an \\(n \\times n\\) square matrix that has ones on its main diagonal (from the upper-left to the lower-right) and zeros in the rest of its elements. In other words, an identity matrix is a square matrix where all elements outside the main diagonal are zero, and all elements on the main diagonal are one.\nThe general form of an \\(n \\times n\\) identity matrix is:\n\\(I_n = \\begin{bmatrix} 1 & 0 & \\ldots & 0 \\\\ 0 & 1 & \\ldots & 0 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & \\ldots & 1 \\end{bmatrix}\\)\nWhere \\(n\\) represents the size of the identity matrix.\nIt is important to note that the distinctive property of the identity matrix is that when any square matrix \\(A\\) is multiplied by the appropriate identity matrix, the result is the same matrix \\(A\\). In mathematical terms, if \\(A\\) is an \\(n \\times m\\) matrix, then \\(I_n \\times A = A \\times I_m = A\\), provided that the dimensions are compatible for multiplication.\nTo define a identity matrix we use the diag() function:\n\n# Identity matrix of 2 x 2\ndiag(2)\n\n     [,1] [,2]\n[1,]    1    0\n[2,]    0    1\n\n# Identity matrix of 4 x 4\ndiag(4)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1"
  },
  {
    "objectID": "posts/linear-algebra/index.html#eigenvalues-and-eingenvectors",
    "href": "posts/linear-algebra/index.html#eigenvalues-and-eingenvectors",
    "title": "Basic Linear Algebra with R",
    "section": "Eigenvalues and eingenvectors",
    "text": "Eigenvalues and eingenvectors\nThe concepts of eigenvalues and eigenvectors are fundamental in linear algebra and apply to linear transformations represented by matrices.\n\nEigenvalues:\n\nGiven a square matrix \\(A\\), an eigenvalue \\(\\lambda\\) is a scalar such that when multiplied by a nonzero vector \\(v\\), the result is simply the same vector scaled by \\(\\lambda\\):\n$A \\mathbf{v} = \\lambda \\mathbf{v}$\nHere, \\(\\mathbf{v}\\) is the corresponding eigenvector associated with the eigenvalue \\(\\lambda\\). It‚Äôs important to note that for an eigenvalue and its corresponding eigenvector, the multiplication of matrix \\(A\\) by vector \\(\\mathbf{v}\\) results in a mere scaling of the vector by the eigenvalue.\n\nEigenvectors:\n\nGiven an eigenvalue \\(\\lambda\\), the corresponding eigenvector \\(\\mathbf{v}\\) is a nonzero vector that satisfies the equation:\n$A \\mathbf{v} = \\lambda \\mathbf{v}$\nIn other words, when matrix \\(A\\) is multiplied by the eigenvector \\(\\mathbf{v}\\), the result is simply the same vector scaled by the eigenvalue \\(\\lambda\\).\nEigenvectors provide special directions in which the linear transformation represented by matrix \\(A\\) has only a scaling effect. The magnitude of this scaling is determined by the corresponding eigenvalue.\nWe use the function eigen() to obtain the eigenvectors and eigenvalues of a given matrix:\n\n# A matrix of 2 x 2\nA &lt;- matrix(c(3, 4, 0, 5), nrow = 2, byrow = TRUE)\nA\n\n     [,1] [,2]\n[1,]    3    4\n[2,]    0    5\n\n# eigen function\nA_ei &lt;- eigen(A)\n\n# eigenvalues\nA_lmd &lt;- A_ei$values\nA_lmd\n\n[1] 5 3\n\n# eigenvectors\nA_ev &lt;- A_ei$vectors\nA_ev\n\n          [,1] [,2]\n[1,] 0.8944272    1\n[2,] 0.4472136    0\n\n\nInterestingly, the original matrix can be decomposed as follows:\n\nA_ev %*% diag(A_lmd) %*% solve(A_ev)\n\n     [,1] [,2]\n[1,]    3    4\n[2,]    0    5"
  },
  {
    "objectID": "posts/linear-algebra/index.html#gram-schmidt-process",
    "href": "posts/linear-algebra/index.html#gram-schmidt-process",
    "title": "Basic Linear Algebra with R",
    "section": "Gram-Schmidt process",
    "text": "Gram-Schmidt process\nThe Gram-Schmidt process is a method used in linear algebra to take a set of linearly independent vectors and construct from them an orthogonal or orthonormal set. This process is useful in various contexts, such as matrix diagonalization, solving systems of linear equations, and QR factorization.\nGiven a set of vectors \\(\\{\\mathbf{v}_1, \\mathbf{v}_2, \\ldots, \\mathbf{v}_n\\}\\) in a Euclidean vector space, the Gram-Schmidt process generates an orthogonal set \\(\\{\\mathbf{u}_1, \\mathbf{u}_2, \\ldots, \\mathbf{u}_n\\}\\) or an orthonormal set \\(\\{\\mathbf{q}_1, \\mathbf{q}_2, \\ldots, \\mathbf{q}_n\\}\\) of vectors.\nThe Gram-Schmidt process is valuable in situations where we need to work with sets of vectors that are not orthogonal initially but desires to obtain orthogonal sets to simplify calculations and analyses.\nTo perform this process, you can use the gramSchmidt() function from pracma package:\n\n#install.packages(\"pracma\")\nlibrary(pracma)\n\n# A matrix of 3 x 3\nA &lt;- matrix(c(2, -2, 18, 2, 1, 0, 1, 2, 0), nrow = 3, byrow = TRUE)\nA\n\n     [,1] [,2] [,3]\n[1,]    2   -2   18\n[2,]    2    1    0\n[3,]    1    2    0\n\n# gramSchmidt function\nA_gs &lt;- gramSchmidt(A)\n\n# Orthogonalized matrix\nA_gs$Q\n\n          [,1]       [,2]       [,3]\n[1,] 0.6666667 -0.6666667  0.3333333\n[2,] 0.6666667  0.3333333 -0.6666667\n[3,] 0.3333333  0.6666667  0.6666667\n\n# Upper triangular matrix \nA_gs$R\n\n     [,1] [,2] [,3]\n[1,]    3    0   12\n[2,]    0    3  -12\n[3,]    0    0    6\n\n\nNote that the above operation allows us to decompose the original matrix such that:\n\nA_gs$Q %*% A_gs$R\n\n     [,1] [,2] [,3]\n[1,]    2   -2   18\n[2,]    2    1    0\n[3,]    1    2    0"
  },
  {
    "objectID": "posts/linear-algebra/index.html#mean",
    "href": "posts/linear-algebra/index.html#mean",
    "title": "Basic Linear Algebra with R",
    "section": "Mean",
    "text": "Mean\nLet‚Äôs simulate some data and obtain the mean with the mean() function:\n\n# Data\nset.seed(5)\ny &lt;- runif(100, min = 10, max = 35)\n\n# mean function\nmean(y)\n\n[1] 22.96092\n\n\nUsing vector multiplication, the mean can be calculated as follows:\n\n# Sample size\nyl &lt;- length(y) \n\n# Vector of ones of the same length as y\nv1 &lt;- rep(1, yl)\n\n# Calculation of the mean\ny_mean &lt;- v1 %*% y / yl\ny_mean\n\n         [,1]\n[1,] 22.96092"
  },
  {
    "objectID": "posts/linear-algebra/index.html#sample-variance",
    "href": "posts/linear-algebra/index.html#sample-variance",
    "title": "Basic Linear Algebra with R",
    "section": "Sample variance",
    "text": "Sample variance\nSomething similar can be done with the sample variance. First, let‚Äôs calculate the sample variance with the var() function:\n\n# var function\nvar(y)\n\n[1] 57.33112\n\n\nNow let‚Äôs calculate the sample variance with vector multiplication:\n\n# Differences between the data and the sample mean\nr &lt;- y - as.numeric(y_mean)\n\n# Calculation of the variance\nr %*% r / (yl - 1)\n\n         [,1]\n[1,] 57.33112\n\n\nThis post is licensed under the Creative Commons Attribution 4.0 International License"
  },
  {
    "objectID": "posts/linear-algebra/index.html#inverse-of-a-matrix",
    "href": "posts/linear-algebra/index.html#inverse-of-a-matrix",
    "title": "Basic Linear Algebra with R",
    "section": "Inverse of a matrix",
    "text": "Inverse of a matrix\nThe inverse of a matrix is a fundamental concept in linear algebra. Given a square matrix \\(A\\), \\(A^{-1}\\) is said to be the inverse of \\(A\\) if the product of \\(A\\) by \\(A^{-1}\\) (or \\(A^{-1}\\) by \\(A\\)) is equal to the identity matrix \\(I\\), of the same size as \\(A\\). Mathematically, this is expressed as:\n\\(A \\cdot A^{-1} = A^{-1} \\cdot A = I\\)\nWhere: * \\(A\\) is the original matrix. * \\(A^{-1}\\) is the inverse matrix. * \\(I\\) is the identity matrix.\nIt‚Äôs important to note that not all matrices have an inverse. For a matrix to have an inverse, it must be a square matrix, and it must be nonsingular, meaning its determinant should not be equal to zero.\nIf a matrix \\(A\\) has an inverse, it is denoted as \\(A^{-1}\\), and it satisfies:\n\\(A \\cdot A^{-1} = A^{-1} \\cdot A = I\\)\nThe inverse matrix has important properties, such as the inverse of the inverse (\\((A^{-1})^{-1} = A\\)) and the inverse of the product of matrices (\\((AB)^{-1} = B^{-1}A^{-1}\\)), among others.\nTo calculate the inverse of a matrix we use the solve() function:\n\nset.seed(9)\n# A matrix of 4 x 4 \nE &lt;- matrix(\n  round(runif(16, min = 1, max = 30), 0), \n  nrow = 4, byrow = TRUE\n)\nE\n\n     [,1] [,2] [,3] [,4]\n[1,]    7    2    7    7\n[2,]   14    5   12   12\n[3,]   20   30    4    1\n[4,]   27   10   15   16\n\n# Inverse of matrix E\nE_i &lt;- solve(E)\nE_i\n\n           [,1]       [,2]       [,3]       [,4]\n[1,]  -6.597222   4.472222 -0.1527778 -0.4583333\n[2,]   7.305556  -5.055556  0.1944444  0.5833333\n[3,] -30.625000  21.750000 -0.6250000 -2.8750000\n[4,]  35.277778 -24.777778  0.7222222  3.1666667"
  }
]