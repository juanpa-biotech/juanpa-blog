[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data analysis, science and other stuff ü§ì",
    "section": "",
    "text": "Hello, welcome to my blog.\nMy name is Juan Pablo, I have a PhD in Biotechnology from the Universidad Aut√≥noma Metropolitana, Mexico City.\nIn this blog I share posts with topics of my interest, such as data analysis with R code, science communication and other things that will take shape little by little.\nSome time ago I maintained a blog called ‚ÄúR in the Lab‚Äù focused exclusively on the analysis with R of data obtained in the laboratory of chemical and biological sciences, but after some stumbles I decided to start again with a blog where I could write about any topic.\nPreviously I was using the ‚Äúblogdown‚Äù package, but I must admit that things became a bit tedious, especially the maintenance and the inclusion of new posts. In the search for alternatives I found the fabulous post by Rebecca Barter - Thanks, Quarto, for saving my blog!, which gave me the idea for the design of my new blog.\nIf you have any questions about any of the posts please email me at jpch_26@outlook.com or jpch_biotech@outlook.com. In the About the author section you will find my social networks, you can also contact me there!\nThank you very much for visiting my blog, I hope you find it useful, see you next time! ü§ì\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\nR tutorial: Principal Component Analysis from scratch\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\npca\n\n\nprincipal component analysis\n\n\nlinear algebra\n\n\n\n\nHow to perform PCA step by step using just basic linear algebra functions and operations\n\n\n\n\n\n\nOct 30, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR tutorial: Simultaneus Optimization of Several Response Variables\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\nsimultaneus optimization\n\n\nresponse surface design\n\n\ndesirability package\n\n\n\n\nA tutorial with R code to perform simultaneous optimization of several response variables\n\n\n\n\n\n\nOct 24, 2023\n\n\n\n\n\n\n  \n\n\n\n\nPrickly pear, a sweet source of nutrients\n\n\n\n\n\n\n\nscience communication\n\n\nprickly pear\n\n\n\n\nSome facts about prickly pears\n\n\n\n\n\n\nOct 19, 2023\n\n\n\n\n\n\n  \n\n\n\n\nR Tutorial: Analysis of results of a nested experimental design\n\n\n\n\n\n\n\nR\n\n\ntutorial\n\n\nstatistical inference\n\n\nnested design\n\n\n\n\nA tutorial with R code for analyzing the results of nested designs\n\n\n\n\n\n\nOct 11, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWhat is science? - The way we understand and model reality\n\n\n\n\n\n\n\nscience communication\n\n\nscientific method\n\n\nscientific thinking\n\n\n\n\nSome ideas about science\n\n\n\n\n\n\nSep 28, 2023\n\n\n\n\n\n\n  \n\n\n\n\nStatistical Inference with R\n\n\n\n\n\n\n\nR\n\n\nstatistical inference\n\n\nstatistics\n\n\npopulation\n\n\nsample\n\n\nparameters\n\n\nmean\n\n\nvariance\n\n\nstandard deviation\n\n\n\n\nSome statistical inference concepts and terms explained using R\n\n\n\n\n\n\nSep 27, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About the author",
    "section": "",
    "text": "Hello! Welcome to my personal blog.\nMy name is Juan Pablo. I have a PhD in Biotechnology from the Universidad Aut√≥noma Universidad Aut√≥noma Metropolitana.\nIn this blog I write about various topics of my interest such as data analysis and science in general. I hope you find it useful.\nYou can contact me through my email or my social networks:\njpch_26@outlook.com\njpch_biotech@outlook.com\nLinkedIn\nTwitter\nMy certifications:\nThe R Programming Environment\nIntroducci√≥n a Data Science: Programaci√≥n Estad√≠stica con R\nProgramming in R for Data Science\nExperimentation for Improvement\nProfessional Certificate: Data Analysis for Life Sciences\nI have also published some scientific articles, which can be consulted in my profile at Scholar Google.\nAll text, code and data in the tutorials are freely available under the Creative Commons Attribution 4.0 International License.\nThank you so much for visiting my blog! ü§ì"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn‚Äôt specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/statistical-inference/index.html",
    "href": "posts/statistical-inference/index.html",
    "title": "Statistical Inference with R",
    "section": "",
    "text": "Perfection is always impossible; always it‚Äôs an approximation"
  },
  {
    "objectID": "posts/statistical-inference/index.html#introduction",
    "href": "posts/statistical-inference/index.html#introduction",
    "title": "Statistical Inference with R",
    "section": "Introduction",
    "text": "Introduction\nFormally, statistical inference can be defined as the process through which inferences about a population are made based on certain statistics calculated from a sample of data drawn from that population.\nIf your job is related to any kind of science I‚Äôm positive you know that statistical inference plays an important role in your results validations. A single statement is often used as irrefutable evidence that you have succeeded in your research: ‚Äúthis results showed significant differences (P &lt; 0.05)‚Äù. But does anything showing significant differences really mean anything?\nI‚Äôm going to try to answer these and more questions with some help, of course, of R code."
  },
  {
    "objectID": "posts/statistical-inference/index.html#populations-samples-parameters-and-statistics",
    "href": "posts/statistical-inference/index.html#populations-samples-parameters-and-statistics",
    "title": "Statistical Inference with R",
    "section": "Populations, Samples, Parameters and statistics",
    "text": "Populations, Samples, Parameters and statistics\nFrom Cambridge Dictionary an inference is a guess that you make or an opinion that you form based on the information that you have. Statistically, the objective of an inference is to draw conclusions about population from a sample.\nBy population I mean the complete set of objects of your interest and can be the trees from a particular species or even the elements from a continuous process like chip cookies in a cookies factory (yeah, I like chip cookies). Most of the time, you don‚Äôt have access to the entire population simply because is too big and it would be impractical analyze each element or, as I said, the population can be considered like a continuous.\nThis is the place where the statistical inference can help. All that is needed is a sample of the population, which is called a sample. For this you must be sure that you sampling process ensures that each element in the population has the same chance to be selected. Once you have your sample you may be interested in some population property, so you proceed to measure this feature on each sample element. This property can be the height, some gene expression, the amount of sugar, the hardness, etc.\nFinally, you calculate a number that summarize the measured feature of your sample. These numbers are usually the mean, the variance, and the standard deviation, which as a whole are referred as statistics.\nWith some assumptions, from your statistics you can infer the population parameters. In other words, from your statistics you can estimate the mean, standard deviation and variance of the whole population.\nSee the next figure where I illustrate all process with cookies:"
  },
  {
    "objectID": "posts/statistical-inference/index.html#sample-mean",
    "href": "posts/statistical-inference/index.html#sample-mean",
    "title": "Statistical Inference with R",
    "section": "Sample Mean",
    "text": "Sample Mean\nYou can calculate the sample mean with the next equation:\n\n\n\n\n\nThe previous is the sum (summation) of all sample measurements divided by the sample size (n). i is the index of each element in your sample.\nWith R code in our previous sample:\n\nms_trees &lt;- sum(sample_trees) / length(sample_trees)\nms_trees\n\n[1] 15.552"
  },
  {
    "objectID": "posts/statistical-inference/index.html#sample-variance",
    "href": "posts/statistical-inference/index.html#sample-variance",
    "title": "Statistical Inference with R",
    "section": "Sample Variance",
    "text": "Sample Variance\nFor the sample variance you use the next equation:\n\n\n\n\n\nThis is the summation of the differences between each measurement in the sample and the sample mean. Each difference is squared, so negative and positive differences will not cancel each other. Then you divide the sum by the sample size minus one.\nWith R code:\n\nvs_trees &lt;- sum((sample_trees - ms_trees)^2) / (length(sample_trees) - 1)\nvs_trees\n\n[1] 10.32456\n\n\nIf you are wondering why we have divided by n - 1, I can briefly tell you that this way offers the best estimation of the population variance. You can also check out the next nice page: Why divide by N - 1 when calculating the variance of a sample?. To obtain the standard deviation you simply calculate the squared root of the variance, this way you have a dispersion sample measurement in the original measurement units.\n\nss_trees &lt;- sqrt(vs_trees)\nss_trees\n\n[1] 3.213186"
  },
  {
    "objectID": "posts/statistical-inference/index.html#population-mean",
    "href": "posts/statistical-inference/index.html#population-mean",
    "title": "Statistical Inference with R",
    "section": "Population mean",
    "text": "Population mean\nTo calculate population mean we use the next equation:\n\n\n\n\n\nYou can see that this equation is pretty the same as sample mean, but here N refers to the entire population size. Let‚Äôs calculate the mean in our previous data.\n\nmp_trees &lt;- sum(population_trees) / length(population_trees)\nmp_trees\n\n[1] 14.89803\n\n\nInstead of the previous operation you can use the mean() function with the same purpose:\n\nmean(population_trees)\n\n[1] 14.89803"
  },
  {
    "objectID": "posts/statistical-inference/index.html#population-variance",
    "href": "posts/statistical-inference/index.html#population-variance",
    "title": "Statistical Inference with R",
    "section": "Population variance",
    "text": "Population variance\nThe equation for population variance:\n\n\n\n\n\nNote that here we divide by the entire population size N, not by n-1 as in sample variance. Let‚Äôs calculate this parameter in our previous data:\n\nsum_square &lt;- sum((population_trees - mean(population_trees))^2)\npv_trees &lt;- sum_square / length(population_trees)\npv_trees\n\n[1] 15.39988\n\n\nYou can also calculate this parameter directly with variance() function:\n\nvar(population_trees)\n\n[1] 15.41016\n\n\nThere‚Äôs a sightly difference, but nothing to be worried about.\nCalculating standard deviation is also easy with sd() function:\n\nsd(population_trees)\n\n[1] 3.925577\n\n\nNote that population mean and standard deviation are close to those specified in the data simulation code rnorm(1500, mean = 15, sd = 4).\nIt is also important to mention that the mean(), var() and sd() functions can also be used on any sample data. You can try this as an exercise."
  },
  {
    "objectID": "posts/statistical-inference/index.html#mean-distribution",
    "href": "posts/statistical-inference/index.html#mean-distribution",
    "title": "Statistical Inference with R",
    "section": "Mean distribution",
    "text": "Mean distribution\nThe concept of distribution can also be applied to statistics like mean. From my cookie population let‚Äôs take repeatedly samples of size 5 and then make an histogram:\n\nset.seed(151) # For reproducibility\n\n# 1000 samples\nn_samples &lt;- 1000\n\n# Empty vector where all means will be stored \nmean_cookies &lt;- vector(\"numeric\", length = n_samples)\n\n# Take a random sample of size 5, calculate the mean and save it in previous vector\n# Repeat the process 1000 times\nfor (i in 1:n_samples) {\n  mean_cookies[i] &lt;- mean(sample(diameter$D, size = 5)) \n}\n\n# Make a data frame with all means from samples of size 5\nmean_cookies &lt;- tibble(mean_n5 = mean_cookies)\n\n# Histogram\nfreq_hist_mean &lt;- mean_cookies %&gt;% \n  ggplot(aes(x = mean_n5)) +\n  geom_histogram(color = \"black\", fill = \"white\", binwidth = 0.1) +\n  xlab(\"Diameter (cm)\") +\n  ylab(\"Count\") +\n  theme_classic() \nfreq_hist_mean\n\n\n\n\n\n\n\n\nThe previous code can be repeated with other sample sizes. This will be important in a later section (Central Limit Theorem)."
  },
  {
    "objectID": "posts/statistical-inference/index.html#the-normal-distribution",
    "href": "posts/statistical-inference/index.html#the-normal-distribution",
    "title": "Statistical Inference with R",
    "section": "The normal distribution",
    "text": "The normal distribution\nSince it is rare to know the actual distribution of data, we have to resort to theoretical distributions. There are a lot of such distributions and undoubtedly the normal distribution is the most important in statistical inference. This distribution applies to continuous data (length, weight, temperature, etc.) and is defined by the following equation:\n\n\n\n\n\nA graph of the above equation looks like this (with mean equal to zero and standard deviation equal to one):"
  },
  {
    "objectID": "posts/statistical-inference/index.html#the-students-t-distribution",
    "href": "posts/statistical-inference/index.html#the-students-t-distribution",
    "title": "Statistical Inference with R",
    "section": "The Student‚Äôs t Distribution",
    "text": "The Student‚Äôs t Distribution\nWhen you have small sample sizes and you don‚Äôt know the real deviation standard of the population, you can use the t distribution to made a good supposition of the real distribution. The shape of this distribution depends on the sample size, and for big sample sizes the t distribution tends to be normal:\n\n\n\n\n\n\n\n\n\nThe black line corresponds to a sample size of 2, the red line to a sample size of 4, the blue one to a size of 10, and the green one to a size of 50."
  },
  {
    "objectID": "posts/statistical-inference/index.html#sample-mean-or-sample-average",
    "href": "posts/statistical-inference/index.html#sample-mean-or-sample-average",
    "title": "Statistical Inference with R",
    "section": "Sample Mean or Sample Average",
    "text": "Sample Mean or Sample Average\nWe can calculate the sample mean with the following equation:\n\n\n\n\n\nThe above is the sum (summation) of all the measurements in the sample divided by the sample size (n). i is the index of each item or measurement.\nWith R code in our previous sample:\n\nms_trees &lt;- sum(sample_trees) / length(sample_trees)\nms_trees\n\n[1] 15.552\n\n\nWe can directly use the mean() function with our data to calculate the mean:\n\nmean(sample_trees)\n\n[1] 15.552"
  },
  {
    "objectID": "posts/statistical-inference/index.html#sample-variance-and-dtandard-deviation",
    "href": "posts/statistical-inference/index.html#sample-variance-and-dtandard-deviation",
    "title": "Statistical Inference with R",
    "section": "Sample Variance and Dtandard Deviation",
    "text": "Sample Variance and Dtandard Deviation\nFor the sample variance you use the next equation:\n\n\n\n\n\nThis is described as the sum of the differences between each sample measurement and the sample mean. Each difference is squared, so negative and positive differences do not cancel each other out. The sum is then divided by the sample size minus one.\nWith R code:\n\nvs_trees &lt;- sum((sample_trees - ms_trees)^2) / (length(sample_trees) - 1)\nvs_trees\n\n[1] 10.32456\n\n\nIf you are wondering why we have divided by n - 1, I can briefly tell you that this way offers the best estimation of the population variance. You can also check out the next page: Why divide by N - 1 when calculating the variance of a sample?.\nTo obtain the standard deviation, it is sufficient to calculate the square root of the variance, thus providing a measure of the dispersion of the sample in the original units of measurement:\n\nss_trees &lt;- sqrt(vs_trees)\nss_trees\n\n[1] 3.213186\n\n\nAs in the case of the mean, with R code we can obtain the variance and standard deviation directly with the var() and sd() functions, respectively:\n\n#Sample varianza\nvar(sample_trees)\n\n[1] 10.32456\n\n# Standard deviation \nsd(sample_trees)\n\n[1] 3.213186"
  },
  {
    "objectID": "posts/statistical-inference/index.html#mean-or-average-of-the-population",
    "href": "posts/statistical-inference/index.html#mean-or-average-of-the-population",
    "title": "Statistical Inference with R",
    "section": "Mean or average of the population",
    "text": "Mean or average of the population\nTo calculate population mean we use the next equation:\n\n\n\n\n\nYou can see that this equation is quite similar to the sample mean, but here N refers to all elements of the population. Let‚Äôs calculate the mean in our simulated data:\n\nmp_trees &lt;- sum(population_trees) / length(population_trees)\nmp_trees\n\n[1] 14.89803\n\n\nInstead of the previous operation you can use the mean() function with the same purpose:\n\nmean(population_trees)\n\n[1] 14.89803"
  },
  {
    "objectID": "posts/statistical-inference/index.html#variance-and-standard-deviation-of-the-population",
    "href": "posts/statistical-inference/index.html#variance-and-standard-deviation-of-the-population",
    "title": "Statistical Inference with R",
    "section": "Variance and Standard Deviation of the Population",
    "text": "Variance and Standard Deviation of the Population\nThe equation for population variance:\n\n\n\n\n\nNote that here we divide by the total population size N, not by n-1 as in the sampling variance. Let us calculate this parameter in our previous data:\n\nsum_square &lt;- sum((population_trees - mean(population_trees))^2)\npv_trees &lt;- sum_square / length(population_trees)\npv_trees\n\n[1] 15.39988\n\n\nThe base functions of R do not have one that calculates the population variance, but this can be solved by multiplying the value obtained with var()by (n-1)/n:\n\nvarp_trees &lt;- var(population_trees)\nvarp_trees &lt;- varp_trees * (length(population_trees) - 1) / length(population_trees)\nvarp_trees\n\n[1] 15.39988\n\n\nTo calculate the standard deviation it would be sufficient to obtain the square root of the previous value:\n\nsqrt(varp_trees)\n\n[1] 3.924268\n\n\nNote that the population mean and standard deviation are close to those specified in the code we used to simulate the data rnorm(1500, mean = 15, sd = 4)."
  },
  {
    "objectID": "posts/statistical-inference/index.html#sampling-mean-distribution",
    "href": "posts/statistical-inference/index.html#sampling-mean-distribution",
    "title": "Statistical Inference with R",
    "section": "Sampling Mean Distribution",
    "text": "Sampling Mean Distribution\nThe concept of distribution can also be applied to statistics such as the mean. To illustrate the above for our population of cookies let‚Äôs repeatedly take samples of size 5 and make a histogram with the averages of these samples:\n\nset.seed(151) # For reproducibility\n\n# 1000 samples\nn_samples &lt;- 1000\n\n# Empty vector where all means will be stored \nmean_cookies &lt;- vector(\"numeric\", length = n_samples)\n\n# Take a random sample of size 5, calculate the mean and save it in previous vector\n# Repeat the process 1000 times\nfor (i in 1:n_samples) {\n  mean_cookies[i] &lt;- mean(sample(diameter$D, size = 5)) \n}\n\n# Make a data frame with all means from samples of size 5\nmean_cookies &lt;- data.frame(mean_n5 = mean_cookies)\n\n# Histogram\nfreq_hist_mean &lt;- mean_cookies %&gt;% \n  ggplot(aes(x = mean_n5)) +\n  geom_histogram(color = \"black\", fill = \"white\", binwidth = 0.1) +\n  xlab(\"Diameter (cm)\") +\n  ylab(\"Count\") +\n  theme_classic() \nfreq_hist_mean\n\n\n\n\n\n\n\n\nThe previous code can be repeated with other sample sizes and this will be important in a later section (Central Limit Theorem)."
  },
  {
    "objectID": "posts/statistical-inference/index.html#students-t-distribution",
    "href": "posts/statistical-inference/index.html#students-t-distribution",
    "title": "Statistical Inference with R",
    "section": "Student‚Äôs t distribution",
    "text": "Student‚Äôs t distribution\nWhen sample sizes are small and the true standard deviation of the population is not known, Student‚Äôs t-distribution can be used to make a good guess of the distribution of the data. The shape of this distribution depends on the sample size and for large sample sizes the t-distribution tends to be normal:\n\n\n\n\n\n\n\n\n\nThe black line corresponds to a sample size of 2, the red line to a sample size of 4, the blue one to a size of 10, and the green one to a size of 50."
  },
  {
    "objectID": "posts/what-is-science/index.html",
    "href": "posts/what-is-science/index.html",
    "title": "What is science? - The way we understand and model reality",
    "section": "",
    "text": "When you want to awaken people‚Äôs interest in science, it is important to explain in a general way what it consists of. I do not think I have enough experience to give a detailed explanation of what science is, so I will limit myself to describing what it represents for me and some related concepts.\n\nWhat is science?\nScience can be defined as a human activity in charge of gathering knowledge and describing the different phenomena that make up reality in all its extension and complexity.\nKnowing and describing different natural phenomena, including social phenomena, gives us the possibility to develop and improve solutions that satisfy diverse human needs. In this way we can use scientific knowledge to influence reality itself: we cure diseases, improve processes that did not work well before, send satellites to outer space and many other things. At the beginning, when scientific knowledge is produced, it may not have any use, but over time some use may be found, which may depend on advances in other areas of science. Also, of course, all of the above also depends on the branch of science to which we are referring.\nScience, above all, is a social activity and, like all human beings, people who do science can and are susceptible to making mistakes. It is worth noting that unlike other human activities, such as religion or politics, science has the necessary mechanisms to detect and correct errors, biases, omissions and other mistakes. Science is undoubtedly an activity that requires the people involved to be self-critical and always willing to improve based on their mistakes.\nAlthough science is not a panacea, a cure for all the ills and problems that afflict humanity, it offers us an objective way to bring prosperity to every human being.\n\n\nThe scientific method\nHow is science done? Each branch of science has its own methods for generating knowledge. To do science we can follow some general guidelines, but not recipes that must be followed point by point. Below I list several steps that we can follow to generate scientific knowledge, but, in line with what I have just mentioned, we must bear in mind that there may be intermediate steps or even some points can be eliminated or modified depending on the area of knowledge we want to address.\n\nRecognize a question. This may be related to some unexplained phenomenon or a problem to which we wish to provide a solution.\nMake an adequate conjecture, a hypothesis, of the possible answers to our question. This is usually done through literature research or with the help of other means.\nPredict the consequences of the hypothesis. Once we have researched the possible answers, we make a prediction of what they would imply if they were actually true.\nWe conduct several experiments to test our predictions. From these experiments we obtain data or records. This step is important because it helps us test the hypothersis against the reality.\nIf our data or records do not agree with our previous predictions, we go back on the fly and look for or generate another hypothesis that might provide the answer to our question.\n\n\n\n\nFlow chart of the scientific method\n\n\nAnd what conditions must be met for something to be considered science? There are two key aspects:\n\nThe observations and/or experiments produced when investigating a particular phenomenon must be reproducible. That is, each time we repeat the experiment and our measurements (or records) under the same conditions, the results should be very similar to each other, even if the research is attempted to be reproduced in different places or countries.\nEvery scientific proposition must be falsifiable, which means that we must be able to formulate experiments whose results would deny the explanations given by such propositions. If it is not possible to formulate, at least with imagination, experiments aimed at demonstrating that what is said by a science is false, then it is quite possible that we are talking not of a science, but of a pseudoscience.\n\n\n\nScientific measurements\nMeasurement plays a central role in science. A measurement, in very general terms, is the assignment of a number to a certain phenomenon in nature. Measurements must be objective and do not depend on the judgment of the person making them. For the mass of an object, for example, we can use a balance, which will show us a number with some associated unit (grams, kilograms, etc.). Undoubtedly, an important part of the development of science is to find good and rigorous ways of making measurements.\n\n\n\nSome basic measuring instruments\n\n\nIt is important to mention that, depending on the phenomenon, there may not be rigorous ways to measure it. How do we measure some of the phenomena described by the social sciences?\n\n\nHypothesis, law and theory\nAs we saw earlier, a hypothesis is an assumption that can be substantiated in research through various means. When a hypothesis is sufficiently tested and no contradiction is found, it is considered a law or principle. A set of concepts, abstractions and laws related to a natural phenomenon becomes a theory. Theories can be considered as the highest level of scientific knowledge, since from these theories research problems can be posed and thus generate more knowledge.\nIt should be noted that there is no hierarchical relationship between law and theory. A theory is not below a law, nor vice versa. In the case of laws, they usually establish the relationship between different concepts and are simpler than theories. Also, depending on the branch of science, laws may not be as rigorous; a law is not the same in biology as in physics, for example. As for theories, they explain a set of phenomena (they are broader than laws) and are susceptible to change and improve over time based on the evidence that is generated from them or with the help of other branches of science.\n\n\nMyths about scientific research\nThere are some beliefs that somehow contribute to people‚Äôs lack of interest in science, mainly two:\n\nScientific research is very complicated and difficult. False, any human being can do research if they follow the right process for the problem they want to address. If we think about it carefully we are always doing research in our daily life.\nResearch is not linked to the everyday world. Also false, many of the conveniences and services we enjoy today have been derived from scientific research: smartphones, internal combustion engines, medicines, nylon, among many other products.\n\nPersonally, I believe that these misconceptions are due to the inability of research centers, universities and governments to communicate science to as many people as possible. Added to this is the superfluous treatment that the sensationalist media give to science and its findings.\n\n\nDeduction and induction\nDeduction and induction play a fundamental role in scientific work.\n\nDeduction. Reaching a particular conclusion from general premises. This is what we do when we formulate hypotheses from existing knowledge or when we make predictions based on established theories and laws.\nInduction. Reaching a general conclusion from observation of particular cases. The conclusion is considered probable, but not necessarily certain since there is always the possibility that new observations or evidence will disprove the conclusion. This is what we do when we perform experimentation to test our hypotheses.\n\nFrom both definitions we can infer that deduction and induction are processes that feed back on each other: we derive hypotheses from existing knowledge (deduction), we test these hypotheses again and again, improving at each step the initial hypothesis (induction). When sufficiently tested, the hypothesis becomes part of the theory that will allow us to generate new hypotheses, thus repeating the process of deduction and induction.\n\n\nFinal reflection\nThe development of science depends and has depended on the contribution of many people throughout history. It is a constantly changing human activity where its different branches feed back to improve our understanding of the world and the universe of which we are a part.\nScientific thinking is within everyone‚Äôs reach, it is not exclusive to universities or special beings. To a greater or lesser extent we can all put into practice the scientific method to ask and solve questions about what surrounds us.\n\n\nTo learn more about this topic\nBelow is a list with a couple of references and web pages that may be of help if you wish to delve a little deeper into the subject of this publication.\nDahlstrom, M.F. (2014) ‚ÄòUsing narratives and storytelling to communicate science with nonexpert audiences‚Äô, Proceedings of the National Academy of Sciences, 111(supplement_4), pp.¬†13614‚Äì13620. Available at: https://doi.org/10.1073/pnas.1320645111.\nRose, S.P.R. (2003) ‚ÄòHow to (or not to) communicate science‚Äô, Biochemical Society Transactions, 31(2), pp.¬†307‚Äì312. Available at: https://doi.org/10.1042/bst0310307.\nscientific method (2023) Encyclop√¶dia Britannica. Available at: https://www.britannica.com/science/scientific-method (Accessed: 27 September 2023).\nWhat Is Science? (2021) NASA Science. Space Place. Explore Earth and Space! Available at: https://spaceplace.nasa.gov/science/en/ (Accessed: 27 September 2023).\nThe text in this publication is free to use under license Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/nested-designs/index.html",
    "href": "posts/nested-designs/index.html",
    "title": "R Tutorial: Analysis of results of a nested experimental design",
    "section": "",
    "text": "A nested design is a type of experimental design in which the levels of one factor are hierarchically nested within the levels of another factor. For example, let‚Äôs imagine that we are interested in the effect of a type of drug on the expression of a specific gene. For this we design a nested experiment with mice as experimental units (in the figure two mice per treatment), where we include a control. From each mouse we took three cells and in each one we evaluated the expression of the gene twice (technical repetitions).\n\n\n\n\n\nAs shown in the figure, our design has a hierarchical appearance.\nIn this type of design we distinguish between two types of factors: fixed and random. A fixed factor is one that has discrete or finite values, while a random factor can take many values. In our example the drug factor would be considered a fixed factor and the mouse, cell and repeated measurements factors would be considered random factors. Note how the random factors are similar, such as mouse and cell, but not identical to each other and these are successively nested until the fixed factor drug.\nNow let‚Äôs see how we can analyze the results of this type of experimental design with the help of R code. It is important to mention that I took as a basis the publication in nature methods: Nested designs and replicated the example shown using, of course, R code. In addition, I added an extra drug and performed a multiple comparisons test to establish significant differences between the means of each treatment.\nIf you are interested about how I simulated the data, please take a look at the code in the data_simulation script found in the repository of this tutorial: (link)."
  },
  {
    "objectID": "posts/nested-designs/index.html#anova-table",
    "href": "posts/nested-designs/index.html#anova-table",
    "title": "R Tutorial: Analysis of results of a nested experimental design",
    "section": "ANOVA Table",
    "text": "ANOVA Table\nAs can be seen in our graph, there seems to be a difference between the effect of drug 2 with the other two levels of this factor (drug 1 and control). We can define if there are significant differences by means of an ANOVA table and subsequently a multiple comparisons test.\nLet‚Äôs obtain the ANOVA table with the GAD package, first we have to specify the fixed and random factors:\n\ndrug &lt;- as.fixed(mice_data$A)\nmice &lt;- as.random(mice_data$B)\ncell &lt;- as.random(mice_data$C)\n\nTo fit the linear model we must take into account the relationship between our response and the previously specified factors:\n\ndata_aov &lt;- aov(\n  expr ~ drug + mice:drug + cell:mice:drug,\n  data = mice_data\n  )\n\nThe term mice:drug denotes the variability of mice within each treatment, and the term cell:mice:drug denotes the variability of cells within each mouse and in turn within each treatment.\nTo display the ANOVA table we use the gad() function:\n\ngad(data_aov)\n\nAnalysis of Variance Table\n\nResponse: expr\n                Df Sum Sq Mean Sq F value    Pr(&gt;F)    \ndrug             2 743.44  371.72  12.796  0.001058 ** \ndrug:mice       12 348.59   29.05   5.163 7.413e-06 ***\ndrug:mice:cell  60 337.59    5.63  11.153 &lt; 2.2e-16 ***\nResidual       150  75.67    0.50                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe gad() function distinguishes between fixed and random effects, as well as the nested structure between these factors, so it makes corrections to calculate the F-ratios. The reason for this is that, depending on whether we are considering fixed or random effects, the expected values for the error mean squares (EM) change as follows:\n\n\n\n\n\nIn the above equations it is possible to observe the nested structure in the mean squares. Also note how for the case of treatments (\\(MS_A\\)) the expected values add up to the contribution to the variation of mice and cells. Therefore, it is necessary to divide \\(MS_A\\) by \\(MS_B\\) to obtain the F-ratio and infer differences between treatments.\nImportantly, in the case of technical replicates the variability of this random factor is properly estimated by the mean square of the Residual term in our ANOVA table."
  },
  {
    "objectID": "posts/nested-designs/index.html#estimation-of-the-variability-of-each-factor",
    "href": "posts/nested-designs/index.html#estimation-of-the-variability-of-each-factor",
    "title": "R Tutorial: Analysis of results of a nested experimental design",
    "section": "Estimation of the variability of each factor",
    "text": "Estimation of the variability of each factor\nWhen dealing with random factors, we are mainly interested in estimating their contribution to the variability of the response, as opposed to fixed factors where we are interested in estimating their effect on the population mean.\nWith the lme4 package we can estimate the contribution to variability of mice, cells and properly the error term (technical replicates). First we need to convert the data type in columns A, B and C into factors and then use the lmer() function as follows:\n\nmice_data3 &lt;- mice_data %&gt;% \n  mutate(A = as.factor(A), B = as.factor(B), C = as.factor(C))\n\ndata_lme &lt;- lmer(expr ~ 1 + A + (1|B:A) + (1|C:B:A), data = mice_data3)\n\nNote that fitting the linear model with the lmer() function requires a somewhat different syntax than that used with aov() and gad(). To show the contribution to variability of each factor we use the summary() function with the data_lme object as argument:\n\nsummary(data_lme)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: expr ~ 1 + A + (1 | B:A) + (1 | C:B:A)\n   Data: mice_data3\n\nREML criterion at convergence: 684.4\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.76545 -0.53144 -0.02996  0.59495  2.20017 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n C:B:A    (Intercept) 1.7073   1.3066  \n B:A      (Intercept) 1.5615   1.2496  \n Residual             0.5045   0.7103  \nNumber of obs: 225, groups:  C:B:A, 75; B:A, 15\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  10.0519     0.6224  16.151\nA2            1.4654     0.8801   1.665\nA3           -2.9085     0.8801  -3.305\n\nCorrelation of Fixed Effects:\n   (Intr) A2    \nA2 -0.707       \nA3 -0.707  0.500\n\n\nUnder Random effects the term Residual refers to technical replicates, the term B:A to mice and the term C:B:A to cells."
  },
  {
    "objectID": "posts/nested-designs/index.html#multiple-comparisons",
    "href": "posts/nested-designs/index.html#multiple-comparisons",
    "title": "R Tutorial: Analysis of results of a nested experimental design",
    "section": "Multiple comparisons",
    "text": "Multiple comparisons\nThe glth() function of the multcomp package can be used to perform the multiple comparison test:\n\nmult_drug &lt;- glht(data_lme, linfct = mcp(A = \"Tukey\"))\nsummary(mult_drug)\n\n\n     Simultaneous Tests for General Linear Hypotheses\n\nMultiple Comparisons of Means: Tukey Contrasts\n\n\nFit: lmer(formula = expr ~ 1 + A + (1 | B:A) + (1 | C:B:A), data = mice_data3)\n\nLinear Hypotheses:\n           Estimate Std. Error z value Pr(&gt;|z|)    \n2 - 1 == 0   1.4654     0.8801   1.665   0.2188    \n3 - 1 == 0  -2.9085     0.8801  -3.305   0.0027 ** \n3 - 2 == 0  -4.3739     0.8801  -4.970   &lt;1e-04 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n(Adjusted p values reported -- single-step method)\n\n\nAlternatively or additionally it is also possible to use the HSD.test() function of the agricolae package. For this I specified the mean squared error for mice (29.05) as well as the degrees of freedom for this factor (12):\n\ntukey_hsd &lt;- with(mice_data, HSD.test(expr, A, DFerror = 12, MSerror = 29.05))\ntukey_hsd$groups\n\n       expr groups\n2 11.517297      a\n1 10.051919      a\n3  7.143408      b\n\n\nAccording to the results of the above analyses, we can conclude that drug 2 significantly reduced the gene expression levels compared to drug 1 and the control."
  },
  {
    "objectID": "posts/simultaneus_optimization/index.html",
    "href": "posts/simultaneus_optimization/index.html",
    "title": "R tutorial: Simultaneus Optimization of Several Response Variables",
    "section": "",
    "text": "The data and code for this publication can be found in the following repository: simultaneous-optimization."
  },
  {
    "objectID": "posts/simultaneus_optimization/index.html#definition-of-functions-for-simultaneous-optimization",
    "href": "posts/simultaneus_optimization/index.html#definition-of-functions-for-simultaneous-optimization",
    "title": "R tutorial: Simultaneus Optimization of Several Response Variables",
    "section": "Definition of functions for simultaneous optimization",
    "text": "Definition of functions for simultaneous optimization\nFor simultaneous optimization I defined a couple of functions. First a prediction function:\n\n# Prediction function\nrsm_opt &lt;- function(x, dObject, space = \"square\"){\n  \n  df &lt;- data.frame(x1 = x[1], x2 = x[2], x3 = x[3])\n  \n  y1 &lt;- predict.lm(y1_m, df)\n  y2 &lt;- predict.lm(y2_m, df)\n  y3 &lt;- predict.lm(y3_m, df)\n  y4 &lt;- predict.lm(y4_m, df)\n  \n  out &lt;- predict(dObject, data.frame(y1 = y1, y2 = y2, y3 = y3, y4 = y4))\n  \n  if(space == \"circular\" & sqrt(sum(x^2)) &gt; 1.63) out &lt;- 0\n  else if(space == \"square\" & any(abs(x) &gt; 1.63)) out &lt;- 0 \n  \n  out\n}\n\nThis function specifies two possible shapes for the experimental region: square and circular. This must be taken into account and may vary according to our experimental design.\nSubsequently, I defined a function that will be in charge of finding the best global desirability value:\n\n# Optimization function\nmaximize_overall &lt;- function(int_1 = c(-1.63, 1.63),\n                             int_2 = c(-1.63, 1.63),\n                             int_3 = c(-1.63, 1.63),\n                             dObject = NULL, \n                             space = \"square\"){\n  \n  searchGrid &lt;- expand.grid(\n    x1 = seq(int_1[1], int_1[2], length.out = 5),\n    x2 = seq(int_2[1], int_2[2], length.out = 5),\n    x3 = seq(int_3[1], int_3[2], length.out = 5)\n  )\n  \n  for(i in 1:dim(searchGrid)[1]){\n    \n    tmp &lt;- optim(as.vector(searchGrid[i,]),\n                 rsm_opt,\n                 dObject = dObject,\n                 space = space, \n                 control = list(fnscale = -1))\n    \n    if(i == 1) best &lt;- tmp\n    if(tmp$value &gt; best$value) best &lt;- tmp\n  }\n  \n  best\n}\n\nNote that in the previous functions I took into account the maximum and minimum coded values of each factor, in this case 1.63 and -1.63, respectively. If you wish to use this code with your own data do not forget to modify these values according to your experimental design."
  },
  {
    "objectID": "posts/simultaneus_optimization/index.html#definition-of-desirability-functions",
    "href": "posts/simultaneus_optimization/index.html#definition-of-desirability-functions",
    "title": "R tutorial: Simultaneus Optimization of Several Response Variables",
    "section": "Definition of desirability functions",
    "text": "Definition of desirability functions\nWith the functions included in the desirability package we define a desirability function for each response as follows:\n\nlibrary(desirability)\n\nD_y1 &lt;- dMax(120, 170)\nD_y2 &lt;- dMax(1000, 1300)\nD_y3 &lt;- dTarget(400, 500, 600)\nD_y4 &lt;- dTarget(60, 67.5, 75)\n\nEach target value was set for convenience and was sought to meet the above specifications as outlined in Derringer & Suich (1980).\nGlobal desirability is in turn defined by taking into account the above functions:\n\nD_overall &lt;- dOverall(D_y1, D_y2, D_y3, D_y4)"
  },
  {
    "objectID": "posts/simultaneus_optimization/index.html#carrying-out-simultaneous-optimization",
    "href": "posts/simultaneus_optimization/index.html#carrying-out-simultaneous-optimization",
    "title": "R tutorial: Simultaneus Optimization of Several Response Variables",
    "section": "Carrying out simultaneous optimization",
    "text": "Carrying out simultaneous optimization\nWith the previously defined functions we proceed to perform the simultaneous optimization:\n\noverall_opt &lt;- maximize_overall(dObject = D_overall)\n\nDepending on your data and the specifications of your computer, this process may be a bit slow.\nThe processing that allows you to obtain the maximum overall desirability can be deployed directly:\n\noverall_opt\n\n$par\n         x1          x2          x3 \n-0.05345891  0.14718872 -0.86635592 \n\n$value\n[1] 0.5833527\n\n$counts\nfunction gradient \n     502       NA \n\n$convergence\n[1] 1\n\n$message\nNULL\n\n\n$value refers to the overall desirability value obtained. This result is practically the same as that published in the Derringer & Suich paper."
  },
  {
    "objectID": "posts/prickly-pear-fruits/index.html",
    "href": "posts/prickly-pear-fruits/index.html",
    "title": "Prickly pear, a sweet source of nutrients",
    "section": "",
    "text": "The global prickly pear market has been growing gradually at present. The top exporter in 2022 was Canada, with an export value of 2.89 million USD , while the United States was the top importer in 2022, with an import value of 4.47 million USD , an increase of 6% over 2021. On the other hand, consumer demand for this fruit has also been growing slowly and continues to thrive thanks to its nutritional value and versatility in cooking and traditional medicine.\n\n\n\n\n\nMexico is considered the largest international producer of prickly pear, followed by South American countries such as Chile, Brazil and Argentina where large quantities are also produced. In Europe, Italy is the largest producer, while Morocco is the leading producer in Africa.\nThe prickly pear is a common fruit in Mexican markets, especially between July and September, which is the peak production period. In 2019 alone, up to 428,300 tons of this sweet delicacy were produced. And although its production is below other fruits such as oranges, bananas and apples, prickly pear fruits can be considered an important source of various nutrients.\nPrickly pear pulp is a good source of carbohydrates, fiber, vitamin C, vitamin A, calcium, potassium and not so well known components such as phenolic compounds, which are known to have several beneficial effects as part of a balanced diet. Its consumption can also be considered as a natural alternative to reduce cholesterol and triglyceride levels, relieve the discomfort caused by ulcers and help stabilize and regulate blood sugar levels.\nAlthough it is more common to find green prickly pears, there are also yellow, orange, red and purple ones. In these different colored varieties there is a group of compounds called betalains, pigments responsible for conferring the color. As an additional benefit, it is also known that the consumption of betalains can reduce cholesterol and triglyceride levels in the blood.\n\n\n\n\n\nPrickly pears can be used for the production of juice, jam, vinegar and liquors. From the varieties of different colors are obtained dyes that can be used as an alternative to synthetic dyes in food. The peel of this fruit is usually discarded, but it can also be considered an important source of pigments. Moreover, even edible oil with a high content of beneficial compounds, such as unsaturated fatty acids, vitamin E and vitamin K, can be obtained from the seeds.\n\n\n\nPrickly pear liqueur\n\n\nAs mentioned above, Mexico is the world‚Äôs leading producer of prickly pear. The main advantage of prickly pear cactus is its low water requirement, which in drought areas makes it a more viable crop than beans and corn. In areas where it is grown extensively, prickly pear cactus is the main source of income for many Mexican families. It is also important to mention that Mexico is the center of origin of the prickly pear, with at least 100 species from which edible fruits can be obtained.\n\n\n\n\n\nGood for you! Now that you know all this, aren‚Äôt you craving for some prickly pears? See you at the market!\nIf you want to know a little more about the prickly pear, do not hesitate to consult the pages and documents in the following links:\n\nFrutas, una delicia del campo mexicano.\nRazones para comer tunas m√°s seguido.\nInnovaci√≥n de productos de alto valor agregado a partir de la tuna mexicana.\nEcolog√≠a del cultivo, manejo y usos del nopal.\nM√©xico exporta tuna y xoconostle a EU, Canad√°, Jap√≥n, Emiratos √Årabes Unidos, Francia y Reino Unido.\n2023 Prickly Pear global market overview today - Tridge.\n Global Production of Cactus Pear - South Africa Online.\nSouth Africa: Prickly pear grower has big plans for the fruit - FreshPlaza.\nPrickly Pear Market Summary - Produce Blue Book.\nPears | Agricultural Marketing Resource Center.\n\nThe text in this publication is free to use under license Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "posts/pca-from-scratch/index.html",
    "href": "posts/pca-from-scratch/index.html",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "",
    "text": "All the data and code of this post can be download on the next repository : pca-from-scratch"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#packages",
    "href": "posts/pca-from-scratch/index.html#packages",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Packages",
    "text": "Packages\nFor this post we will use some functions included in ggplot2, ggpubr, readr, purrr and dplyr:\n\n# Run the next line if you have not installed the packages:\n# install.packages(c(\"ggplot2\", \"ggpubr\", \"readr\", \"purrr\", \"dplyr\"))\n\nlibrary(ggplot2)\nlibrary(ggpubr)\nlibrary(readr)\nlibrary(purrr)\nlibrary(dplyr)"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#data-simulation",
    "href": "posts/pca-from-scratch/index.html#data-simulation",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Data simulation",
    "text": "Data simulation\nFirst let‚Äôs simulate data with two dimensions. To do this let‚Äôs make the second variable directly dependent on the first one and store everything in a data frame:\n\nset.seed(1) # For data reproducibility\n\n# Variable 1\nvar_1 &lt;- rnorm(50, 50, sd = 3)\n\n# Variable 2\nvar_2 &lt;- .5*var_1 + rnorm(50, sd = sqrt(3))\n\n# Both variables in a data frame\ndata_set_1 &lt;- tibble(var_1, var_2)\n\nhead(data_set_1)\n\n# A tibble: 6 √ó 2\n  var_1 var_2\n  &lt;dbl&gt; &lt;dbl&gt;\n1  48.1  24.7\n2  50.6  24.2\n3  47.5  24.3\n4  54.8  25.4\n5  51.0  28.0\n6  47.5  27.2\n\n\nIf we make a scatter plot we can observe the clear dependence between both variables:\n\n# A scatter plot with the two simulated variables\nggplot(data_set_1, aes(x = var_1, y = var_2)) +\n  geom_point(color = \"blue\", size = 2) +\n  xlab(\"Variable 1\") +\n  ylab(\"Variable 2\") +\n  theme_classic()"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#first-step-center-each-variable",
    "href": "posts/pca-from-scratch/index.html#first-step-center-each-variable",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "First step: Center each variable",
    "text": "First step: Center each variable\nThe first step in the PCA is to center each variable with respect to its average value:\n\ndata_set_1 &lt;- data_set_1 %&gt;% \n  mutate(varc_1 = var_1 - mean(var_1), varc_2 = var_2 - mean(var_2))\n\nhead(data_set_1)\n\n# A tibble: 6 √ó 4\n  var_1 var_2 varc_1  varc_2\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1  48.1  24.7 -2.18  -0.604 \n2  50.6  24.2  0.250 -1.14  \n3  47.5  24.3 -2.81  -1.02  \n4  54.8  25.4  4.48   0.0829\n5  51.0  28.0  0.687  2.62  \n6  47.5  27.2 -2.76   1.85  \n\n\nThis did not change the relative position between each point, so the data look the same:\n\nggplot(data_set_1, aes(x = varc_1, y = varc_2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  theme_classic()"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#second-step-obtaining-the-covariance-matrix",
    "href": "posts/pca-from-scratch/index.html#second-step-obtaining-the-covariance-matrix",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Second step: Obtaining the covariance matrix",
    "text": "Second step: Obtaining the covariance matrix\nThe next step is to obtain the covariance matrix for the previous data set. For this we perform a matrix multiplication with the data as follows:\n\n# Select only the centered variables\ndata_set_2 &lt;- data_set_1 %&gt;% \n  select(varc_1, varc_2) %&gt;% \n  as.matrix()\n\n# Calculate the covariance matrix\ncov_m &lt;- (t(data_set_2) %*% data_set_2) / (nrow(data_set_2) - 1) \n\ncov_m\n\n         varc_1   varc_2\nvarc_1 6.220943 2.946877\nvarc_2 2.946877 4.207523\n\n\nIn the resulting matrix the diagonal contains the variance of each variable while the values outside the diagonal are the covariances between the variables (see figure below):\n\n\n\n\n\nWe can obtain the covariance matrix directly with the cov() function:\n\ncov(data_set_2)\n\n         varc_1   varc_2\nvarc_1 6.220943 2.946877\nvarc_2 2.946877 4.207523\n\n\nOr with the crossprod() function as follows:\n\ncrossprod(data_set_2) / (nrow(data_set_2) - 1)\n\n         varc_1   varc_2\nvarc_1 6.220943 2.946877\nvarc_2 2.946877 4.207523"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#third-step-obtain-the-eigenvalues-and-eigenvectors-of-the-covariance-matrix",
    "href": "posts/pca-from-scratch/index.html#third-step-obtain-the-eigenvalues-and-eigenvectors-of-the-covariance-matrix",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Third step: Obtain the eigenvalues and eigenvectors of the covariance matrix",
    "text": "Third step: Obtain the eigenvalues and eigenvectors of the covariance matrix\nPrincipal components represent the directions in the data that explain the maximum amount of variance. They are ‚Äúlines‚Äù that collect most of the information in the data. These directions can be obtained by calculating the eigenvalues and eigenvectors of the covariance matrix:\n\n# Use eigen() to obtain eigenvectors and eigenvalues\ncov_e &lt;- eigen(cov_m)\n\n# Eigenvectors\ne_vec &lt;- cov_e$vectors\n\n# Eigenvalues\ne_val &lt;- cov_e$values\n\nThe span of each eigenvector can be considered the ‚Äúline‚Äù that captures most of the variation:\n\n# First eigenvector \nev_1 &lt;- e_vec[,1]\n\n# Slope of the first eigenvector\nev1_m &lt;- ev_1[2] / ev_1[1]\n\n# Second eigenvector \nev_2 &lt;- e_vec[,2]\n\n# Slope of the second eigenvector\nev2_m &lt;- ev_2[2] / ev_2[1]\n\n# Scatter plot showing the span of both eigenvectors \nggplot(data.frame(data_set_2), aes(x = varc_1, y = varc_2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  geom_abline(slope = ev1_m, color = \"blue\", linewidth = 0.7) +\n  geom_abline(slope = ev2_m, color = \"red\", linewidth = 0.7) +\n  theme_classic()\n\n\n\n\nAs can be seen, there is one eigenvector for each variable in the data set, in this case two. Also note that the eigenvectors are perpendicular:\n\n# Multiply both eigenvectors \nev_1 %*% ev_2\n\n     [,1]\n[1,]    0\n\n\nAs for the eigenvalues, their numerical values are equal to the sum of squares of the distances of each projected data point in the corresponding principal component. This sum of squares is maximized in the first principal component."
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#fourth-step-make-a-scree-plot",
    "href": "posts/pca-from-scratch/index.html#fourth-step-make-a-scree-plot",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Fourth step: Make a Scree Plot",
    "text": "Fourth step: Make a Scree Plot\nDividing each eigenvalue by n - 1 (n is the number of rows in the original data) will give an estimate of the variance represented by each principal component. The sum of all variances (the total variance) can be used to calculate the percentage of variation and visualized with a Scree Plot:\n\n# Calculate the estimated variance for each eigenvalue\ne_var &lt;- e_val / (nrow(data_set_2) - 1)\n\n# Data frame with variance percentages\nvar_per &lt;- tibble(\n  PC  = c(\"PC1\", \"PC2\"),\n  PER = c(e_var) * 100 / sum(e_var) # Calculate the percentage\n    )\n\n# Scree plot \nggplot(var_per, aes(x = PC, y = PER)) +\n  geom_col(width = 0.5, color = \"black\") +\n  xlab(\"Principal component\") +\n  ylab(\"Percentage of variation (%)\") +\n  theme_classic()"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#fifth-step-obtain-the-loadings-of-each-variable.",
    "href": "posts/pca-from-scratch/index.html#fifth-step-obtain-the-loadings-of-each-variable.",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Fifth step: Obtain the loadings of each variable.",
    "text": "Fifth step: Obtain the loadings of each variable.\nThe eigenvectors obtained using the eigen() function are normalized. This means that their length is equal to 1:\n\n# Norm of the first eigenvector\nnorm(as.matrix(ev_1), \"F\")\n\n[1] 1\n\n# Norm of the second eigenvector\nnorm(as.matrix(ev_2), \"F\")\n\n[1] 1\n\n\nThe elements of each eigenvector are also called loadings and can be interpreted as the contribution of each variable in the data set to the corresponding principal component or, more strictly, as the coefficients of the linear combination of the original variables from which the principal components are constructed.\nWe can make a table with these values and see the contributions of each variable to each principal component:\n\n# Data frame with both eigenvectors\nloads &lt;- tibble(\n  VAR   = c(\"var_1\", \"var_2\"),\n  PC1 = ev_1, # First eigenvecor\n  PC2 = ev_2  # Second eigenvectors\n)\n\nloads\n\n# A tibble: 2 √ó 3\n  VAR      PC1    PC2\n  &lt;chr&gt;  &lt;dbl&gt;  &lt;dbl&gt;\n1 var_1 -0.813  0.582\n2 var_2 -0.582 -0.813\n\n\nThe above can be useful in data with many dimensions to get an idea of which variables cause the groupings or differences in the PCA plot."
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#sixth-step-representing-data-in-fewer-dimensions",
    "href": "posts/pca-from-scratch/index.html#sixth-step-representing-data-in-fewer-dimensions",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Sixth step: Representing data in fewer dimensions",
    "text": "Sixth step: Representing data in fewer dimensions\nIf we change the base of the original data to that indicated by the eigenvectors, we are basically rotating the data:\n\n# Change the basis of the original data \ndata_set_3 &lt;- data_set_2 %*% solve(e_vec) # Inverse of eigenvectors matrix\n\n# Scatter showing the rotation \nggplot(data.frame(data_set_3), aes(X1, X2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  xlab(\"PC1 (78.8%)\") +\n  ylab(\"PC2 (21.2%)\") +\n  theme_classic()\n\n\n\n\nComparing the two graphs gives us an idea of how the data have been rotated once we change the base:\n\n# Scatter plot with the centered data \nplot_data &lt;- ggplot(data.frame(data_set_2), aes(x = varc_1, y = varc_2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  ylim(c(-8, 8.5)) +\n  ggtitle(\"Original Data\") +\n  theme_classic()\n\n# Scatter plot with the rotated data\nplot_rotation &lt;- ggplot(data.frame(data_set_3), aes(X1, X2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  xlab(\"PC1 (78.8%)\") +\n  ylab(\"PC2 (21.2%)\") +\n  ylim(c(-8, 8.5)) +\n  ggtitle(\"Change of Basis to Eigenvectors\") +\n  theme_classic()\n\n# Both graphs side by side\nggarrange(plot_data, plot_rotation)\n\n\n\n\nSince principal component 1 (PC1) explains most of the variance in the data, we can omit principal component 2 (PC2) and represent each point in a single dimension, here as red dots:\n\n# Data points just from PC 1\ndata_pc1 &lt;- data.frame(v1 = data_set_3[,1], v2 = rep(0, nrow(data_set_3)))\n\n# Scatter plot showing the projected points from PC1 (red points)\nggplot(data.frame(data_set_3), aes(X1, X2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_point(data = data_pc1, aes(v1, v2), color = \"red\", size = 2) +\n  geom_vline(xintercept = 0, linewidth = .5) +\n  geom_hline(yintercept = 0, linewidth = .5) +\n  xlab(\"PC1 (78.8%)\") +\n  ylab(\"PC2 (21.2%)\") +\n  ylim(c(-8, 8.5)) +\n  theme_classic()\n\n\n\n\nThe above ideas can be used in data with many variables to reduce the dimensions and make two-dimensional representations of the data."
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#first-step-center-the-data",
    "href": "posts/pca-from-scratch/index.html#first-step-center-the-data",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "First step: Center the data",
    "text": "First step: Center the data\nSubtract the average of each variable (columns) and divide by its standard deviation:\n\n# Means for each variable\nvar_means &lt;- unlist(map(data_set_wine, mean))\n\n# Standard deviation for each variable\nvar_sd &lt;- unlist(map(data_set_wine, sd))\n\n# Center each variable\ndata_set_wine_2 &lt;- map2(\n  data_set_wine, var_means, .f = function(x, mean) x - mean\n  )\n\n# Devide by the standard deviation of each variable\ndata_set_wine_2 &lt;- map2(\n  data_set_wine_2, var_sd, .f = function(x, sd) x / sd\n)\n\n# Make a matrix from the previous list\ndata_set_wine_2 &lt;- as.matrix(data.frame(data_set_wine_2))\n\nEach row of the transformed data corresponds to the same wine sample as the original data.\nThe first six rows of the transformed data look like this:\n\nhead(data_set_wine_2)\n\n         Ethanol   TotalAcid  VolatileA  MalicAcid         pH    LacticAc\n[1,] -0.63678489 -0.40870999 -0.7811641  2.2627949 0.18823560 -0.88226788\n[2,]  0.29224039  0.01096889  1.3231961 -0.5694123 0.40060357  0.38388752\n[3,] -0.38341492 -0.97527630  0.4814521 -1.9201573 1.88717938  0.06061381\n[4,]  0.05998306 -0.15690246  0.4814521 -1.2229986 1.03770749 -0.28959935\n[5,]  1.15792167 -0.15690246 -0.1498560  1.0427673 0.08205162 -0.80144937\n[6,]  1.45351898 -0.59756526  0.8321787 -0.9179917 2.41810183  1.75780072\n        ReSugar CitricAcid        CO2    Density     FolinC     Glycerol\n[1,] -0.5311188  1.1594919 -1.6049333 -0.7844233 -0.1371321 -0.525885112\n[2,]  0.1996454  0.1084153 -0.9482843  1.2458487  1.2376582 -0.160707985\n[3,] -0.4854461  1.8063082  1.5330409 -0.7844233  0.2405111  0.802031815\n[4,]  1.5317676  1.9680124  0.5483973  1.2458487  1.6138874 -0.559083800\n[5,] -0.6909735 -0.2149929 -1.0972195 -0.7844233  1.3861700  0.680306454\n[6,] -0.5767916 -0.5384012 -1.0868116 -0.7844233  1.4003137 -0.005784995\n       Methanol   TartaricA\n[1,] -1.1081037  0.43877622\n[2,] -0.0870238 -0.03865555\n[3,] -0.5975636 -1.05319836\n[4,]  0.6787860  1.99042973\n[5,] -0.0870238 -1.11287729\n[6,] -0.3422939 -2.06774119\n\n\nDividing by standard deviations is a way of giving each variable equal importance despite its range, magnitude and/or scale of measurement. In addition to dividing by the standard deviation, it is possible to apply other transformations depending on the data. See the resources at the end of this post if you are interested."
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#second-step-calculating-the-covariance-matrix",
    "href": "posts/pca-from-scratch/index.html#second-step-calculating-the-covariance-matrix",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Second step: Calculating the covariance matrix",
    "text": "Second step: Calculating the covariance matrix\nWe multiply the data (in matrix form) by its transpose:\n\n# Calculate the covariance matrix\ncov_wine &lt;- (t(data_set_wine_2) %*% data_set_wine_2) / \n  (nrow(data_set_wine_2) - 1)\n\ncov_wine[1:5, 1:5]\n\n             Ethanol  TotalAcid  VolatileA   MalicAcid         pH\nEthanol   1.00000000  0.3262321  0.2028382  0.04166778  0.1697347\nTotalAcid 0.32623209  1.0000000  0.4660575 -0.26067574 -0.3518303\nVolatileA 0.20283816  0.4660575  1.0000000 -0.74628880  0.3011611\nMalicAcid 0.04166778 -0.2606757 -0.7462888  1.00000000 -0.2929542\npH        0.16973470 -0.3518303  0.3011611 -0.29295421  1.0000000\n\n\nHere we only show the first five rows and columns of the covariance matrix. If you want to display the whole matrix a good way is to use the View() function.\nEach value of the cov_wine matrix has the same interpretation as in the example with two variables, the values on the diagonal are the variances of each variable, and the values outside the diagonal are the covariances between variables. As can be seen, all variances are equal to 1. This is precisely the effect of centering and dividing by the standard deviation."
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#third-step-obtain-the-eigenvalues-and-eigenvectors-of-the-covariance-matrix.",
    "href": "posts/pca-from-scratch/index.html#third-step-obtain-the-eigenvalues-and-eigenvectors-of-the-covariance-matrix.",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Third step: Obtain the eigenvalues and eigenvectors of the covariance matrix.",
    "text": "Third step: Obtain the eigenvalues and eigenvectors of the covariance matrix.\nLet‚Äôs use the eigen() function to obtain the eigenvectors and their eigenvalues:\n\n# eigen() to obtain eigenvalues and eigenvectors\neg_wine &lt;- eigen(cov_wine)\n\n# Eigenvalues\neg_vals &lt;- eg_wine$values\n\n# Eigenvectors\neg_vecs &lt;- eg_wine$vectors\n\nThe number of vectors and eigenvalues is the same as the number of variables in the original data set:\n\n# Number of eigenvalues\nlength(eg_vals)\n\n[1] 14\n\n# Number of eigenvectors\nncol(eg_vecs)\n\n[1] 14"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#fourth-step-make-a-scree-plot-1",
    "href": "posts/pca-from-scratch/index.html#fourth-step-make-a-scree-plot-1",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Fourth step: Make a Scree Plot",
    "text": "Fourth step: Make a Scree Plot\nWe calculated the percentage of variation of each component and made a Scree plot:\n\n# Calculate variances from each eigenvalue\neg_vars &lt;- eg_vals / (nrow(data_set_wine_2) - 1)\n\n# Data frame with variance percenatges\nvars_perc &lt;- tibble(\n  PC  = unlist(map(1:14, function(x) paste0(\"PC\", x))),\n  PER = round((eg_vars * 100) / sum(eg_vars), 4)\n    )\n\n# Scree plot\nggplot(\n  vars_perc, \n  aes(x = reorder(PC, order(PER, decreasing = TRUE)), y = PER)\n       ) +\n  geom_col(width = 0.5, color = \"black\") +\n  xlab(\"Principal component\") +\n  ylab(\"Percentage of variation (%)\") +\n  theme_classic()"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#fifth-step-obtain-the-loadings-of-each-variable",
    "href": "posts/pca-from-scratch/index.html#fifth-step-obtain-the-loadings-of-each-variable",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Fifth step: Obtain the loadings of each variable",
    "text": "Fifth step: Obtain the loadings of each variable\nThe elements of each eigenvector represent the weight of each variable in the corresponding principal component:\n\n# Data frame with loading scores\nloads_wine &lt;- data.frame(eg_vecs)\ncolnames(loads_wine) &lt;- vars_perc$PC\nrownames(loads_wine) &lt;- var_names\n\nhead(loads_wine)\n\n                 PC1        PC2         PC3         PC4          PC5\nEthanol   -0.2050720 -0.3452884  0.28833198 -0.33833697  0.001102612\nTotalAcid -0.1457210 -0.4545803 -0.06374908  0.42139270  0.041508881\nVolatileA  0.2959952 -0.4418538  0.03338975  0.08430388 -0.010891943\nMalicAcid -0.3401714  0.3173079  0.17907611 -0.04002812 -0.252870227\npH         0.2413635 -0.1030772 -0.04459655 -0.66472873 -0.170592677\nLacticAc   0.3462627 -0.3745378 -0.16738658  0.11850141 -0.108643313\n                  PC6          PC7         PC8        PC9        PC10\nEthanol   -0.02992588  0.106645514  0.40565762  0.2220668 -0.07364281\nTotalAcid -0.10721872  0.109080109 -0.15801515  0.1647662 -0.12640759\nVolatileA  0.12676353 -0.117134957 -0.01066998 -0.2323371  0.19698524\nMalicAcid  0.09495473  0.247290148 -0.35870967  0.1792700 -0.27203174\npH         0.01717167 -0.191605073 -0.30040129  0.3352703  0.28827462\nLacticAc   0.05496474  0.008009813 -0.10169448  0.1377493 -0.28605639\n                 PC11        PC12        PC13        PC14\nEthanol    0.52247393 -0.22486081  0.29025392 -0.04195694\nTotalAcid -0.09209975 -0.25488777 -0.01537897  0.65083548\nVolatileA  0.19114512 -0.33800898 -0.59864002 -0.27743084\nMalicAcid -0.07338526 -0.53559762 -0.18854505 -0.23066288\npH        -0.21200577 -0.09998927 -0.02838824  0.28223548\nLacticAc  -0.32247769 -0.11583440  0.49548494 -0.45696212\n\n\nMaking a scatter plot with these values can help determine correlations between variables and/or explain why a particular clustering is observed in principal component scatter plots (next section):\n\n# Scatter plot with loadings of PC1 and PC2\nld_pc12 &lt;- ggplot(loads_wine, aes(PC1, PC2)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_text(aes(label = rownames(loads_wine)), hjust = -.2) +\n  ggtitle(\"Loadings for PC1 and PC2\") +\n  xlim(c(-.7, .7)) +\n  ylim(c(-.7, .7)) +\n  xlab(\"PC1 (24.4%)\") +\n  ylab(\"PC2 (21.3%)\") +\n  theme_classic()\n\n# Scatter plot with loadings of PC3 and PC4\nld_pc34 &lt;- ggplot(loads_wine, aes(PC3, PC4)) +\n  geom_point(color = \"blue\", size = 2) +\n  geom_vline(xintercept = 0) +\n  geom_hline(yintercept = 0) +\n  geom_text(aes(label = rownames(loads_wine)), hjust = -.2) +\n  ggtitle(\"Loadings for PC3 and PC4\") +\n  xlim(c(-.7, .7)) +\n  ylim(c(-.7, .7)) +\n  xlab(\"PC3 (17.5%)\") +\n  ylab(\"PC4 (10.0%)\") +\n  theme_classic()\n\n# Both graphs side by side\nggarrange(ld_pc12, ld_pc34)"
  },
  {
    "objectID": "posts/pca-from-scratch/index.html#sixth-step-representing-data-in-fewer-dimensions-1",
    "href": "posts/pca-from-scratch/index.html#sixth-step-representing-data-in-fewer-dimensions-1",
    "title": "R tutorial: Principal Component Analysis from scratch",
    "section": "Sixth step: Representing data in fewer dimensions",
    "text": "Sixth step: Representing data in fewer dimensions\nIdeally, if PC1 and PC2 picked up most of the variation, say more than 90%, it would be possible to make a good representation of the data in a two-dimensional scatter plot. But since real data are almost never ideal, in this case PC1, PC2, PC3 and PC4 account for 73% of the variation. Let us try to observe clustering using two scatter plots, the first with PC1 and PC2, and the second with PC3 and PC4.\nFirst, we change the basis of the transformed data to that indicated by the eigenvectors:\n\n# Change the basis of the data\ndata_set_wine_eb &lt;- data_set_wine_2 %*% solve(eg_vecs)\n\n# Transfrom to a data frame\ndata_set_wine_eb &lt;- data.frame(data_set_wine_eb)\ncolnames(data_set_wine_eb) &lt;- vars_perc$PC\n\n# Add a column with the origin of each wine sample\ndata_set_wine_eb &lt;- data_set_wine_eb %&gt;% \n  mutate(\n    WineSample = unlist(map(wine_label, function(x) substr(x, 1, 3)))\n    ) %&gt;% \n  relocate(WineSample)\n\nhead(data_set_wine_eb)\n\n  WineSample        PC1        PC2        PC3        PC4         PC5\n1        ARG -0.8708602  1.4271679  1.0042682 -0.4970542 -2.40194963\n2        ARG  0.9214933 -0.8023313  0.9170387 -0.3788901  0.04278493\n3        ARG  2.0993602 -1.1582579  0.1104490 -0.8588001  0.36878072\n4        ARG  2.6784566  0.6585042 -0.5914731 -1.1987690  0.55471303\n5        ARG -0.3093155 -0.7061611  0.9719731 -0.8791527 -1.48911945\n6        ARG  0.6656888 -1.9938918  2.0606329 -0.4319683 -0.58522246\n          PC6        PC7        PC8         PC9         PC10        PC11\n1 -0.50073335  0.7380182 -0.7113971  1.28454429  0.141197785  0.91258285\n2 -1.11669152 -0.6204253  0.1024002 -0.66345802  0.217715309 -0.44779807\n3 -0.01460188  0.6155704  1.2988198 -0.31647718 -1.945505546  1.56333545\n4 -1.77225038  1.1845744  0.7517347 -0.01741376 -0.458803511 -1.38374975\n5  0.58458758 -1.3525017  0.3597807  1.06873458  0.656603378 -0.04068736\n6  0.76213010 -0.9997045  2.4380628 -1.36498248 -0.004949351  1.27477965\n        PC12        PC13        PC14\n1 -0.0961029 -0.01151848  0.12807023\n2  0.6191680 -1.13863351 -0.22032005\n3 -0.2843139  1.44732864  0.08942808\n4  0.1136096 -1.16558478 -0.14095094\n5 -0.4097057  0.16732603 -0.70481769\n6 -0.5940964 -0.93709414 -0.52629641\n\n\nNow, let‚Äôs make both scatter plots taking only the values of PC1, PC2, PC3 and PC4:\n\n# Scatter plot for PC1 and PC2\npc12 &lt;- ggplot(\n  data_set_wine_eb, \n  aes(PC1, PC2, color = WineSample, shape = WineSample)\n) +\n  geom_point(size = 3) +\n  ggtitle(\"PC1 and PC2\") +\n  xlab(\"PC1 (24.4%)\") +\n  ylab(\"PC2 (21.3%)\") +\n  theme_classic() +\n  theme(legend.position = \"none\") \n\n# Scatter plot for PC3 and PC4\npc34 &lt;- ggplot(\n  data_set_wine_eb, \n  aes(PC3, PC4, color = WineSample, shape = WineSample)\n) +\n  geom_point(size = 3) +\n  scale_color_discrete(\n    name = \"Wine origin\", \n    labels = c(\"Argentina\", \"Australia\", \"Chile\", \"South Africa\")\n    ) +\n  scale_shape_discrete(\n    name = \"Wine origin\", \n    labels = c(\"Argentina\", \"Australia\", \"Chile\", \"South Africa\")\n  ) +\n  ggtitle(\"PC3 and PC4\") +\n  xlab(\"PC3 (17.5%)\") +\n  ylab(\"PC4 (10.0%)\") +\n  theme_classic()\n\n# Both graphs side by side\nggarrange(pc12, pc34)"
  }
]